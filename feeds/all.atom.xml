<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Alex Liebscher</title><link href="/" rel="alternate"></link><link href="/feeds/all.atom.xml" rel="self"></link><id>/</id><updated>2024-03-24T00:00:00-07:00</updated><entry><title>Guide to Simulating Objects in aÂ Container</title><link href="/posts/2024/Mar/guide-to-objects-container/" rel="alternate"></link><published>2024-03-24T00:00:00-07:00</published><updated>2024-03-24T00:00:00-07:00</updated><author><name>Alex Liebscher</name></author><id>tag:None,2024-03-24:/posts/2024/Mar/guide-to-objects-container/</id><summary type="html">&lt;p&gt;Do smaller objects (like small apple slices) always fit better in a container than larger objects (like big apple&amp;nbsp;slices)?&lt;/p&gt;</summary><content type="html">&lt;p&gt;I was thinking the other day about whether it&amp;#8217;s always more space efficient to pack smaller apple slices in a Tupperware container compared to larger ones. If that were true, and we run into issues fitting things like apple slices in their containers, we should just adopt the habit of cutting those slices&amp;nbsp;smaller.&lt;/p&gt;
&lt;p&gt;Likewise, maybe when packing moving boxes, trying to break down objects, like side tables or ottomans or vacuums, into smaller component pieces in order to fit them into your&amp;nbsp;boxes.&lt;/p&gt;
&lt;p&gt;This had me thinking for an afternoon, until I brought up the question to my partner Kailyn who quickly surmised that this was simply integration in three dimensions! An explanation of this at the end. By then though I had day-dreamed something like a simulation-based computational method for &amp;#8220;throwing&amp;#8221; virtual apple slices in a virtual container and measuring how compact they&amp;nbsp;get.&lt;/p&gt;
&lt;p&gt;I thought it&amp;#8217;d be a good learning experience to try to see this through just a bit, even if I thought I already knew the&amp;nbsp;answer.&lt;/p&gt;
&lt;h2&gt;A Virtual&amp;nbsp;World&lt;/h2&gt;
&lt;p&gt;To simulate throwing apple slices in a container, we need a place to do such throwing. I imagined this would be some virtual world with a physics engine. I started Googling Python physics engines, but realized it&amp;#8217;d probably be easier to try this problem in a web browser. A web browser has much greater visual support than Python, in my opinion. A web browser also runs JavaScript, which I figured would have a ready-to-go physics engine&amp;nbsp;library.&lt;/p&gt;
&lt;h3&gt;Discovering&amp;nbsp;Matter.js&lt;/h3&gt;
&lt;p&gt;A quick search turned up &lt;a href="https://brm.io/matter-js/"&gt;Matter.js&lt;/a&gt;. I decided to keep it simpler with a two-dimensional solution. This being the first time I&amp;#8217;ve built anything physics related, three-dimensions felt like too complicated. Matter.js looked like it&amp;#8217;d be able to build the virtual world I needed to throw apples&amp;nbsp;in.&lt;/p&gt;
&lt;p&gt;I copied over &lt;a href="https://github.com/liabru/matter-js/wiki/Getting-started"&gt;some starter code&lt;/a&gt; and was able to get something running in my local web browser within&amp;nbsp;minutes.&lt;/p&gt;
&lt;p&gt;With this, I had built a very simple two boxes falling to the ground. This is already almost exactly what I&amp;nbsp;envisioned!&lt;/p&gt;
&lt;p&gt;&lt;img data-src="/images/objects-container/step1.gif" width="640px" class="uk-align-center" uk-img/&gt;&lt;/p&gt;
&lt;h3&gt;What does one put in a&amp;nbsp;container?&lt;/h3&gt;
&lt;p&gt;If we&amp;#8217;re going to simulate apple slices, or any objects, falling into the container, we need to create those objects. To do so, I wrote the following function for building some very minimal&amp;nbsp;boxes.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kd"&gt;let&lt;/span&gt; &lt;span class="nx"&gt;box_width&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;325&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;box_height&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;70&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="c1"&gt;// create boxes and bounds&lt;/span&gt;
&lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="nx"&gt;buildBoxes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;count&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;boxes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;let&lt;/span&gt; &lt;span class="nx"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="nx"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="nx"&gt;count&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="kd"&gt;let&lt;/span&gt; &lt;span class="nx"&gt;random_x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;10&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;Math&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;random&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mf"&gt;640&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
        &lt;span class="kd"&gt;let&lt;/span&gt; &lt;span class="nx"&gt;box&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;Matter&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Bodies&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;rectangle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="nx"&gt;random_x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="mf"&gt;40&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="nx"&gt;box_width&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="nx"&gt;box_height&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nx"&gt;restitution&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.05&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;slop&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="p"&gt;);&lt;/span&gt;
        &lt;span class="nx"&gt;Body&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;setDensity&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;box&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
        &lt;span class="nx"&gt;boxes&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;push&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;box&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;boxes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;boxes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;buildBoxes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;7&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I first define a width and height of the &amp;#8220;apple slices&amp;#8221;. Obviously these don&amp;#8217;t mimic the shape of actual cross-sectional apple slice drawings, but it&amp;#8217;s a first&amp;nbsp;approximation.&lt;/p&gt;
&lt;p&gt;In the function, we make an array of &lt;code&gt;boxes&lt;/code&gt;. We add a certain &lt;code&gt;count&lt;/code&gt; of boxes, and to do so, we need an &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; coordinate to put it at. We just randomly put the box on the x-axis of the virtual world, inset a little bit from the edges. And we put each successive box increasingly further down the screen (Matter.js has a &lt;code&gt;(0, 0)&lt;/code&gt; coordinate in the top&amp;nbsp;left).&lt;/p&gt;
&lt;p&gt;We also set a couple properties of the boxes to help them &amp;#8220;bounce&amp;#8221; around like &amp;#8220;actual&amp;#8221; apple slices. Namely, &lt;a href="https://brm.io/matter-js/docs/classes/Body.html#property_restitution"&gt;restitution&lt;/a&gt;, which is basically how elastic objects colliding is, and &lt;a href="https://brm.io/matter-js/docs/classes/Body.html#property_slop"&gt;slop&lt;/a&gt;, which allows objects to &amp;#8220;sink&amp;#8221; into each other upon&amp;nbsp;collision.&lt;/p&gt;
&lt;p&gt;We also set the density of the objects so they collide and fall a bit more realistically. Finally, we &lt;code&gt;push&lt;/code&gt; the newly created box to our&amp;nbsp;array.&lt;/p&gt;
&lt;p&gt;In the end, we create seven of these boxes, because somehow I decided that we usually cut apples into seven&amp;nbsp;slices.&lt;/p&gt;
&lt;p&gt;Now when I add these shapes to the &lt;code&gt;Composite&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;// add all of the bodies to the world&lt;/span&gt;
&lt;span class="nx"&gt;Composite&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;engine&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;world&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[...&lt;/span&gt;&lt;span class="nx"&gt;boxes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;ground&lt;/span&gt;&lt;span class="p"&gt;]);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I get something that looks like the following (although random variation in the &lt;code&gt;x&lt;/code&gt; positioning makes each run a little&amp;nbsp;different):&lt;/p&gt;
&lt;p&gt;&lt;img data-src="/images/objects-container/step2.gif" width="640px" class="uk-align-center" uk-img/&gt;&lt;/p&gt;
&lt;h3&gt;Apples all over the&amp;nbsp;counter&lt;/h3&gt;
&lt;p&gt;The previous step is more like throwing apples all over the counter, instead of in a container. It&amp;#8217;s easy to add new bounds to our virtual world to model a&amp;nbsp;container.&lt;/p&gt;
&lt;p&gt;Simply&amp;nbsp;add:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;wallL&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;Bodies&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;rectangle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;160&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;640&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nx"&gt;isStatic&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;true&lt;/span&gt; &lt;span class="p"&gt;});&lt;/span&gt;
&lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;wallR&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;Bodies&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;rectangle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;640&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;160&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;640&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nx"&gt;isStatic&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;true&lt;/span&gt; &lt;span class="p"&gt;});&lt;/span&gt;

&lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;bounds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;ground&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;wallL&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;wallR&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I found it difficult figuring out the numbers to use as arguments. If we were being more methodical, we might have set parameters at the top of our script to define, for example, the width of the screen to then be used in places like&amp;nbsp;this.&lt;/p&gt;
&lt;p&gt;Adding these bounds, along with the boxes, to the Composite gives us something like&amp;nbsp;this:&lt;/p&gt;
&lt;p&gt;&lt;img data-src="/images/objects-container/step3.gif" width="640px" class="uk-align-center" uk-img/&gt;&lt;/p&gt;
&lt;h2&gt;Measuring how packed the container&amp;nbsp;is&lt;/h2&gt;
&lt;p&gt;Ultimately I&amp;#8217;m interested in measuring how packed our container gets. This way we can vary the dimensions of our apple slices (boxes) and measure the height that all that apple stacks up. I&amp;#8217;m predicting that smaller pieces will pack more densely than larger pieces. To begin, we need to measure the height of the pieces, and then record that&amp;nbsp;height.&lt;/p&gt;
&lt;h3&gt;Trigonemtry and rotated apple&amp;nbsp;slices&lt;/h3&gt;
&lt;p&gt;The first solution I had was to draw a line on top of all the boxes. I wasn&amp;#8217;t sure of the best way to do this. At first I tried to do it with Matter.js Bodies. The goal was to place a constantly flat Body on top of everything, in a way that sort of &amp;#8220;compressed&amp;#8221; the boxes (but actually did not interact with them at&amp;nbsp;all).&lt;/p&gt;
&lt;p&gt;I needed at the very least a &lt;code&gt;y&lt;/code&gt; value of the top-most point of all boxes. The way I approached this was to iterate over each box on every tick of the engine&amp;#8217;s clock, and calculate the heighest corner of each box. Then look for the highest across boxes&amp;nbsp;too.&lt;/p&gt;
&lt;p&gt;It looks like&amp;nbsp;this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kd"&gt;let&lt;/span&gt; &lt;span class="nx"&gt;line_y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="kd"&gt;let&lt;/span&gt; &lt;span class="nx"&gt;hypo&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;Math&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;Math&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;pow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;box_width&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;Math&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;pow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;box_height&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;2&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;span class="kd"&gt;let&lt;/span&gt; &lt;span class="nx"&gt;box_theta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;Math&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;atan&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;box_height&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="nx"&gt;box_width&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;

&lt;span class="nx"&gt;Events&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;on&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;runner&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;tick&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;event&lt;/span&gt; &lt;span class="p"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="kd"&gt;let&lt;/span&gt; &lt;span class="nx"&gt;ys&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;boxes&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;map&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="nx"&gt;box&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="kd"&gt;let&lt;/span&gt; &lt;span class="nx"&gt;box_y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;box&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;position&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="kd"&gt;let&lt;/span&gt; &lt;span class="nx"&gt;box_rads&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;box&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;angle&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

        &lt;span class="kd"&gt;let&lt;/span&gt; &lt;span class="nx"&gt;y_primes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
            &lt;span class="nx"&gt;box_y&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nx"&gt;hypo&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nb"&gt;Math&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;sin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;box_rads&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="nx"&gt;box_theta&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mf"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="nx"&gt;box_y&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nx"&gt;hypo&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nb"&gt;Math&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;sin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;box_rads&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nx"&gt;box_theta&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mf"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="nx"&gt;box_y&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nx"&gt;hypo&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nb"&gt;Math&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;sin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;box_rads&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="nx"&gt;box_theta&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mf"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="nx"&gt;box_y&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nx"&gt;hypo&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nb"&gt;Math&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;sin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;box_rads&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nx"&gt;box_theta&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mf"&gt;2&lt;/span&gt;
        &lt;span class="p"&gt;];&lt;/span&gt;

        &lt;span class="kd"&gt;let&lt;/span&gt; &lt;span class="nx"&gt;new_y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;Math&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(...&lt;/span&gt;&lt;span class="nx"&gt;y_primes&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;new_y&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="p"&gt;})&lt;/span&gt;

    &lt;span class="nx"&gt;line_y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;Math&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(...&lt;/span&gt;&lt;span class="nx"&gt;ys&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;

&lt;span class="p"&gt;});&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;There might be a more efficient way of doing this, but for this article it works just fine and is easy to understand. To walk through it though, first off I want to establish a new variable for our &lt;code&gt;line_y&lt;/code&gt;. This will hold the value of the top-most point of our apple slices. I also precompute two values: the &lt;code&gt;hypo&lt;/code&gt;tenuse of each box, and the angle from the mid-point of the box to the corner (&lt;span class="math"&gt;\(r\)&lt;/span&gt;). These requires some basic trigonometry, which I &lt;em&gt;always&lt;/em&gt; struggle with and &lt;em&gt;always&lt;/em&gt; end up going to Kailyn and the internet for&amp;nbsp;help.&lt;/p&gt;
&lt;p&gt;Then I have an event handler attached to &lt;code&gt;runner&lt;/code&gt;. This says, on each virtual time tick of the runner, run the following&amp;nbsp;function.&lt;/p&gt;
&lt;p&gt;In that anonymous function, I start off by creating a variable to hold our &lt;code&gt;ys&lt;/code&gt; values. To populate this array, we iterate over each &lt;code&gt;box&lt;/code&gt; in &lt;code&gt;boxes&lt;/code&gt;. For each box, I store it&amp;#8217;s current &lt;code&gt;y&lt;/code&gt; value, or the distance from the top of the virtual environment to the mid-point of the box. I also store the angle of the box (&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;), in&amp;nbsp;radians.&lt;/p&gt;
&lt;p&gt;The most math-y part of this all is finally here. I am calculating the y-value for each of the box&amp;#8217;s four corners. That way I can determine which one is the highest. To do this, I add the mid-point of the box&amp;#8217;s &lt;code&gt;y&lt;/code&gt; value to the vertical distance between that mid-point and each corner (&lt;span class="math"&gt;\(h\)&lt;/span&gt;). This distance is dependent on the rotation of the box (&lt;span class="math"&gt;\(\theta + r\)&lt;/span&gt;). These calculations correspond to the following&amp;nbsp;illustration:&lt;/p&gt;
&lt;p&gt;&lt;img data-src="/images/objects-container/illustration1.gif" width="640px" class="uk-align-center" uk-img/&gt;&lt;/p&gt;
&lt;p&gt;You&amp;#8217;ll notice how we&amp;#8217;re taking the minimum. Again, this is because the 0 of the y-axis is at the top of the&amp;nbsp;screen.&lt;/p&gt;
&lt;h3&gt;Visualizing the highest&amp;nbsp;point&lt;/h3&gt;
&lt;p&gt;To help debug and also as a nice visual aid, I needed a line of some sort that tracks this changing &lt;code&gt;line_y&lt;/code&gt; computations. I tried to do this with Matter.js Bodies, but that didn&amp;#8217;t work. I think those shapes tend to be reserved for objects that have some collision possibility. My line technically won&amp;#8217;t collide with anything. So in that case, I found a way to draw on &lt;a href="https://brm.io/matter-js/docs/classes/Render.html#property_canvas"&gt;the &lt;span class="caps"&gt;HTML&lt;/span&gt; canvas&lt;/a&gt; generated by&amp;nbsp;Matter.js.&lt;/p&gt;
&lt;p&gt;I&amp;#8217;ve had to change the dimensions of the box just a little so that the boxes fall nicely and aren&amp;#8217;t overlapping when they&amp;#8217;re drawn. To display a line at the top of the boxes though we&amp;nbsp;need:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nx"&gt;Events&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;on&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;render&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;afterRender&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="kd"&gt;const&lt;/span&gt; &lt;span class="nx"&gt;ctx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;render&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;context&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

    &lt;span class="nx"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;lineWidth&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;4&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="nx"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;strokeStyle&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;red&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="nx"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;beginPath&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt; &lt;span class="c1"&gt;// Start a new path&lt;/span&gt;
    &lt;span class="nx"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;moveTo&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;line_y&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="nx"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;lineTo&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;640&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;line_y&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="nx"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;stroke&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Hopefully this is pretty self-explanatory. All together, we have the&amp;nbsp;following:&lt;/p&gt;
&lt;p&gt;&lt;img data-src="/images/objects-container/step4.gif" width="640px" class="uk-align-center" uk-img/&gt;&lt;/p&gt;
&lt;p&gt;There are two things remaining though: the repeated simulation aspect, and the&amp;nbsp;recording.&lt;/p&gt;
&lt;h2&gt;Repeating the simulation ad&amp;nbsp;infinitum&lt;/h2&gt;
&lt;p&gt;My goal is to compare how well small pieces fit compared to large pieces. It would be helpful if I could simulate throwing apple slices into the container many, many times to collect&amp;nbsp;evidence.&lt;/p&gt;
&lt;p&gt;To do this, I set an arbitrary threshold to determine when all the boxes have settled, and then we just redraw the canvas with new boxes. To really take advantage of the fact that this is a computer simulation, we&amp;#8217;ll also increase the speed of the physics engine (i.e. make time go&amp;nbsp;faster).&lt;/p&gt;
&lt;p&gt;Back in our tick event listener, right after we compute the &lt;code&gt;ys&lt;/code&gt; we&amp;#8217;ll figure out if the line has&amp;nbsp;settled:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;tick_delay&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mf"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;line_y&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="nx"&gt;prev_line_y&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="nx"&gt;eps&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="c1"&gt;// the line has stopped&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="nx"&gt;tick_delay&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;tick_max_delay&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nx"&gt;tick_delay&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="mf"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="nx"&gt;prev_line_y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;line_y&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="c1"&gt;// after this comes: line_y = Math.min(...ys);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Above of the tick event listener, we define the delay variables&amp;nbsp;here.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kd"&gt;let&lt;/span&gt; &lt;span class="nx"&gt;tick_max_delay&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;120&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;engine&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;timing&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;timeScale&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="kd"&gt;let&lt;/span&gt; &lt;span class="nx"&gt;tick_delay&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;tick_max_delay&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="kd"&gt;let&lt;/span&gt; &lt;span class="nx"&gt;eps&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0001&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We&amp;#8217;ll get to it in a moment, but the &lt;code&gt;engine.timing.timeScale&lt;/code&gt; is basically a factor determining how fast time progresses. Less than 1 is like slow motion, greater than 1 is time sped up. The 120 here is just an arbitrary number I came up with through trial and error. It basically controls how often to check if the line has settled. It&amp;#8217;s the number of frames/ticks that&amp;nbsp;pass.&lt;/p&gt;
&lt;p&gt;Back in the logic, the code first checks if &lt;code&gt;tick_delay&lt;/code&gt; is 0. If it is, which means it&amp;#8217;s time to check how settled the boxes are, the code looks to see if the difference between the red line&amp;#8217;s y-value now, compared to the last time we checked, is less than &lt;code&gt;eps&lt;/code&gt; units. If it is, that would indicate the line has settled and we should restart the simulation. If it&amp;#8217;s not yet time to check the line, we just decrease the tick counter until it is&amp;nbsp;time.&lt;/p&gt;
&lt;p&gt;Now, where I commented &lt;code&gt;// the line has stopped&lt;/code&gt; we&amp;#8217;ll fill in&amp;nbsp;with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nx"&gt;Composite&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;remove&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;engine&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;world&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;boxes&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="nx"&gt;line_y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;prev_line_y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="nx"&gt;boxes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;buildBoxes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;7&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="nx"&gt;Composite&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;engine&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;world&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;boxes&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This is just code I already wrote. First I remove all the existing boxes from the engine&amp;#8217;s virtual world. We make sure not to remove the bounds. Then we reset the line&amp;#8217;s position. Then we draw new boxes, and finally we add those new boxes into the&amp;nbsp;world.&lt;/p&gt;
&lt;p&gt;Altogether it looks like the&amp;nbsp;following:&lt;/p&gt;
&lt;p&gt;&lt;img data-src="/images/objects-container/step5.gif" width="640px" class="uk-align-center" uk-img/&gt;&lt;/p&gt;
&lt;p&gt;It might not be exactly perfect, or exactly fine-tuned, but it&amp;#8217;s good&amp;nbsp;enough.&lt;/p&gt;
&lt;h3&gt;Let&amp;#8217;s do the time&amp;nbsp;warp&lt;/h3&gt;
&lt;p&gt;I said we&amp;#8217;d get back to time scale part. In Matter.js, you can control how quickly time passes with the &lt;code&gt;timing&lt;/code&gt; option of the &lt;code&gt;engine&lt;/code&gt;. So, for example, after we create our engine, we&amp;nbsp;have:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;engine&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;Engine&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;create&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;span class="nx"&gt;engine&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;timing&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;timeScale&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;4&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This now speeds up the simulation to 4x speed! Here&amp;#8217;s what that looks&amp;nbsp;like:&lt;/p&gt;
&lt;p&gt;&lt;img data-src="/images/objects-container/step6.gif" width="640px" class="uk-align-center" uk-img/&gt;&lt;/p&gt;
&lt;h3&gt;Recording the height for later&amp;nbsp;analysis&lt;/h3&gt;
&lt;p&gt;Just before we remove the boxes from the screen, I wanted to record the value of &lt;code&gt;line_y&lt;/code&gt; so that we could collect some data. Remember, this is all running right in an &lt;span class="caps"&gt;HTML&lt;/span&gt; file so we don&amp;#8217;t have any file writing capabilities. That&amp;#8217;s okay though, for my intents I&amp;#8217;ll just write the data to the &lt;span class="caps"&gt;DOM&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Above our script tags in the &lt;span class="caps"&gt;HTML&lt;/span&gt; body itself, I added a &lt;code&gt;table&lt;/code&gt; container: &lt;code&gt;&amp;lt;table id="data"&amp;gt;&amp;lt;/table&amp;gt;&lt;/code&gt;. And then in the &lt;span class="caps"&gt;JS&lt;/span&gt;, just before removing the boxes, I&amp;nbsp;added:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nx"&gt;data_counter&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mf"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="nb"&gt;document&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;getElementById&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;data&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;innerHTML&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&amp;lt;tr&amp;gt;&amp;lt;td&amp;gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nx"&gt;data_counter&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;480&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="nb"&gt;Math&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;round&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;line_y&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mf"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mf"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Super easy to add these numbers as data, which we can just copy/paste into a&amp;nbsp;spreadsheet.&lt;/p&gt;
&lt;p&gt;&lt;img data-src="/images/objects-container/step7.gif" width="640px" class="uk-align-center" uk-img/&gt;&lt;/p&gt;
&lt;h2&gt;Empirically packing apple slice&amp;nbsp;containers&lt;/h2&gt;
&lt;p&gt;The question now is, if we simulate some number of throws of large apple slices and some number of throws of small apple slices, will the smaller pieces on average condense further in the&amp;nbsp;container?&lt;/p&gt;
&lt;p&gt;Our big apple slices are boxes with width 325 units and height 70 units, and we draw 7 of them. Thus, our small slices (let&amp;#8217;s just say we cut the big ones in half) will be 163 units by 70 units, and we&amp;#8217;ll draw 14 of&amp;nbsp;them.&lt;/p&gt;
&lt;p&gt;How many simulations should we run of each? First, I need to decide what will provide me with evidence that smaller slices pack better. I don&amp;#8217;t actually know if the distribution of &lt;code&gt;y&lt;/code&gt; values will be normally distributed. So, in that case, I&amp;#8217;ll run two tests to empirically decide if smaller pieces fit better. I&amp;#8217;ll run a one-sided &lt;i&gt;t&lt;/i&gt;-test with unequal variances, and a non-parametric one-sided Mann-Whitney&amp;nbsp;test.&lt;/p&gt;
&lt;p&gt;I want to collect enough data so that I&amp;#8217;m 90% sure that if smaller pieces actually do pack better that we&amp;#8217;ll identify that difference. Simulations are relatively cheap; they just take time, so I can afford to increase this percentage (i.e. power) fairly&amp;nbsp;high.&lt;/p&gt;
&lt;p&gt;Second, I want to be sure not have a false positive here since I only want to do this test once. I&amp;#8217;ll set my Type I error threshold to 0.01 (i.e. 1% of tests will be false&amp;nbsp;positives).&lt;/p&gt;
&lt;p&gt;I&amp;#8217;m going to consider two different effect sizes before computing the required sample size to reject our null hypotheses. These include the smallest effect size I&amp;#8217;m interested in detecting and also the expected effect size. A very wonderful resource that I leaned heavily on for this section is DaniÃ«l Lakens&amp;#8217; &lt;a href="https://lakens.github.io/statistical_inferences/08-samplesizejustification.html"&gt;Improving Your Statistical Inferences&lt;/a&gt; online&amp;nbsp;textbook.&lt;/p&gt;
&lt;h3&gt;Determining effect sizes and sample&amp;nbsp;sizes&lt;/h3&gt;
&lt;p&gt;I figure the smallest effect size of interest here (the standardized difference between two groups) is around &lt;i&gt;d&lt;/i&gt;=0.5. In other words, the average difference between how packed large and small boxes will get is around one-half of a standard&amp;nbsp;deviation.&lt;/p&gt;
&lt;p&gt;What I expect though is that small boxes will condense quite a bit more than large boxes. I&amp;#8217;m going to say that the average difference of the means will be roughly a whole standard&amp;nbsp;deviation.&lt;/p&gt;
&lt;p&gt;Assuming that the data are log-normally distributed (very slightly), using a Mann-Whitney &lt;i&gt;U&lt;/i&gt; test I figure I need about 160 observations per condition to detect a difference between these&amp;nbsp;distributions.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;R&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="m"&gt;2000&lt;/span&gt;
&lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="m"&gt;160&lt;/span&gt;
&lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;numeric&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;numeric&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;for &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;large&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;rlnorm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nf"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;400&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="m"&gt;0.7&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;small&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;rlnorm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nf"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;300&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="m"&gt;0.7&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# t &amp;lt;- t.test(large, small,&lt;/span&gt;
    &lt;span class="c1"&gt;#                alternative = &amp;quot;g&amp;quot;,&lt;/span&gt;
    &lt;span class="c1"&gt;#                var.equal = F)&lt;/span&gt;

    &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;wilcox.test&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;large&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;small&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alternative&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;g&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;p.value&lt;/span&gt;
    &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;as.numeric&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;effectsize&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="nf"&gt;r_to_d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;effectsize&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="nf"&gt;effectsize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;)))[&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="nf"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="code-output"&gt;
    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
0.09244 0.38156 0.46996 0.47423 0.56070 1.00036
&lt;/pre&gt;

&lt;p&gt;As we can see, our toy data has a simulated average effect size of &lt;i&gt;d&lt;/i&gt;=0.47, which is close enough to the smallest effect size of interest I want to capture. If the real effect size were smaller, it wouldn&amp;#8217;t be interesting to us, and if it&amp;#8217;s bigger we&amp;#8217;ll catch it with this sample&amp;nbsp;size.&lt;/p&gt;
&lt;p&gt;These sample sizes with this average difference would give us 90% power&amp;nbsp;too:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="m"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="code-output"&gt;0.9&lt;/pre&gt;

&lt;p&gt;About 40-45 samples per condition would be needed to detect a difference in Cohen&amp;#8217;s &lt;i&gt;d&lt;/i&gt; of&amp;nbsp;1.0.&lt;/p&gt;
&lt;p&gt;With an estimated Cohen&amp;#8217;s &lt;i&gt;d&lt;/i&gt; of 0.5 for a &lt;i&gt;t&lt;/i&gt;-test, the required sample size to detect a difference with 90% power is 95 observations per sample (for both large boxes and small boxes). For a Cohen&amp;#8217;s &lt;i&gt;d&lt;/i&gt; of 1.0, I&amp;#8217;d need 32 observations per&amp;nbsp;sample.&lt;/p&gt;
&lt;p&gt;Unsurprisingly, the nonparametric test requires a greater sample size to detect a&amp;nbsp;difference.&lt;/p&gt;
&lt;p&gt;Let&amp;#8217;s go simulate enough throws for have enough power to detect our smaller effect&amp;nbsp;size.&lt;/p&gt;
&lt;h3&gt;Exhausting my arm with all these&amp;nbsp;throws&lt;/h3&gt;
&lt;p&gt;Our simulation engine is great for throwing this many apple slices into the container. In total, I threw 320 apples (1,120 big slices, and 2,240 small slices). At $1.29 an apple (&lt;a href="https://www.traderjoes.com/home/products/pdp/honeycrisp-apples-093872"&gt;thanks TJs&lt;/a&gt;), that&amp;#8217;s $412 worth of&amp;nbsp;apples.&lt;/p&gt;
&lt;p&gt;Let&amp;#8217;s plot the distributions of &lt;code&gt;y&lt;/code&gt; values.&lt;/p&gt;
&lt;p&gt;&lt;img data-src="/images/objects-container/distribution.png" width="640px" alt="" class="uk-align-center" uk-img&gt;&lt;/p&gt;
&lt;p&gt;Already it looks clear that smaller boxes pack better than larger boxes, but let&amp;#8217;s run our tests to be&amp;nbsp;sure.&lt;/p&gt;
&lt;h3&gt;Do smaller objects fit better than larger&amp;nbsp;ones?&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;wilcox.test&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;line_y&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="n"&gt;experiment&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;results&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;alternative&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;g&amp;quot;&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="code-output"&gt;
data:  line_y by experiment
W = 22785, p-value &lt; 2.2e-16
&lt;/pre&gt;

&lt;p&gt;and the stricter&amp;nbsp;t-test:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;t.test&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;line_y&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="n"&gt;experiment&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;results&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;alternative&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;g&amp;quot;&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="code-output"&gt;
data:  line_y by experiment
t = 12.108, df = 251.73, p-value &lt; 2.2e-16
&lt;/pre&gt;

&lt;p&gt;Hardly a contest at all. Smaller boxes packed better than larger boxes, and there&amp;#8217;s almost no chance that this was a mistake&amp;nbsp;finding.&lt;/p&gt;
&lt;p&gt;Lastly, through induction, I&amp;#8217;ll make the generalized claim that yes, smaller objects will on average pack better in a container than larger&amp;nbsp;objects.&lt;/p&gt;
&lt;h2&gt;Isn&amp;#8217;t this just&amp;nbsp;integration?&lt;/h2&gt;
&lt;p&gt;We really didn&amp;#8217;t &lt;em&gt;need&lt;/em&gt; to do any of this work in order to get an answer. What we&amp;#8217;re doing is basically approximating the surface area of our apple slices more and more accurately the smaller the slice we have. This is akin to a &lt;a href="https://en.wikipedia.org/wiki/Riemann_sum"&gt;Reimann sum&lt;/a&gt; in two dimensions. For apple slices to be packed well in the container, that implies there&amp;#8217;s no space in between slices. If there&amp;#8217;s no space, then the question is just, how much apple is there? We&amp;#8217;re working in two dimensions, so that can also be worded, what is the area of the apple&amp;nbsp;slices?&lt;/p&gt;
&lt;p&gt;If we knew of a function that, given some &lt;span class="math"&gt;\(x\)&lt;/span&gt; or &lt;span class="math"&gt;\(y\)&lt;/span&gt; value, would tell us what portion under that curve was apple, we would need to sum all of that function across our &lt;span class="math"&gt;\(x\)&lt;/span&gt; range and again over all the &lt;span class="math"&gt;\(y\)&lt;/span&gt; range. If that function is &lt;span class="math"&gt;\(g(x, y)\)&lt;/span&gt; then, analytically speaking, this makes the Riemann sum &lt;a href="https://en.wikipedia.org/wiki/Riemann_sum#Connection_with_integration"&gt;equivalent&lt;/a&gt; to definite Riemann&amp;nbsp;integration:&lt;/p&gt;
&lt;div class="math"&gt;$$
\int_{y_{min}}^{y_{max}} \int_{x_{min}}^{x_{max}} g(x, y)\enspace dx dy = \lim_{\Delta x, \Delta y \to 0} \sum_{j = 1}^n \sum_{i = 1}^n g(x_{ij}, y_{ij}) \Delta x_i \Delta y_j
$$&lt;/div&gt;
&lt;p&gt;If &lt;span class="math"&gt;\(\Delta x\)&lt;/span&gt; or &lt;span class="math"&gt;\(\Delta y\)&lt;/span&gt; are large, then this is a poor approximation of the actual &lt;span class="math"&gt;\(g(x, y)\)&lt;/span&gt; since there&amp;#8217;s so much space between the curve and the rectangles. However they are small, perhaps infinitely small, then we get a very good approximation of the curve (i.e. a good approximation of how much surface area the apple slices&amp;nbsp;are).&lt;/p&gt;
&lt;p&gt;If we step back a moment, we&amp;#8217;ll actually realize then that small is better, although the improvements are decreasing as we reduce the slice size. At a certain point, especially in the real world, improvements in approximating the surface area function just aren&amp;#8217;t worth the decrease in size necessary. We can cut the apple slices smaller and smaller, and yes they will pack into the container better and better, but eventually you either A) won&amp;#8217;t be able to cut any smaller or B) will turn your apple into&amp;nbsp;pulp.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="meta"></category><category term="javascript"></category><category term="physics"></category></entry><entry><title>Exploring TheÂ Iliad</title><link href="/posts/2024/Mar/the-iliad/" rel="alternate"></link><published>2024-03-04T00:00:00-08:00</published><updated>2024-03-04T00:00:00-08:00</updated><author><name>Alex Liebscher</name></author><id>tag:None,2024-03-04:/posts/2024/Mar/the-iliad/</id><summary type="html">&lt;p&gt;Analyzing The Iliad of Homer through a computational&amp;nbsp;lens&lt;/p&gt;</summary><content type="html">&lt;p&gt;Lately I&amp;#8217;ve been reading &lt;a href="https://en.wikipedia.org/wiki/Iliad"&gt;&lt;em&gt;The Iliad&lt;/em&gt;&lt;/a&gt; of Homer. As I was reading, the idea came to me of analyzing the text of the epic, just for fun. I kept that idea in my head a bit longer to mull it over as I read, to see what things I might want to&amp;nbsp;analyze.&lt;/p&gt;
&lt;p&gt;I&amp;#8217;ve noticed a number of quirks about the text that I thought I might be able to study in more depth. What I&amp;#8217;ve ended up doing is really just an exploration of the&amp;nbsp;book.&lt;/p&gt;
&lt;p&gt;To start, there are some low hanging fruit in terms of quantifying what I see in the book. For example, the number of words in each of the 24 books that &lt;em&gt;The Iliad&lt;/em&gt; is separated (by editors, not Homer himself)&amp;nbsp;into.&lt;/p&gt;
&lt;p&gt;The characters are diverse and numerous in the epic, and so I&amp;#8217;ve been dreaming of some sort of social network analysis. The data (i.e. the text) isn&amp;#8217;t &amp;#8220;clean&amp;#8221; per-se, but there are some quick things we can do to make it easier to&amp;nbsp;analyze.&lt;/p&gt;
&lt;p&gt;There&amp;#8217;s also an interesting honorific system used in throughout the story. For example, often warriors and kings are contextualized through their father (i.e. &amp;#8220;Achilles, son of Peleus&amp;#8221;). This patrilineality is interesting and new to me, so I want to look into it a bit&amp;nbsp;deeper.&lt;/p&gt;
&lt;p&gt;Another quirk of the epic, which I suspect is due to the story originally being an &lt;a href="https://en.wikipedia.org/wiki/Oral_storytelling"&gt;oral tale&lt;/a&gt;, is the repetition of certain passages. This appears to happen when someone is relaying a message from one person to another. These messengers (sometimes actual heralds, other times kings themselves) often repeat verbatim what was told to them. There are many things to be impressed by here, and it&amp;#8217;d be fascinating to describe this repetition&amp;nbsp;numerically.&lt;/p&gt;
&lt;p&gt;One thing which I&amp;#8217;m not analyzing here, but might in another post, is the expression of leadership in the epic. There are many leaders throughout the tale, and I&amp;#8217;m finding it interesting to think about how Homer portrays strong&amp;nbsp;leadership.&lt;/p&gt;
&lt;p&gt;This post is really just an exploration of the epic. It&amp;#8217;s not unforgivingly systematic, so I&amp;#8217;ll weave in some lessons and mistakes&amp;nbsp;throughout.&lt;/p&gt;
&lt;p&gt;Overall, I&amp;#8217;ll start with a quick descriptive look at the most surface level anatomy of the book. I&amp;#8217;ll explore a little data art, and slowly move deeper in the analysis to answer some of the curiosities I mention&amp;nbsp;above.&lt;/p&gt;
&lt;h2&gt;Analysis&amp;nbsp;Setup&lt;/h2&gt;
&lt;p&gt;The first step is to load in some packages for analyzing text. Text analysis in R feels a bit clunky and underdeveloped, so I&amp;#8217;m loading up several packages that are pretty&amp;nbsp;similar.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tidyverse&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# text analysis&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;quanteda&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;quanteda.textplots&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;quanteda.textstats&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tidytext&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# visualization&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;patchwork&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;scales&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;glue&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ggtext&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;showtext&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I found a few photos of ancient Grecian urns (known as amphora), and used the native Digital Color Meter on the Mac to sample some of the colors off these urns. This gives us a nice themed color palette to work&amp;nbsp;with.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.britishmuseum.org/collection/object/G_1836-0224-127" target="_blank"&gt;&lt;img data-src="/images/the-iliad/amphora.png" class="uk-align-center" width="70%" height="" alt="Black-figured pottery amphora. Achilles slaying Penthesilea." uk-img&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class="caption"&gt;
    Black-figured pottery amphora. Achilles slaying Penthesilea.
&lt;/div&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;greek_colors&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="nf"&gt;rgb&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;18&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;18&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;18&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;maxColorValue&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;255&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="c1"&gt;# black&lt;/span&gt;
    &lt;span class="nf"&gt;rgb&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;60&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;68&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;78&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;maxColorValue&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;255&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="c1"&gt;# gray blue&lt;/span&gt;
    &lt;span class="nf"&gt;rgb&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;86&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;53&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;35&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;maxColorValue&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;255&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="c1"&gt;# dark brown&lt;/span&gt;
    &lt;span class="nf"&gt;rgb&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;155&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;75&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;maxColorValue&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;255&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="c1"&gt;# dark orange&lt;/span&gt;
    &lt;span class="nf"&gt;rgb&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;200&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;110&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;55&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;maxColorValue&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;255&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="c1"&gt;# orange&lt;/span&gt;
    &lt;span class="nf"&gt;rgb&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;230&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;204&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;134&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;maxColorValue&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;255&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="c1"&gt;# cream&lt;/span&gt;
    &lt;span class="nf"&gt;rgb&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;137&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;157&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;107&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;maxColorValue&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;255&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# green&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;show_col&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;greek_colors&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="code-output"&gt;
&lt;img data-src="/images/the-iliad/greek-colors.png" class="uk-align-left" width="60%" height="" alt="Greek colors" uk-img&gt;
&lt;/pre&gt;

&lt;p&gt;To bolster the style of my findings even more, I&amp;#8217;ve found a nice serif font called &lt;a href="https://fonts.google.com/specimen/Newsreader"&gt;Newsreader&lt;/a&gt; which I&amp;#8217;d like to use in my plots. I was hoping for a serif that reflected the sharp lines of ancient Greek and Cuniform a bit better, but this works well for now. We&amp;#8217;re using &lt;a href="https://github.com/yixuan/showtext"&gt;showtext&lt;/a&gt; here in order to pull in new fonts. I find this package is great for that, and any font restrictions I face really boil down to ggplot&amp;nbsp;limitations.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;font_add_google&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Newsreader&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;newsreader&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;showtext_auto&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Lastly, I&amp;#8217;ll pull this all into a single theme that we can add right onto plots for quick&amp;nbsp;styling.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;theme_iliad&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="nf"&gt;theme_light&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
    &lt;span class="nf"&gt;theme&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;panel.grid&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;element_blank&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
        &lt;span class="n"&gt;panel.border&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;element_blank&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;

        &lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;element_text&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;greek_colors&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;family&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;newsreader&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;

        &lt;span class="n"&gt;axis.text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;element_text&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;10&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
        &lt;span class="n"&gt;title&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;element_text&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;face&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;bold&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;14&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
        &lt;span class="n"&gt;plot.caption&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;element_text&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;gray70&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;face&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;plain&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Loading &lt;em&gt;The&amp;nbsp;Iliad&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;First off, if we&amp;#8217;re going to analyze the book we need the digitized text. Luckily, this is very easy to accomplish by heading to Project Gutenberg. My own physical copy is a &lt;a href="https://en.wikipedia.org/wiki/Samuel_Butler_(novelist)"&gt;Samuel Butler&lt;/a&gt; translation (1942) and I was able to find &lt;a href="https://www.gutenberg.org/ebooks/2199"&gt;a digital copy&lt;/a&gt; as&amp;nbsp;well.&lt;/p&gt;
&lt;p&gt;Looking over the text though it&amp;#8217;s not &lt;em&gt;exactly&lt;/em&gt; the same as mine. I notice some minor punctuation differences mostly. The digital version uses Roman names which we revert to Greek in a&amp;nbsp;second.&lt;/p&gt;
&lt;p&gt;After downloading the text file though, I manually removed the preamble and post-script. This leaves me with primarily just the epic&amp;#8217;s&amp;nbsp;text.&lt;/p&gt;
&lt;p&gt;Here&amp;#8217;s a sample of what we&amp;nbsp;see:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;read_file&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;butler.txt&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;substr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;400&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;...&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="code-output"&gt;
BOOK I.


      The quarrel between Agamemnon and AchillesâAchilles withdraws
      from the war, and sends his mother Thetis to ask Jove to help the
      TrojansâScene between Jove and Juno on Olympus.

      Sing, O goddess, the anger of Achilles son of Peleus, that
      brought countless ills upon the Achaeans. Many a brave soul did
      it send hurrying down to Hades, and many a he ...
&lt;/pre&gt;

&lt;p&gt;There is the famous and &lt;a href="https://lithub.com/a-master-class-in-words-on-the-vitality-and-vividness-of-the-iliads-opening-lines/"&gt;beautiful opening line&lt;/a&gt; (which has many translations) after the book&amp;nbsp;summary.&lt;/p&gt;
&lt;p&gt;As mentioned, this particular transcription of Samuel Butler&amp;#8217;s translation uses the Roman names of some of the characters. My translation was written with the Greek names, and so I&amp;#8217;d like to &lt;a href="https://www.theoi.com/articles/roman-gods-vs-greek-gods-know-the-difference/"&gt;revert&lt;/a&gt; to those using &lt;code&gt;stringr&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;str_replace_all&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Jove&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Zeus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;str_replace_all&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Saturn&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Cronus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;str_replace_all&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Juno&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Hera&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;str_replace_all&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Neptune&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Poseidon&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;str_replace_all&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Minerva&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Athene&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;str_replace_all&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Diana&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Artemis&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;str_replace_all&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Venus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Aphrodite&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;str_replace_all&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Mars&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Ares&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;str_replace_all&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Vulcan&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Hephaestus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;str_replace_all&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Mercury&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Hermes&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;str_replace_all&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Ulysses&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Odysseus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Although Homer did not intend the epic to be split into chapters or books, this is how most modern copies are divided. This organization is helpful so we&amp;#8217;ll split the text into its&amp;nbsp;books.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;books&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;str_split_1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;BOOK [XIV]+\\.&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# filter out some remaining empty books&lt;/span&gt;
&lt;span class="n"&gt;books&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;books&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;books&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Finally, each book begins with a quick book summary. We want to remove those so that the text is not polluted by&amp;nbsp;them.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;books_ps&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;books&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;\&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;book&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;paragraphs&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;book&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
            &lt;span class="nf"&gt;str_split_1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;\r\n\r\n&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
            &lt;span class="nf"&gt;str_squish&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="n"&gt;paragraphs&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;paragraphs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;paragraphs&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

        &lt;span class="n"&gt;paragraphs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="nf"&gt;length&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;paragraphs&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
            &lt;span class="nf"&gt;paste0&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;collapse&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;\n\n&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;})&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We&amp;#8217;ll make use of &lt;code&gt;quanteda&lt;/code&gt; next and put the books into a corpus&amp;nbsp;object.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;book_names&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;str_match_all&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;BOOK [XIV]+&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)[[&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]][,&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;key_text&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;corpus&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="nf"&gt;as.character&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;books_ps&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;docnames&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;book_names&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This then gives us some nice helper functions, for example we can see how many words are in each&amp;nbsp;book:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key_text&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="code-output"&gt;
Corpus consisting of 24 documents, showing 24 documents:

       Text Types Tokens Sentences
     BOOK I  1313   6687       225
    BOOK II  1893   9121       299
   BOOK III  1066   4624       147
    BOOK IV  1300   5685       166
     BOOK V  1702   9675       295
    BOOK VI  1239   5676       164
   BOOK VII  1109   4977       142
  BOOK VIII  1335   5915       175
    BOOK IX  1449   7722       225
     BOOK X  1241   6237       197
    BOOK XI  1740   9449       237
   BOOK XII  1157   5270       125
  BOOK XIII  1708   9355       231
   BOOK XIV  1277   5840       158
    BOOK XV  1589   8245       236
   BOOK XVI  1776   9888       263
  BOOK XVII  1458   8071       211
 BOOK XVIII  1454   6723       187
   BOOK XIX  1057   4541       123
    BOOK XX  1246   5611       164
   BOOK XXI  1426   6905       206
  BOOK XXII  1288   6087       151
 BOOK XXIII  1737  10054       284
  BOOK XXIV  1545   9398       264
&lt;/pre&gt;

&lt;h2&gt;Scratching the Surface of the&amp;nbsp;Epic&lt;/h2&gt;
&lt;p&gt;First, I want to visualize the above word count summary, to get a feel for how each book compares in terms of&amp;nbsp;length:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key_text&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;mutate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;factor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;levels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;rev&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;book_names&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;ggplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;aes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Tokens&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="nf"&gt;coord_flip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ylim&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kc"&gt;NA&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="nf"&gt;geom_col&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fill&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;greek_colors&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;width&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;4&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="m"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="nf"&gt;geom_text&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;aes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;number&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Tokens&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;big.mark&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;,&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;200&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hjust&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;white&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;family&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;newsreader&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="nf"&gt;scale_y_continuous&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;label_number&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;big.mark&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;,&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="nf"&gt;labs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;element_blank&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;element_blank&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;title&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Words per Book&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;caption&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;liebscher.github.io&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="n"&gt;theme_iliad&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="nf"&gt;theme&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;axis.text.x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;element_blank&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
        &lt;span class="n"&gt;axis.ticks.x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;element_blank&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="code-output"&gt;
&lt;img data-src="/images/the-iliad/words-per-book.png" class="uk-align-left" width="70%" height="" alt="Words per Book of The Iliad" uk-img&gt;
&lt;/pre&gt;

&lt;p&gt;Next we&amp;#8217;ll break the books into&amp;nbsp;tokens.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;text_tokens&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;key_text&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;tokens&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;And just out of curiosity, we&amp;#8217;ll take a look at word frequency across all books. Before doing so we&amp;#8217;ll remove all punctuation, numbers, and common stopwords so that our output consists primarily of &amp;#8220;content&amp;#8221; words, or words that will be more interesting to think about compared to &amp;#8220;and&amp;#8221; or &amp;#8220;to&amp;#8221; (i.e. &lt;a href="https://en.wikipedia.org/wiki/Stop_word"&gt;stopwords&lt;/a&gt;).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;token_freq&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;text_tokens&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;tokens&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;remove_punct&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;remove_numbers&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;T&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;tokens_remove&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;stopwords&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;dfm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tolower&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;F&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;textstat_frequency&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;30&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;token_freq&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;mutate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;if_else&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;frequency&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nf"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;frequency&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
            &lt;span class="nf"&gt;glue&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;{frequency} total occurances&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nf"&gt;as.character&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;frequency&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;ggplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;aes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;frequency&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nf"&gt;fct_reorder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;feature&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;frequency&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="nf"&gt;coord_cartesian&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xlim&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;1200&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="nf"&gt;geom_segment&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;aes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;xend&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;frequency&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;gray90&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="nf"&gt;geom_point&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;greek_colors&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="nf"&gt;geom_text&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="nf"&gt;aes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
        &lt;span class="n"&gt;family&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;newsreader&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hjust&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nudge_x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;greek_colors&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="nf"&gt;labs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;element_blank&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;element_blank&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
        &lt;span class="n"&gt;title&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Most Common Words in The Iliad&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;caption&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;liebscher.github.io&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="n"&gt;theme_iliad&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="nf"&gt;theme&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;axis.text.x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;element_blank&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="code-output"&gt;
&lt;img data-src="/images/the-iliad/most-common-words.png" class="uk-align-left" width="80%" height="" alt="Most Common Words in The Iliad" uk-img&gt;
&lt;/pre&gt;

&lt;p&gt;Knowing the text, these frequencies make sense. For example &amp;#8220;son&amp;#8221; is said often as part of the noun phrase, &amp;#8220;son of&amp;#8230;&amp;#8221; some patriarch, like Zeus is &amp;#8220;son of Cronus&amp;#8221;. We&amp;#8217;ll actually return to this in the last&amp;nbsp;section.&lt;/p&gt;
&lt;p&gt;It&amp;#8217;s neat to see ships, spear, horses, and hand show up as often as they&amp;nbsp;do.&lt;/p&gt;
&lt;h2&gt;Diving&amp;nbsp;Below&lt;/h2&gt;
&lt;p&gt;While reading the &lt;code&gt;quanteda&lt;/code&gt; documentation, I discovered a technique I haven&amp;#8217;t seen before. Apparently it&amp;#8217;s called an &amp;#8220;x-ray&amp;#8221; plot, based off &lt;a href="https://thegogglesdonothing.com/archives/2012/09/amazon_kindle_and_text_mining.shtml"&gt;the original visualization for Kindle&lt;/a&gt;. It shows the relative occurrences of a word across documents or&amp;nbsp;books.&lt;/p&gt;
&lt;p&gt;I thought it&amp;#8217;d be interesting to look at when certain characters come in and out of the story through one of these plots. These aren&amp;#8217;t entirely accurate plots (e.g. some characters are only mentioned in terms of their father), but they&amp;#8217;re a first&amp;nbsp;approximation.&lt;/p&gt;
&lt;p&gt;To get started, we&amp;#8217;ll create two helper functions. The first to create the data to plot, and the second to&amp;nbsp;plot.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;pull_xray&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;token&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;token_xray&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;text_tokens&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
        &lt;span class="nf"&gt;kwic&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;token&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="nf"&gt;tibble&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;token&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;token&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;docname&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;factor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;token_xray&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;docname&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;levels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;book_names&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
        &lt;span class="n"&gt;from&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;token_xray&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;from&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;ntokens&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;attr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;token_xray&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;ntoken&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="n"&gt;token_xray&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;docname&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="n"&gt;relative&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;from&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;ntokens&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="n"&gt;plot_xray&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xray_df&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;xray_df&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
        &lt;span class="nf"&gt;mutate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;docname&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;fct_rev&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;docname&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
        &lt;span class="nf"&gt;ggplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;aes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;relative&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;docname&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
        &lt;span class="nf"&gt;geom_segment&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;aes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;xend&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;after_stat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
            &lt;span class="n"&gt;yend&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;after_stat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="m"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
            &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;stage&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;docname&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;after_stat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="m"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="p"&gt;),&lt;/span&gt;
        &lt;span class="n"&gt;linewidth&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="m"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
        &lt;span class="nf"&gt;scale_color_identity&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;guide&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;guide_legend&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;position&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;top&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
            &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;unique&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xray_df&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;token&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
        &lt;span class="nf"&gt;labs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;element_blank&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
            &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;element_blank&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
            &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;element_blank&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
            &lt;span class="n"&gt;caption&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;liebscher.github.io&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
        &lt;span class="n"&gt;theme_iliad&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
        &lt;span class="nf"&gt;theme&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;axis.ticks.y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;element_blank&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
            &lt;span class="n"&gt;axis.text.x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;element_blank&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
            &lt;span class="n"&gt;legend.text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;element_text&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;14&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;While writing this code, I actually started with the data art aspect (covered in the next section). I&amp;#8217;ll start with the simpler version first, but while exploring these solutions I ran into an unexpected headache. Two problems arose: I wanted the x-ray bars to show up on a normal discrete y-axis, and I wanted the x-ray bars to be centered on each discrete&amp;nbsp;tick.&lt;/p&gt;
&lt;p&gt;The solution was to modify the plot layers for &lt;code&gt;y&lt;/code&gt; and &lt;code&gt;yend&lt;/code&gt; with &lt;code&gt;after_stat&lt;/code&gt; and &lt;code&gt;stage&lt;/code&gt;. The problem was that the discrete axis was being converted to a numeric axis under the hood, and in order to shift the segments vertically I needed to modify this conversion&amp;nbsp;function.&lt;/p&gt;
&lt;p&gt;Early on in &lt;em&gt;The Iliad&lt;/em&gt; we say goodbye to Achilles for a while. He returns a bit in the middle and at the end, but he has a distinct ebb and flow of appearances in &lt;em&gt;The Iliad&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;#8217;s compare how often he is mentioned to how often Zeus, who is present throughout, is&amp;nbsp;mentioned:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;bind_rows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="nf"&gt;pull_xray&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Achilles&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;greek_colors&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
    &lt;span class="nf"&gt;pull_xray&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Zeus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;greek_colors&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;4&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;plot_xray&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="code-output"&gt;
&lt;img data-src="/images/the-iliad/xray-achilles.png" class="uk-align-left" width="80%" height="" alt="X-ray plot of Achilles and Zeus" uk-img&gt;
&lt;/pre&gt;

&lt;p&gt;It&amp;#8217;s fairly easy to see different points in the plot here. For example, Achilles leaves the battle field (and plot) after his argument with Agamemnon in Book I. When faced with manic Hector and the encroaching Trojan army, Agamemnon sends his warriors to appeal to Achilles in Book &lt;span class="caps"&gt;IX&lt;/span&gt;. I like how you can see Agamemnon&amp;#8217;s discussion of Achilles followed by the interactions with Achilles in Book &lt;span class="caps"&gt;IX&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Another character that interests me is wise old Nestor, portrayed as an elder whose word is final, he offers sage advice to the Greeks. I suspect that Nestor must come in especially at pivotal decision points in the book. He seems the closest to the omniscient gods as the Greeks will get, so perhaps he plays an important plot delivery&amp;nbsp;part.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;pull_xray&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Nestor&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;greek_colors&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;plot_xray&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="code-output"&gt;
&lt;img data-src="/images/the-iliad/xray-nestor.png" class="uk-align-left" width="80%" height="" alt="X-ray plot of Nestor" uk-img&gt;
&lt;/pre&gt;

&lt;p&gt;I find this very compelling! For example, after Achilles turns down Agamemnon&amp;#8217;s plea to join back in the battle in Book &lt;span class="caps"&gt;IX&lt;/span&gt;, Agamemnon is distressed and in need of guidance. He seeks out Nestor in the middle of the night in Book X. This even feels like a turning point in the novel &amp;#8212; Hector and the Trojans are so close to laying siege the ships of the Achaeans and fate is too close for Agamemnon&amp;#8217;s comfort. Nestor says to him, in a very prescient way, &amp;#8220;Most noble son of Atreus, king of men, Agamemnon, &lt;strong&gt;Zeus will not do all for Hector that Hector thinks he will.&lt;/strong&gt;&amp;#8221; Very&amp;nbsp;resolute!&lt;/p&gt;
&lt;p&gt;I&amp;#8217;m also curious how the Gods appear in the story as well. I would think that Gods who favor different peoples are unlikely to co-appear, just as enemies are unlikely to have conversations with each other. We&amp;#8217;ll explore this by looking at mentions of Athene (who favors the Greeks) and Apollo (who favors the&amp;nbsp;Trojans):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;bind_rows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="nf"&gt;pull_xray&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Athene&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;greek_colors&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
    &lt;span class="nf"&gt;pull_xray&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Apollo&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;greek_colors&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;7&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;plot_xray&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="code-output"&gt;
&lt;img data-src="/images/the-iliad/xray-gods.png" class="uk-align-left" width="80%" height="" alt="X-ray plot of Athene and Apollo" uk-img&gt;
&lt;/pre&gt;

&lt;p&gt;It does appear that Athene, who favors the Greeks, and Apollo, who favors the Trojans, generally don&amp;#8217;t overlap very often. Obviously this is just eye-balling, and we&amp;#8217;d need to be more specific if we wanted to determine if these gods do interact&amp;nbsp;often.&lt;/p&gt;
&lt;h3&gt;Being More&amp;nbsp;Artful&lt;/h3&gt;
&lt;p&gt;I was introduced to &lt;a href="https://www.visualcinnamon.com/about/"&gt;Nadieh Bremer&amp;#8217;s&lt;/a&gt; &lt;a href="https://www.visualcinnamon.com/2020/06/sony-music-data-art/"&gt;data art posters for Sony Music&lt;/a&gt; a few months ago. I felt inspired by this work, and wanted to mimic it just for fun and to try something new. My childhood and teenage years were very artistic and so some day I might like to come back to this, perhaps merging it with my technical&amp;nbsp;expertise.&lt;/p&gt;
&lt;p&gt;Here I&amp;#8217;m combining the x-ray plots of our two Achaean heros and the word count of each book. This is hardly a useful plot, but it is interesting to play with the &lt;a href="https://ggplot2.tidyverse.org/news/index.html#ggplot2-350"&gt;new ggplot2 3.5.0&lt;/a&gt; &lt;a href="https://ggplot2.tidyverse.org/reference/coord_polar.html"&gt;&lt;code&gt;coord_radial()&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;p1&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;bind_rows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="nf"&gt;pull_xray&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Achilles&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;greek_colors&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
    &lt;span class="nf"&gt;pull_xray&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Agamemnon&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;greek_colors&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;ggplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;aes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;relative&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="m"&gt;0.05&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="m"&gt;1.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;docname&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="nf"&gt;coord_radial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inner.radius&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;expand&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;F&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;start&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;end&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;-0.2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="nf"&gt;geom_segment&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="nf"&gt;aes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;xend&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;after_stat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;yend&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;after_stat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="m"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;stage&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;docname&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;after_stat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="m"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
        &lt;span class="n"&gt;linewidth&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;4&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="m"&gt;5&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="nf"&gt;scale_color_identity&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="nf"&gt;scale_y_discrete&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;drop&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;F&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="nf"&gt;labs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;element_blank&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;element_blank&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="nf"&gt;theme_minimal&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="nf"&gt;theme&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;panel.border&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;element_blank&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
        &lt;span class="n"&gt;axis.ticks.y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;element_blank&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
        &lt;span class="n"&gt;axis.text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;element_blank&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
        &lt;span class="n"&gt;panel.grid&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;element_blank&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
        &lt;span class="n"&gt;legend.text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;element_text&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;14&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
        &lt;span class="n"&gt;plot.background&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;element_rect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fill&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;#F2F4F4&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;NA&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="n"&gt;p2&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key_text&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;ggplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;aes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Tokens&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Text&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="nf"&gt;coord_radial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;start&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;-0.06&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="kc"&gt;pi&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;end&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;0.06&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="kc"&gt;pi&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="nf"&gt;geom_col&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fill&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;greek_colors&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;5&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;width&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="nf"&gt;theme_void&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="nf"&gt;theme&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;panel.background&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;element_rect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fill&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;transparent&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;NA&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
        &lt;span class="n"&gt;plot.background&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;element_rect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fill&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;transparent&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;NA&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="n"&gt;p1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nf"&gt;inset_element&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;0.495&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="code-output"&gt;
&lt;img data-src="/images/the-iliad/xray-art.png" class="uk-align-left" width="80%" height="" alt="X-ray art" uk-img&gt;
&lt;/pre&gt;

&lt;p&gt;Like I said, maybe not the most useful. But also it could be prettier if I kept experimenting with it. I could continue to layer and layer new data, but I felt like just this much was enough to try some new things. There was a surprising amount of hard-coded-ness to this tangent, unfortunately, which might just be part of what makes this work akin to&amp;nbsp;art.&lt;/p&gt;
&lt;h2&gt;Love and&amp;nbsp;Death&lt;/h2&gt;
&lt;p&gt;Although I&amp;#8217;m quick to disparage sentiment analyses, I wanted to give it a try with this project. I thought there might be an interesting visualization opportunity with these data&amp;nbsp;too.&lt;/p&gt;
&lt;p&gt;For example, &amp;#8220;fortune&amp;#8221; and &amp;#8220;death&amp;#8221; have distinct sentiments according to the &lt;a href="https://www.tidytextmining.com/sentiment#the-sentiments-datasets"&gt;&lt;span class="caps"&gt;AFINN&lt;/span&gt;&lt;/a&gt; (where greater than 0 is positive language and less than 0 is negative&amp;nbsp;language):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;tibble&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;love&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;death&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;left_join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;get_sentiments&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;afinn&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="code-output"&gt;
  word  value
1 love      3
2 death    -2
&lt;/pre&gt;

&lt;p&gt;The question is: how does the sentiment change over time? The goal is to measure sentiment from words like these over the course of each book. We&amp;#8217;ll take every word in the book and match it to a sentiment rating. Here we do that and print out the first six words in the book along with their sentiment&amp;nbsp;rating:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;tidy_corpus&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;tibble&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;doc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;as.character&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;books_ps&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;docnames&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;str_match_all&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;BOOK [XIV]+&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)[[&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]][,&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;tidy_tokens&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;tidy_corpus&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;unnest_tokens&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;doc&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;to_lower&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;T&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;sentiment&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;tidy_tokens&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;left_join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;get_sentiments&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;afinn&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;sentiment&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;head&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="code-output"&gt;
  docnames word    value
1 BOOK I   sing       NA
2 BOOK I   o          NA
3 BOOK I   goddess    NA
4 BOOK I   the        NA
5 BOOK I   anger      -3
6 BOOK I   of         NA
&lt;/pre&gt;

&lt;p&gt;We finally compute the cumulative sentiment over each&amp;nbsp;book.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;book_labels&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;sentiment&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;distinct&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;docnames&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;mutate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;docnames&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;factor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;docnames&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;levels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;docnames&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;

        &lt;span class="c1"&gt;# for visualization purposes we want just the roman numeral&lt;/span&gt;
        &lt;span class="n"&gt;book&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;str_remove&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;docnames&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;BOOK &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
        &lt;span class="n"&gt;book&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;factor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;book&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;levels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;book&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;sentiment&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;mutate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;docnames&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;factor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;docnames&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;levels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;book_names&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;group_by&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;docnames&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;mutate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;row_number&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
        &lt;span class="n"&gt;rel&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="nf"&gt;n&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
        &lt;span class="n"&gt;cum_sent&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;cumsum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;replace_na&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;filter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;round&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rel&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="m"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%%&lt;/span&gt; &lt;span class="m"&gt;5&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;

    &lt;span class="nf"&gt;ggplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;aes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rel&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cum_sent&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="m"&gt;150&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cum_sent&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="nf"&gt;coord_radial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;expand&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;F&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;inner.radius&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;0.25&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="nf"&gt;facet_wrap&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="n"&gt;docnames&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ncol&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;6&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="nf"&gt;geom_smooth&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;aes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;after_stat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="m"&gt;200&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;group&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;method&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;loess&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;formula&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;se&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;F&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;linewidth&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;1.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;show.legend&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;F&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="nf"&gt;geom_text&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;aes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;book&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;book_labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;family&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;newsreader&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;-155&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;2.25&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;greek_colors&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="nf"&gt;scale_color_gradient2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;low&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;red3&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mid&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;gray90&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;high&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;green4&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="nf"&gt;labs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;caption&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;liebscher.github.io&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="nf"&gt;theme_void&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="nf"&gt;theme&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;strip.text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;element_blank&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
        &lt;span class="n"&gt;plot.caption&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;element_text&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;family&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;newsreader&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;gray70&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;face&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;plain&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="code-output"&gt;
&lt;img data-src="/images/the-iliad/sentiment.png" class="uk-align-left" width="80%" height="" alt="Sentiment" uk-img&gt;
&lt;/pre&gt;

&lt;p&gt;To explain this just a&amp;nbsp;bit:&lt;/p&gt;
&lt;p&gt;We have a data frame of every word and its associated &lt;span class="caps"&gt;AFINN&lt;/span&gt; sentiment. Then for each book, we create a cumulative sum of those sentiment ratings. Where &lt;span class="caps"&gt;AFINN&lt;/span&gt; rates 0 as neutral sentiment, and positive and negative sentiment. We scale the books so that we&amp;#8217;re visualizing sentiment from the relative beginning to&amp;nbsp;end.&lt;/p&gt;
&lt;p&gt;This is on a by-word basis, which is a lot of data to plot on its own. That&amp;#8217;s why we sample only every 500th cumulative sentiment measure. We then plot a Loess smoothing curve over these ratings to give us a smooth line from beginning to end. We could do the same thing with the raw data as points, but I think this looks&amp;nbsp;nicer.&lt;/p&gt;
&lt;p&gt;As the swirls drift into green, the story has gotten more positive. As they head toward red, it&amp;#8217;s more&amp;nbsp;negative.&lt;/p&gt;
&lt;p&gt;We can see that most books are rather dark, except Book &lt;span class="caps"&gt;IX&lt;/span&gt;. We probably could expect this, since, after all, it is on the surface a book about war (with lots of slaying and misfortune). It is interesting to see Book &lt;span class="caps"&gt;XXII&lt;/span&gt; particularly dark (this is where Hector is&amp;nbsp;killed).&lt;/p&gt;
&lt;p&gt;One shortcoming of this approach is the sheer missingness of sentiments for&amp;nbsp;tokens.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;sentiment&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;summarize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;total_words&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;n&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;words_with_rating&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;!&lt;/span&gt;&lt;span class="nf"&gt;is.na&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="code-output"&gt;
  total_words words_with_rating
1      153268              8833
&lt;/pre&gt;

&lt;p&gt;In total, only 8,833 words of the 153,268 have a sentiment rating (barely 5%)! In an ideal world, many more words would have an attached sentiment rating, that way our visuals are more certain. However, sentiment is highly contextual, so it&amp;#8217;s unsurprising that this data is so&amp;nbsp;incomplete.&lt;/p&gt;
&lt;h2&gt;The Web of&amp;nbsp;Characters&lt;/h2&gt;
&lt;p&gt;I&amp;#8217;ll be honest, there&amp;#8217;s &lt;em&gt;a lot&lt;/em&gt; of characters in &lt;em&gt;The Iliad&lt;/em&gt;, and even more names mentioned. At first it was a bit hard to keep track of, but five books in and you definitely start to solidify your understanding of who&amp;#8217;s&amp;nbsp;who.&lt;/p&gt;
&lt;p&gt;All these characters though have had me dreaming about some sort of &amp;#8220;web of characters&amp;#8221;. I was partially motivated by similar work, such as &lt;a href="https://mediarep.org/server/api/core/bitstreams/699b22aa-9bf8-4a75-92e8-c9caf00530e9/content"&gt;Venturini et al. (2017)&lt;/a&gt;. Unlike this work though, I want to focus solely on characters. I think this is a particularly interesting way to evaluate the story at a high&amp;nbsp;level.&lt;/p&gt;
&lt;p&gt;First, we have a few new packages to load for working with&amp;nbsp;graphs:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;widyr&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tidygraph&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;igraph&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ggraph&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ggnewscale&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Next, I&amp;#8217;m interested in looking at how characters co-occur as a proxy for who&amp;#8217;s interacting with who. We might anticipate that certain characters will have interactions with the gods more than other characters, and hopefully we see a distinction between the Gods, the Achaeans, and the&amp;nbsp;Trojans.&lt;/p&gt;
&lt;p&gt;We&amp;#8217;ll start by splitting the text into paragraphs. This is extremely rudimentary, but it&amp;#8217;s a first step which we can later critique. I firmly believe that one step must be taken, no matter how poor, so that we can continuously improve based on concrete&amp;nbsp;evidence.&lt;/p&gt;
&lt;p&gt;I&amp;#8217;ll count how often characters are mentioned in the same paragraph together. I also split the paragraphs into tokens, and finally calculate the pairwise correlation between tokens within&amp;nbsp;paragraphs.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;tidy_paragraphs&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;tidy_corpus&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;unnest_paragraphs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;paragraph&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;doc&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;to_lower&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;F&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;mutate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;paragraph_ix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;row_number&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;ungroup&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;tidy_paragraph_tokens&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;tidy_paragraphs&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;unnest_tokens&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;paragraph&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;to_lower&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;F&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;token_cor&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;tidy_paragraph_tokens&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;anti_join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;stop_words&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;group_by&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;filter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;n&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="m"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;pairwise_cor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;paragraph_ix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sort&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;T&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;token_cor&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="code-output"&gt;
   item1     item2     correlation
 1 mell      pell            1    
 2 pell      mell            1    
 3 Argus     slayer          0.925
 4 slayer    Argus           0.925
 5 bowl      mixing          0.728
 6 mixing    bowl            0.728
 7 bearing   aegis           0.703
 8 aegis     bearing         0.703
 9 addressed goodwill        0.616
10 goodwill  addressed       0.616
&lt;/pre&gt;

&lt;p&gt;As we can see, the resulting data frame lists all token pairs and their correlation. The most frequently co-occuring tokens are &amp;#8220;pell&amp;#8221; and &amp;#8220;mell&amp;#8221;, which always show up in the same paragraph together. Another pair which stands out to me is &amp;#8220;&lt;a href="https://en.wikipedia.org/wiki/Aegis"&gt;aegis&lt;/a&gt;-bearing&amp;#8221;. This was new to me when I first read it, and the phrase is typically &amp;#8220;aegis-bearing Zeus&amp;#8221;, meaning Zeus who carries the aegis&amp;nbsp;shield.&lt;/p&gt;
&lt;p&gt;Next, I&amp;#8217;ll filter to only pairs of key characters, including gods, Greeks, Trojans, and Women. The Women in &lt;em&gt;The Iliad&lt;/em&gt; are worthy of their own entire study (and this has probably already been done), but I separate them because they are quite important to the epic. Helen is the reason why this monstrous war is even happening, and Breseis is the part of the reason Achilles leaves the battle in the&amp;nbsp;beginning.&lt;/p&gt;
&lt;p&gt;This doesn&amp;#8217;t capture every mention of these characters though. For example, Homer sometimes used &amp;#8220;Olympian Thunderer&amp;#8221; as an alias for Zeus. That&amp;#8217;s okay though, this is just a rough&amp;nbsp;start.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;gods&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Zeus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Hera&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Athene&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Poseidon&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Hades&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Apollo&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Artemis&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s"&gt;&amp;quot;Aphrodite&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Ares&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Hephaestus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Hermes&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Iris&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Thetis&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;greeks&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Agamemnon&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Achilles&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Menelaus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Nestor&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Odysseus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Ajax&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s"&gt;&amp;quot;Diomed&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Calchas&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Idomeneus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;trojans&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Priam&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Paris&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Hector&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Helenus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Antenor&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Sarpedon&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Galaucus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;women&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Helen&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Briseis&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;character_graph&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;token_cor&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;filter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;item1&lt;/span&gt; &lt;span class="o"&gt;%in%&lt;/span&gt; &lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gods&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;greeks&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trojans&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;women&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
        &lt;span class="n"&gt;item2&lt;/span&gt; &lt;span class="o"&gt;%in%&lt;/span&gt; &lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gods&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;greeks&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trojans&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;women&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;graph_from_data_frame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;directed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;F&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;as_tbl_graph&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;activate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nodes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;left_join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;tidy_paragraph_tokens&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
            &lt;span class="nf"&gt;count&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
        &lt;span class="n"&gt;by&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;name&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;word&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;mutate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;group&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;case_when&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;%in%&lt;/span&gt; &lt;span class="n"&gt;gods&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;God&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;%in%&lt;/span&gt; &lt;span class="n"&gt;greeks&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Greek&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;%in%&lt;/span&gt; &lt;span class="n"&gt;trojans&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Trojan&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;%in%&lt;/span&gt; &lt;span class="n"&gt;women&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Woman&amp;quot;&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;activate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;edges&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;filter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;correlation&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="m"&gt;0.05&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;character_graph&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="code-output"&gt;
IGRAPH c9d4c51 UN-- 29 166 -- 
+ attr: name (v/c), n (v/n), group (v/c), correlation (e/n)
&lt;/pre&gt;

&lt;p&gt;We can refer to &lt;a href="https://igraph.org/r/doc/print.igraph.html"&gt;the documentation&lt;/a&gt; to interpret this summary. It tells us that we have an undirected graph with named nodes, and that it is unweighted and&amp;nbsp;non-bipartite.&lt;/p&gt;
&lt;p&gt;Most of the above code should be self-explanatory. The last line though I&amp;#8217;m filtering to only retain dyads with a slightly positive correlation (co-occurance). This will keep the edges in our graph to the bare&amp;nbsp;minimum.&lt;/p&gt;
&lt;p&gt;Next, we&amp;#8217;ll plot this&amp;nbsp;graph:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;character_graph&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;ggraph&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;layout&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;nicely&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="nf"&gt;geom_edge_link&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;aes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;correlation&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;gray50&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;show.legend&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;F&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="nf"&gt;geom_node_point&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;aes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;group&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;show.legend&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;F&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="nf"&gt;scale_size_continuous&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;range&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;8&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="nf"&gt;new_scale&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;size&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="nf"&gt;geom_node_label&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="nf"&gt;aes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;group&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
        &lt;span class="n"&gt;family&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;newsreader&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;repel&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label.size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fill&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;rgb&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;0.3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
        &lt;span class="n"&gt;show.legend&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;F&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="nf"&gt;scale_size_continuous&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;range&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;2.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;6&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="nf"&gt;scale_color_manual&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;greek_colors&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;greek_colors&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;greek_colors&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;5&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;greek_colors&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;7&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="nf"&gt;labs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;element_blank&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;caption&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;liebscher.github.io&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="nf"&gt;theme&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;panel.background&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;element_rect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fill&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;white&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;NA&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
        &lt;span class="n"&gt;plot.caption&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;element_text&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;family&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;newsreader&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;gray70&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;face&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;plain&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="code-output"&gt;
&lt;img data-src="/images/the-iliad/network.png" class="uk-align-left" width="80%" height="" alt="Network" uk-img&gt;
&lt;/pre&gt;

&lt;p&gt;Spectacular!&lt;/p&gt;
&lt;p&gt;We can see some interesting patterns arise. For example, Helen connects Paris and Menelaus (she was originally Menelaus&amp;#8217;s wife, but Paris of Troy abducted her, thus initiating the war). We can also see Diomed linked to Athene (who seems to favor him), Iris sending messages between gods, and the overall three separate camps of&amp;nbsp;people.&lt;/p&gt;
&lt;p&gt;I&amp;#8217;m pleased that this graph closely resembles more rigorous network analyses of the book (see e.g. &lt;a href="https://www.euppublishing.com/doi/abs/10.3366/ijhac.2015.0141"&gt;Kydros et al (2015)&lt;/a&gt;). Some shortcomings of our approaches are the rather 1-dimensional perspective that characters are nodes, and edges are simple interactions. It would be nice to be a bit more inclusive with these&amp;nbsp;definitions.&lt;/p&gt;
&lt;h2&gt;Who&amp;#8217;s your&amp;nbsp;patriarch?&lt;/h2&gt;
&lt;p&gt;In &lt;em&gt;The Iliad&lt;/em&gt;, often times men are &lt;a href="https://en.wikipedia.org/wiki/Patrilineality"&gt;mentioned by their father&amp;#8217;s name&lt;/a&gt;, which appears to be an&amp;nbsp;honorific.&lt;/p&gt;
&lt;p&gt;I&amp;#8217;m curious which fathers are mentioned the most relative to how often their sons are mentioned. First, we&amp;#8217;ll join all the text into a single string so that it&amp;#8217;s easier to search for substring across all&amp;nbsp;books:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;flat_text&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;tidy_corpus&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;pull&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;doc&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;str_flatten&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;collapse&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot; &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then we&amp;#8217;ll define a regular expression to search for mentions of patriarchs. This often is written like &amp;#8220;son of Atreus&amp;#8221; but could also be &amp;#8220;Sons of aegis-bearing Zeus&amp;#8221;. In both cases, we&amp;#8217;d want to capture &amp;#8220;Atreus&amp;#8221; and &amp;#8220;Zeus&amp;#8221;&amp;nbsp;respectively.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;fathers&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;flat_text&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;str_match_all&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;[Ss]ons? of[\\- A-Za-z]*? (?:King )?(?&amp;lt;name&amp;gt;[A-Z][a-z]+)&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="n"&gt;magrittr&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="nf"&gt;extract2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;as_tibble&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;filter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;!&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;%in%&lt;/span&gt; &lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Achaeans&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Troy&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Trojans&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Great, how many unique fathers are&amp;nbsp;mentioned?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;n_distinct&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fathers&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="code-output"&gt;
162
&lt;/pre&gt;

&lt;p&gt;Which fathers are most often&amp;nbsp;mentioned?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;fathers&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;count&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sort&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;T&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;head&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="code-output"&gt;
  name        n
1 Atreus    133
2 Peleus     99
3 Tydeus     84
4 Cronus     73
5 Priam      48
6 Telamon    45
&lt;/pre&gt;

&lt;p&gt;Next we&amp;#8217;ll map each son to his father, and count mentions of the son&amp;#8217;s name. We&amp;#8217;ll work with just a subset of characters here to keep the analysis&amp;nbsp;interpretable.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;patrilineality&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;tribble&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="n"&gt;father&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="n"&gt;son&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="n"&gt;people&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;Cronus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Zeus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Gods&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;Cronus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Hades&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Gods&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;Cronus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Poseidon&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Gods&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;Zeus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Apollo&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Gods&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;Zeus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Ares&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Gods&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;Priam&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Hector&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Trojans&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;Priam&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Paris&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Trojans&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;Priam&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Helenus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Trojans&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;Atreus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Agamemnon&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Achaeans&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;Atreus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Menelaus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Achaeans&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;Peleus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Achilles&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Achaeans&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;Laertes&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Odysseus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Achaeans&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;Neleus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Nestor&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Achaeans&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;Laomedon&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Priam&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Trojans&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;Telamon&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Ajax&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Achaeans&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;Oileus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Ajax&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Achaeans&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;Tydeus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Diomed&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Achaeans&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;Menoetius&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Patroclus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Achaeans&amp;quot;&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Finally, I&amp;#8217;ll count how often each son is mentioned in the text, and join that with the data frame of how often his father is&amp;nbsp;mentioned.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;patrilineality_mentions&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;patrilineality&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;rowwise&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;mutate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;son_mentions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;str_count&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;flat_text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;son&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;group_by&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;father&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;summarize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;sons&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;paste0&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;son&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;collapse&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;, &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
        &lt;span class="n"&gt;sons_mentions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;son_mentions&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
        &lt;span class="n"&gt;people&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;first&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;people&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;left_join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;fathers&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
            &lt;span class="nf"&gt;count&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;father_mentions&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sort&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;T&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
        &lt;span class="n"&gt;by&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;father&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;ungroup&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;patrilineality_mentions&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;head&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="code-output"&gt;
  father    sons                  sons_mentions people   father_mentions
1 Atreus    Agamemnon, Menelaus             342 Achaeans             133
2 Cronus    Zeus, Hades, Poseidon           580 Gods                  73
3 Laertes   Odysseus                        124 Achaeans               9
4 Laomedon  Priam                           184 Trojans                3
5 Menoetius Patroclus                       173 Achaeans              27
6 Neleus    Nestor                           93 Achaeans              10
&lt;/pre&gt;

&lt;p&gt;This looks good. Let&amp;#8217;s assume that the more a father is mentioned compared to his son, the more important that father is. Let&amp;#8217;s plot this importance measure as the ratio of father mentions to son&amp;nbsp;mentions:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;patrilineality_mentions&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;mutate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;pair&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;paste0&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;father&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot; &amp;gt; &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sons&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
        &lt;span class="n"&gt;father_importance&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;father_mentions&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;sons_mentions&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="nf"&gt;ggplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;aes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;father_importance&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nf"&gt;fct_reorder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pair&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;father_importance&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="nf"&gt;coord_cartesian&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xlim&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="nf"&gt;geom_segment&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;aes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;xend&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;father_importance&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;gray90&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="nf"&gt;geom_point&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;aes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;people&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="nf"&gt;scale_x_continuous&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;breaks&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;0.9&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Very Unimportant&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Very Important&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="nf"&gt;labs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;element_blank&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;element_blank&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
        &lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;element_blank&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
        &lt;span class="n"&gt;title&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Homer&amp;#39;s Patriarchy&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;caption&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;liebscher.github.io&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="n"&gt;theme_iliad&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="nf"&gt;theme&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;legend.position&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;top&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;legend.text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;element_text&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;10&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="code-output"&gt;
&lt;img data-src="/images/the-iliad/patriarchy.png" class="uk-align-left" width="80%" height="" alt="Patriarchy" uk-img&gt;
&lt;/pre&gt;

&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Tydeus"&gt;Tydeus&lt;/a&gt; comes out as a very important patriarch, as the story is told. &lt;a href="https://en.wikipedia.org/wiki/Atreus"&gt;Atreus&lt;/a&gt; as well. At first I was surprised to see Zeus so low in importance, although I think we can explain this as a difference between Zeus as the patriarch and Zeus as the subject. Essentially, his name is being double counted as a patriarch when some of those mentions are him as an individual. We could say the same of&amp;nbsp;Priam.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;The Iliad&lt;/em&gt; is a must-read. It is one of the foundational layers of Western civilization: whether we realize it not, contemporary thought reflects authors like Goethe (&lt;a href="https://www.tandfonline.com/doi/pdf/10.1080/19306962.1967.11754688"&gt;Wohlleben, 1967&lt;/a&gt;) and Socrates (&lt;a href="https://www.google.com/books/edition/The_Philosophy_Of_Socrates/_1NRAAAAYAAJ?hl=en"&gt;Brickhouse &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; Smith, 2000&lt;/a&gt;), who reflect Homer. Reading &lt;em&gt;The Iliad&lt;/em&gt; has not been nearly as difficult or monotonous as I expected. On the contrary, it&amp;#8217;s engaging, accessible, and colorful. The characters are rich, emotive, and complex. There may be many of them (nearly 100), but Homer&amp;#8217;s storytelling is crisp and we don&amp;#8217;t get too lost in the web. I would love to dive deeper in a more literary analysis of leadership and values as expressed by the characters. For example, I really enjoy such lessons as, &amp;#8220;he that foments civil discord is a clanless, hearthless&amp;nbsp;outlaw.&amp;#8221;&lt;/p&gt;
&lt;p&gt;There&amp;#8217;s too many ways to dive computationally into a story like this, so this is just a start. In fact, I&amp;#8217;d estimate that &lt;em&gt;The Iliad&lt;/em&gt; is one of the most analyzed stories ever. I&amp;#8217;m happy to be able to contribute one piece more to this timeless endeavor. I&amp;#8217;d like to explore more of the extant analyses on the subject, although even that feels like a big&amp;nbsp;task.&lt;/p&gt;
&lt;p&gt;I have a copy of &lt;em&gt;The Odyssey&lt;/em&gt; which I bought at the same time, and I&amp;#8217;d like to read that next. I will consider writing a follow-up post on any analyses I do for that&amp;nbsp;story.&lt;/p&gt;</content><category term="meta"></category><category term="Rlang"></category><category term="Nlp"></category><category term="Data Science"></category><category term="Art"></category></entry><entry><title>Bayes for my BestÂ Workouts</title><link href="/posts/2024/Jan/models-best-workouts/" rel="alternate"></link><published>2024-01-28T00:00:00-08:00</published><updated>2024-01-28T00:00:00-08:00</updated><author><name>Alex Liebscher</name></author><id>tag:None,2024-01-28:/posts/2024/Jan/models-best-workouts/</id><summary type="html">&lt;p&gt;Building models to predict my optimal&amp;nbsp;workouts&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Determining the right amount of weight or reps to lift during home workouts can be quite an obstacle. After months of trial and error, I decided to take a systematic, logical, and calculated approach to this dilemma. I&amp;#8217;ve since built a personalized model that guides me through my workouts, blending rigor and intuition for optimal&amp;nbsp;performance.&lt;/p&gt;
&lt;h2&gt;The Initial&amp;nbsp;Model&lt;/h2&gt;
&lt;p&gt;In the early stages of my project, I was looking for a solution that would streamline the decision-making process without sacrificing all the idiosyncracies of each exercise and workout. In December, I developed a basic model that employed linear regressions for each exercise, considering sets, reps, and my perceived exertion level. I could then determine which exercises I wanted to do, along with the reps and sets, and the model would tell me how much weight. While this approach provided some guidance, it felt too rigid and had the opportunity to yield impractical suggestions, such as negative&amp;nbsp;weights.&lt;/p&gt;
&lt;h2&gt;The Improved&amp;nbsp;Model&lt;/h2&gt;
&lt;p&gt;Dissatisfied with the limitations of the initial model, I turned toward more flexible Bayesian modeling that would also help me understand those idiosyncracies I mentioned. This new approach not only accounts for physical constraints (weight must be greater than zero) but also is based on informed priors and physical constraints. By doing so, even exercises with limited data can be predicted accurately. I have two models: one for predicting the target weight of an exercise, the other for predicting the target number of reps (in case I cannot adjust the weight). The Bayesian models represents a step closer to a more nuanced, precise, and flexible workout planning&amp;nbsp;tool.&lt;/p&gt;
&lt;p&gt;&lt;img data-src="/images/pred-weights.png"  uk-img/&gt;&lt;/p&gt;
&lt;h3&gt;A Few&amp;nbsp;Details&lt;/h3&gt;
&lt;p&gt;I&amp;#8217;m leaving out many of the details, but to start, the target weight prediction model is formulated as&amp;nbsp;follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;weight&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;reps&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;sets&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;rpe_of_10&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;reps&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;rpe_of_10&lt;/span&gt;&lt;span class="o"&gt;|&lt;/span&gt;&lt;span class="n"&gt;exercise&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In other words, we predict the target weight according the number of reps, sets, and rating of perceived exertion, accounting for baseline variation by exercise and variation in the slope of reps and &lt;span class="caps"&gt;RPE&lt;/span&gt; by exercise. We replace the fixed intercept with random intercepts so that the random intercepts are the baseline weight for each, not the offset from the mean of all. Moreover, after sampling, we reject all posterior samples where the coefficient on sets is greater than zero, the sum of the fixed and random slopes for reps is greater than zero, or the sum of the fixed and random slopes for &lt;span class="caps"&gt;RPE&lt;/span&gt; is less than zero. Ideally we&amp;#8217;d constrain the model early on without this ad hoc post-sampling modification, but I&amp;#8217;ve found that too time consuming to figure out. Finally, the likelihood function models a Log-Normal&amp;nbsp;distribution.&lt;/p&gt;
&lt;p&gt;We also have a model for predicting the number of total reps for an exercise in the case that the weight in non-adjustable. The model looks&amp;nbsp;like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;total&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="nf"&gt;trials&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;60&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;rpe_of_10&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rpe_of_10&lt;/span&gt;&lt;span class="o"&gt;|&lt;/span&gt;&lt;span class="n"&gt;exercise&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;A slightly simpler model, we assume that the number of total reps is some proportion of the possible 60 (chosen arbitrarily). This binomial model is actually far superior to a poisson model, or a negative binomial model, since there&amp;#8217;s such strong under-dispersion. I constrain post-sampling here as&amp;nbsp;well. &lt;/p&gt;
&lt;h2&gt;Bridging the&amp;nbsp;Gap&lt;/h2&gt;
&lt;p&gt;One challenge I encountered in this work was the vast disparity between anecdotal gym advice and the highly specialized sports science literature. While internet advice often felt impersonal and arbitrary, academic research was specific to elite athletes and controlled conditions. My attempts aimed to bridge this gap, offering practical solutions grounded in data and applicable to the average home gym&amp;nbsp;enthusiast.&lt;/p&gt;
&lt;h2&gt;Ongoing&amp;nbsp;Exploration&lt;/h2&gt;
&lt;p&gt;As I transition into relying on this new model for my workout planning, the work is far from over. I plan to continuously refine and expand the model by exploring new predictors that could improve its accuracy. Every day I think of predictors and ways of changing the model that could either improve its accuracy or have it reflect real-world constraints even better. The goal is not only to optimize my own workouts but also to share insights that may benefit others seeking a more personalized and data-driven approach to their fitness&amp;nbsp;routines.&lt;/p&gt;
&lt;p&gt;One benefit of this work is that I&amp;#8217;ve been even more motivated to workout in order to collect data and sample the space of possible workouts to understand my limits. Possibly a very nerdy thing to admit (workout data collection motivates me to exercise!), but both activities are personal growth so I don&amp;#8217;t see any&amp;nbsp;pitfalls.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The intersection of data science and fitness has opened up exciting possibilities for home gym enthusiasts. By leveraging a more sophisticated model, I&amp;#8217;ve aimed to create a tool that strikes a balance between precision and practicality, making the process of determining workout weights both informed and enjoyable. As I continue to experiment and refine my approach, I look forward to sharing the evolution of this project and its impact on my fitness. Stay tuned for more insights and discoveries around data-driven home&amp;nbsp;workouts.&lt;/p&gt;</content><category term="meta"></category><category term="Rlang"></category><category term="Data Science"></category><category term="Fitness"></category></entry><entry><title>EstimatingÂ Quantiles</title><link href="/posts/2022/Nov/estimating-quantiles/" rel="alternate"></link><published>2022-11-05T00:00:00-07:00</published><updated>2022-11-05T00:00:00-07:00</updated><author><name>Alex Liebscher</name></author><id>tag:None,2022-11-05:/posts/2022/Nov/estimating-quantiles/</id><summary type="html">&lt;p&gt;What are quantiles, and what&amp;#8217;s happening underneath the hood of&amp;nbsp;quantile()?&lt;/p&gt;</summary><content type="html">&lt;style&gt;
#avkrsexhvb .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 14px;
  font-weight: normal;
  font-style: normal;
  background-color: #&lt;span class="caps"&gt;FFFFFF&lt;/span&gt;;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #&lt;span class="caps"&gt;A8A8A8&lt;/span&gt;;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #&lt;span class="caps"&gt;D3D3D3&lt;/span&gt;;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #&lt;span class="caps"&gt;A8A8A8&lt;/span&gt;;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #&lt;span class="caps"&gt;D3D3D3&lt;/span&gt;;
}

#avkrsexhvb .gt_heading {
  background-color: #&lt;span class="caps"&gt;FFFFFF&lt;/span&gt;;
  text-align: center;
  border-bottom-color: #&lt;span class="caps"&gt;FFFFFF&lt;/span&gt;;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #&lt;span class="caps"&gt;D3D3D3&lt;/span&gt;;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #&lt;span class="caps"&gt;D3D3D3&lt;/span&gt;;
}

#avkrsexhvb .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  border-bottom-color: #&lt;span class="caps"&gt;FFFFFF&lt;/span&gt;;
  border-bottom-width: 0;
}

#avkrsexhvb .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 6px;
  border-top-color: #&lt;span class="caps"&gt;FFFFFF&lt;/span&gt;;
  border-top-width: 0;
}

#avkrsexhvb .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #&lt;span class="caps"&gt;D3D3D3&lt;/span&gt;;
}

#avkrsexhvb .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #&lt;span class="caps"&gt;D3D3D3&lt;/span&gt;;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #&lt;span class="caps"&gt;D3D3D3&lt;/span&gt;;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #&lt;span class="caps"&gt;D3D3D3&lt;/span&gt;;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #&lt;span class="caps"&gt;D3D3D3&lt;/span&gt;;
}

#avkrsexhvb .gt_col_heading {
  color: #333333;
  background-color: #&lt;span class="caps"&gt;FFFFFF&lt;/span&gt;;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #&lt;span class="caps"&gt;D3D3D3&lt;/span&gt;;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #&lt;span class="caps"&gt;D3D3D3&lt;/span&gt;;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#avkrsexhvb .gt_column_spanner_outer {
  color: #333333;
  background-color: #&lt;span class="caps"&gt;FFFFFF&lt;/span&gt;;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#avkrsexhvb .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#avkrsexhvb .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#avkrsexhvb .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #&lt;span class="caps"&gt;D3D3D3&lt;/span&gt;;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 5px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#avkrsexhvb .gt_group_heading {
  padding: 8px;
  color: #333333;
  background-color: #&lt;span class="caps"&gt;FFFFFF&lt;/span&gt;;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #&lt;span class="caps"&gt;D3D3D3&lt;/span&gt;;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #&lt;span class="caps"&gt;D3D3D3&lt;/span&gt;;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #&lt;span class="caps"&gt;D3D3D3&lt;/span&gt;;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #&lt;span class="caps"&gt;D3D3D3&lt;/span&gt;;
  vertical-align: middle;
}

#avkrsexhvb .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #&lt;span class="caps"&gt;FFFFFF&lt;/span&gt;;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #&lt;span class="caps"&gt;D3D3D3&lt;/span&gt;;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #&lt;span class="caps"&gt;D3D3D3&lt;/span&gt;;
  vertical-align: middle;
}

#avkrsexhvb .gt_from_md &gt; :first-child {
  margin-top: 0;
}

#avkrsexhvb .gt_from_md &gt; :last-child {
  margin-bottom: 0;
}

#avkrsexhvb .gt_row {
  padding-top: 6px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #&lt;span class="caps"&gt;D3D3D3&lt;/span&gt;;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #&lt;span class="caps"&gt;D3D3D3&lt;/span&gt;;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #&lt;span class="caps"&gt;D3D3D3&lt;/span&gt;;
  vertical-align: middle;
  overflow-x: hidden;
}

#avkrsexhvb .gt_stub {
  color: #333333;
  background-color: #&lt;span class="caps"&gt;FFFFFF&lt;/span&gt;;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #&lt;span class="caps"&gt;D3D3D3&lt;/span&gt;;
  padding-left: 12px;
}

#avkrsexhvb .gt_summary_row {
  color: #333333;
  background-color: #&lt;span class="caps"&gt;FFFFFF&lt;/span&gt;;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#avkrsexhvb .gt_first_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #&lt;span class="caps"&gt;D3D3D3&lt;/span&gt;;
}

#avkrsexhvb .gt_grand_summary_row {
  color: #333333;
  background-color: #&lt;span class="caps"&gt;FFFFFF&lt;/span&gt;;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#avkrsexhvb .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #&lt;span class="caps"&gt;D3D3D3&lt;/span&gt;;
}

#avkrsexhvb .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#avkrsexhvb .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #&lt;span class="caps"&gt;D3D3D3&lt;/span&gt;;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #&lt;span class="caps"&gt;D3D3D3&lt;/span&gt;;
}

#avkrsexhvb .gt_footnotes {
  color: #333333;
  background-color: #&lt;span class="caps"&gt;FFFFFF&lt;/span&gt;;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #&lt;span class="caps"&gt;D3D3D3&lt;/span&gt;;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #&lt;span class="caps"&gt;D3D3D3&lt;/span&gt;;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #&lt;span class="caps"&gt;D3D3D3&lt;/span&gt;;
}

#avkrsexhvb .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding: 4px;
}

#avkrsexhvb .gt_sourcenotes {
  color: #333333;
  background-color: #&lt;span class="caps"&gt;FFFFFF&lt;/span&gt;;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #&lt;span class="caps"&gt;D3D3D3&lt;/span&gt;;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #&lt;span class="caps"&gt;D3D3D3&lt;/span&gt;;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #&lt;span class="caps"&gt;D3D3D3&lt;/span&gt;;
}

#avkrsexhvb .gt_sourcenote {
  font-size: 90%;
  padding: 4px;
}

#avkrsexhvb .gt_left {
  text-align: left;
}

#avkrsexhvb .gt_center {
  text-align: center;
}

#avkrsexhvb .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#avkrsexhvb .gt_font_normal {
  font-weight: normal;
}

#avkrsexhvb .gt_font_bold {
  font-weight: bold;
}

#avkrsexhvb .gt_font_italic {
  font-style: italic;
}

#avkrsexhvb .gt_super {
  font-size: 65%;
}

#avkrsexhvb .gt_footnote_marks {
  font-style: italic;
  font-weight: normal;
  font-size: 65%;
}
&lt;/style&gt;

&lt;p&gt;We see medians, quartiles, percentiles, and quantiles everywhere we look. For example, we see them &lt;a href="https://nypost.com/2022/11/02/astros-vs-phillies-prediction-game-4-odds-and-pick-today/"&gt;sports&lt;/a&gt; (&amp;#8220;the native of the Dominican Republic ranked in the&amp;#8230; 95th percentile in expected slugging percentage and 82nd percentile in hard-hit rate&amp;#8221;), &lt;a href="https://www.marketwatch.com/story/heres-strong-new-evidence-that-a-u-s-stock-market-rally-is-coming-soon-11667526884"&gt;finance&lt;/a&gt; (&amp;#8220;the index currently stands at just the 20th percentile of the historical distribution&amp;#8221;), and &lt;a href="https://richmond.com/business/local/central-virginia-housing-market-cools-median-price-still-32-000-higher-than-last-year/article_68219452-3f66-58f0-9c4d-9addd67e6dcc.html"&gt;real estate&lt;/a&gt; (&amp;#8220;median price still $32,000 higher than last year&amp;#8221;). For those who&amp;#8217;ve taken a probability course, quantiles must sound familiar. But, what &lt;em&gt;is&lt;/em&gt; a quantile, or a percentile? And how do we compute and interpret&amp;nbsp;them?&lt;/p&gt;
&lt;p&gt;This came up for me and my colleagues at BetterUp recently. We had run a typical analysis and ended up computing some important quantiles of the data. To ensure our story was clear and crisp, we asked, What exactly happens when we run &lt;code&gt;quantile()&lt;/code&gt; in R? What does the output really mean? We all sort of knew, but I wish I&amp;#8217;d firmly known the&amp;nbsp;answer.&lt;/p&gt;
&lt;p&gt;Let&amp;#8217;s take a look at two examples that help show what sorts of questions can arise when computing&amp;nbsp;quantiles.&lt;/p&gt;
&lt;p&gt;First, in example A, consider the 0th, 25th, 50th (median), 75th, and 100th quantiles of the numbers 0 through&amp;nbsp;10:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;quantile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;10&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="code-output"&gt;
  0%  25%  50%  75% 100%
 0.0  2.5  5.0  7.5 10.0
&lt;/pre&gt;

&lt;p&gt;And now in example B, let&amp;#8217;s look at the same quantiles for just three numbers, 0, 1, and&amp;nbsp;2:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;quantile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="code-output"&gt;
  0%  25%  50%  75% 100%
 0.0  0.5  1.0  1.5  2.0
&lt;/pre&gt;

&lt;p&gt;Some questions that come to my mind include, how can we have a quantile that&amp;#8217;s not in the original data (e.g. 7.5 in example A)? How can 50% of our data in example B be less than 1, when we know that one out of three numbers in the data are less than 1? If the 0th-quantile means 0% of the data is less than 0, how can we have it such that 100% of the data is less than the maximum (but not include the&amp;nbsp;maximum)?&lt;/p&gt;
&lt;p&gt;In this article we will explore and learn about what quantiles are (since I seem to have forgotten), and will dissect the &lt;code&gt;quantile()&lt;/code&gt; function in&amp;nbsp;R. &lt;/p&gt;
&lt;h1&gt;What&amp;#8217;s a&amp;nbsp;Quantile?&lt;/h1&gt;
&lt;p&gt;We&amp;#8217;ll talk about quantiles here, but there&amp;#8217;s a summary below about medians, quartiles, and percentiles. To begin though, I&amp;#8217;ve found &lt;a href="https://www.google.com/books/edition/Statistical_Modelling_with_Quantile_Func/7c1LimP_e-AC?hl=en&amp;amp;gbpv=1&amp;amp;pg=PP1&amp;amp;printsec=frontcover"&gt;Gilchrist&amp;#8217;s (2000)&lt;/a&gt; definition of a quantile the easiest to&amp;nbsp;understand:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A quantile is simply the value that corresponds to a specified proportion of a sample or&amp;nbsp;population.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A couple things stand out to me here. First, I like his use of &amp;#8220;proportion&amp;#8221; â this feels to me like the easiest-to-understand term to use. Second, notice the use of both sample and population. We know going forward then that we need to consider&amp;nbsp;both.&lt;/p&gt;
&lt;p&gt;A quantile is best seen as a function, where we put in a proportion from 0 to 1 (inclusive) and the output denotes the value of the data that has that proportion of data less than&amp;nbsp;it.&lt;/p&gt;
&lt;p&gt;As &lt;a href="https://stat.ethz.ch/R-manual/R-devel/library/stats/html/quantile.html"&gt;the R documentation&lt;/a&gt; for the quantile function puts&amp;nbsp;it:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The generic function &lt;code&gt;quantile&lt;/code&gt; produces sample quantiles corresponding to the given probabilities. The smallest observation corresponds to a probability of 0 and the largest to a probability of&amp;nbsp;1.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;They refer to the proportions as probabilities, and specifically indicate this function produces sample quantiles. What importance does the sample quantile hold for us? &lt;a href="https://www.maths.usyd.edu.au/u/UG/SM/STAT3022/r/current/Misc/Sample%20Quantiles%20in%20Statistical%20Packages.pdf"&gt;Hyndman and Fan (1996)&lt;/a&gt;&amp;nbsp;specifies:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Sample quantiles provide nonparametric estimators of their population&amp;nbsp;counterparts.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here we have a very critical relationship: the sample quantile estimates the population quantile. Like the standard deviation, or even the mean, we have definitions for the sample estimators in the event we don&amp;#8217;t have the population parameters (which is almost always the case). &lt;a href="https://en.wikipedia.org/wiki/Estimator"&gt;Sample estimators&lt;/a&gt; aim to fulfill a number of qualities that make one better than&amp;nbsp;another.&lt;/p&gt;
&lt;p&gt;This means it&amp;#8217;s an open question &lt;em&gt;how&lt;/em&gt; to estimate the population parameters. Over many decades, many researchers have identified multiple ways we can do that. Hyndman and Fan (1996) review nine different ways to estimate the sample quantiles. R implements these and sets a default for the&amp;nbsp;user.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;In review, a quantile is the value that corresponds to a specified proportion of the sample data or population.&lt;/strong&gt; Almost never do we know the population parameters, so we must rely on estimating them with sample estimators. The &lt;code&gt;quantile&lt;/code&gt; funciton in R provides nine different quantile sample estimator functions for the user to choose&amp;nbsp;from.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;What are the median, quartiles, and percentiles?&lt;/summary&gt;

&lt;p&gt;We can refer to Gilchrist (2000) for the definition of the&amp;nbsp;median:&lt;/p&gt;

&lt;blockquote&gt;
Thus the median of a sample of data is the quantile corresponding to a proportion of 0.5 of the ordered data. For a theoretical population it corresponds to the quantile with probability of 0.5.
&lt;/blockquote&gt;

&lt;p&gt;Quartiles correspond to proportions of the data at 0.25 and 0.75 as well. Percentiles correspond to proprtions of the data every 1-percentage points. So, the 50th-percentile corresponds to the 50th-quantile which corresponds to the 2nd-quartiles which corresponds to the&amp;nbsp;median.&lt;/p&gt;

&lt;/details&gt;

&lt;!-- ## Formulation

So if Q(p) is the quantile function, the median is Q(0.5).

The Quantile Function, denoted $Q(p)$:

$x_p =$ the value of $x$ for which $P(X \le x_p) = p$

$x_p$ is called the p-quantile of the population. $Q(p) = x_p$. --&gt;

&lt;h2&gt;Population&amp;nbsp;Quantiles&lt;/h2&gt;
&lt;p&gt;In this example, we&amp;#8217;ll look at population quantiles for the normal distribution. In this case we know how to parameterize the distribution and we have a continuous equation for the distribution. Both make the following case&amp;nbsp;possible.&lt;/p&gt;
&lt;p&gt;Here is the normal (gaussian) distribution, which you might be familiar&amp;nbsp;with:&lt;/p&gt;
&lt;p&gt;&lt;img data-src="/images/quantiles-fig1.png" class="uk-align-center" width="80%" height="" alt="" uk-img&gt;&lt;/p&gt;
&lt;p&gt;Most of the density of this distribution is between about -3 and 3. If we were to draw a random number from this distribution, it&amp;#8217;d probably be close to 0. After many draws, about 95% would be between -1.96 and 1.96. I drew this plot in R with the &lt;code&gt;dnorm&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;If you&amp;#8217;ve taken an intro probability course, you might recognize that with the &lt;a href="https://en.wikipedia.org/wiki/Cumulative_distribution_function"&gt;density function&lt;/a&gt; (like the one above), we can take the integral and compute the &lt;a href="https://en.wikipedia.org/wiki/Cumulative_distribution_function"&gt;cumulative density function&lt;/a&gt; (&lt;span class="caps"&gt;CDF&lt;/span&gt;). We do so&amp;nbsp;here:&lt;/p&gt;
&lt;p&gt;&lt;img data-src="/images/quantiles-fig2.png" class="uk-align-center" width="80%" height="" alt="" uk-img&gt;&lt;/p&gt;
&lt;p&gt;This makes the y-axis our cumulative density, and we know that the cumulative density describes at each value the proportion of data up to that value. So here I&amp;#8217;ve labeled the median, which describes 50% of the data up to that point. I drew this plot in R with the &lt;code&gt;pnorm&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;But as you can see, our median isn&amp;#8217;t yet an easily computed result of a function. We&amp;#8217;d have to take the y-axis value, 0.5, and compute the x-axis value of the blue line at that value. How do we make this easier to compute and&amp;nbsp;interpret?&lt;/p&gt;
&lt;p&gt;The answer is the take the inverse of the cumulative distribution, which is known as the Quantile&amp;nbsp;Function.&lt;/p&gt;
&lt;!-- , $Q(p) = F^{-1}(p)$. --&gt;

&lt;p&gt;&lt;img data-src="/images/quantiles-fig3.png" class="uk-align-center" width="80%" height="" alt="" uk-img&gt;&lt;/p&gt;
&lt;p&gt;It&amp;#8217;s possible to visualize this plot because the equation for the normal distribution has an inverse. Now, it&amp;#8217;s easy to take a proportion, say 0.5, and use that equation to quickly and easily output the value of the data that corresponds with it. I drew this plot in R with the &lt;code&gt;qnorm&lt;/code&gt; function.&lt;/p&gt;
&lt;h2&gt;Sample Estimation of Quantiles with Real&amp;nbsp;Data&lt;/h2&gt;
&lt;p&gt;Let&amp;#8217;s play with some real data. Here we&amp;#8217;ll take the publication dates of the top 20 most cited scientific papers in history. The years these papers were published are, in chronological&amp;nbsp;order,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="mf"&gt;1951&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1957&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1958&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1959&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1962&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1970&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1975&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1975&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1976&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1977&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1979&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1987&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1987&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1988&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1990&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1993&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1994&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1996&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1997&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;2008&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This data was put together in 2014 by Thompson Reuters/Web of Science and &lt;a href="https://www.nature.com/news/the-top-100-papers-1.16224"&gt;written up by a few at Nature&lt;/a&gt;. There&amp;#8217;s an alternative list put together by &lt;a href="https://www.nature.com/news/the-top-100-papers-1.16224#/alternative"&gt;Google Scholar&lt;/a&gt;.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;What are the most cited scientific papers?&lt;/summary&gt;

According to the list I found, the top three most cited papers are:
&lt;ol&gt;
&lt;li&gt;305,148 citations: Lowry, &lt;span class="caps"&gt;O. H.&lt;/span&gt;(1951). Protein measurement with the Folin phenol reagent. J biol Chem, 193,&amp;nbsp;265-275.&lt;/li&gt;
&lt;li&gt;213,005 citations: Laemmli, &lt;span class="caps"&gt;U. K.&lt;/span&gt;(1970). Cleavage of structural proteins during the assembly of the head of bacteriophage T4. Nature, 227(5259),&amp;nbsp;680-685.&lt;/li&gt;
&lt;li&gt;155,530 citations: Bradford, &lt;span class="caps"&gt;M. M.&lt;/span&gt;(1976). A rapid and sensitive method for the quantitation of microgram quantities of protein utilizing the principle of protein-dye binding. Analytical biochemistry, 72(1-2),&amp;nbsp;248-254.&lt;/li&gt;
&lt;/ol&gt;

&lt;/details&gt;

&lt;p&gt;This is 1-dimensional data, and here&amp;#8217;s a visualization of the&amp;nbsp;dates:&lt;/p&gt;
&lt;p&gt;&lt;img data-src="/images/quantiles-fig4.png" class="uk-align-center" width="80%" height="" alt="" uk-img&gt;&lt;/p&gt;
&lt;p&gt;For illustration purposes I&amp;#8217;ve added a little random horizontal jitter. You can see that our oldest paper was published in the 50s, and the most recent just before&amp;nbsp;2010.&lt;/p&gt;
&lt;p&gt;Creating a denisty plot is fairly easy; we just create a histogram of the&amp;nbsp;data.&lt;/p&gt;
&lt;p&gt;&lt;img data-src="/images/quantiles-fig5.png" class="uk-align-center" width="80%" height="" alt="" uk-img&gt;&lt;/p&gt;
&lt;p&gt;We&amp;#8217;ve bucketed in 10-year increments, and we can see that each decade from 1950 to 2010 has at least one paper which made it to the top 20 papers. The decade with the most top papers is the 1970s, with 6&amp;nbsp;papers.&lt;/p&gt;
&lt;p&gt;Now knowing how we progressed from density function to cumulative density function in the population example above, we have to do a similar step here. To do this, we sort our data and assign each a number in ascending order, up to 20. We know that 100% of our data lies less than or equal to 20 and above 0, so we can normalize the y-axis to set 20 to 1.0. This also holds with our definition of the Quantile Function above (the probability that data is less than or equal to 20 is&amp;nbsp;1.0).&lt;/p&gt;
&lt;p&gt;&lt;img data-src="/images/quantiles-fig6.png" class="uk-align-center" width="80%" height="" alt="" uk-img&gt;&lt;/p&gt;
&lt;p&gt;I found it confusing how the smallest data point, 1951, falls at &lt;span class="math"&gt;\(p=0.05\)&lt;/span&gt;. Why not 0? We&amp;#8217;re saying that the probability that the data are less than 1951 is one-twentieth or less. Or, equivalently, that 95% or more of the data are 1951 or more recent. As you can tell, it&amp;#8217;s very important to have the incusivity of range boundaries defined. We can see this slightly better in the next&amp;nbsp;section.&lt;/p&gt;
&lt;p&gt;We can though interpret this graph a little for some interesting results. For example, in this sample, see can see that about 50% of the data lie before 1980. Almost all of the data lie before 2000. All of the data lie after&amp;nbsp;1950.&lt;/p&gt;
&lt;p&gt;The next step is to formalize these conclusions into a formula that we can use to plug in numbers like 0.5, 0.95, or 0.0 to come to similar&amp;nbsp;conclusions.&lt;/p&gt;
&lt;p&gt;How we do this is where things get&amp;nbsp;interesting!&lt;/p&gt;
&lt;h3&gt;Sample Estimation of&amp;nbsp;Quantiles&lt;/h3&gt;
&lt;p&gt;Unlike our theoretical population example, whose density function is based off a continuous equation, the sample density function is not defined by an invertible function. This is always true when working with a set of raw sample&amp;nbsp;data.&lt;/p&gt;
&lt;p&gt;Because the sample density function, and thus &lt;span class="caps"&gt;CDF&lt;/span&gt;, are step functions, we can&amp;#8217;t take the inverse. This would lead to some input values resulting in multiple output values, which means this cannot be a&amp;nbsp;function.&lt;/p&gt;
&lt;p&gt;Instead, we need to estimate the population quantile function using one of many estimation methods. Sample quantiles provide nonparametric estimators of their population counterparts. Researchers have created these estimators over the years, and popular packages like the &lt;code&gt;stats&lt;/code&gt; package in R allow the user to choose one from a subset of these. We&amp;#8217;ll take a look at three such&amp;nbsp;methods.&lt;/p&gt;
&lt;h4&gt;Discontinuous Sample&amp;nbsp;Quantiles&lt;/h4&gt;
&lt;p&gt;The easiest and least useful way to compute sample quantiles is to take our ordered step data and compute quantiles for only those data points along the step&amp;nbsp;function.&lt;/p&gt;
&lt;p&gt;Here is a table of our ordered data, the index of that year, and the computed quantile of that year (the index divided by&amp;nbsp;N):&lt;/p&gt;
&lt;div id="avkrsexhvb"&gt;
&lt;table class="gt_table"&gt;

  &lt;thead class="gt_col_headings"&gt;
    &lt;tr&gt;
      &lt;th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1"&gt;Year&lt;/th&gt;
      &lt;th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1"&gt;Index&lt;/th&gt;
      &lt;th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1"&gt;Quantile&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody class="gt_table_body"&gt;
    &lt;tr&gt;&lt;td class="gt_row gt_right"&gt;1951&lt;/td&gt;
&lt;td class="gt_row gt_right"&gt;1&lt;/td&gt;
&lt;td class="gt_row gt_right"&gt;0.05&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td class="gt_row gt_right"&gt;1957&lt;/td&gt;
&lt;td class="gt_row gt_right"&gt;2&lt;/td&gt;
&lt;td class="gt_row gt_right"&gt;0.10&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td class="gt_row gt_right"&gt;1958&lt;/td&gt;
&lt;td class="gt_row gt_right"&gt;3&lt;/td&gt;
&lt;td class="gt_row gt_right"&gt;0.15&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td class="gt_row gt_right"&gt;1959&lt;/td&gt;
&lt;td class="gt_row gt_right"&gt;4&lt;/td&gt;
&lt;td class="gt_row gt_right"&gt;0.20&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td class="gt_row gt_right"&gt;1962&lt;/td&gt;
&lt;td class="gt_row gt_right"&gt;5&lt;/td&gt;
&lt;td class="gt_row gt_right"&gt;0.25&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td class="gt_row gt_right"&gt;1970&lt;/td&gt;
&lt;td class="gt_row gt_right"&gt;6&lt;/td&gt;
&lt;td class="gt_row gt_right"&gt;0.30&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td class="gt_row gt_right"&gt;1975&lt;/td&gt;
&lt;td class="gt_row gt_right"&gt;7&lt;/td&gt;
&lt;td class="gt_row gt_right"&gt;0.35&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td class="gt_row gt_right"&gt;1975&lt;/td&gt;
&lt;td class="gt_row gt_right"&gt;8&lt;/td&gt;
&lt;td class="gt_row gt_right"&gt;0.40&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td class="gt_row gt_right"&gt;1976&lt;/td&gt;
&lt;td class="gt_row gt_right"&gt;9&lt;/td&gt;
&lt;td class="gt_row gt_right"&gt;0.45&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td class="gt_row gt_right"&gt;1977&lt;/td&gt;
&lt;td class="gt_row gt_right"&gt;10&lt;/td&gt;
&lt;td class="gt_row gt_right"&gt;0.50&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td class="gt_row gt_right"&gt;1979&lt;/td&gt;
&lt;td class="gt_row gt_right"&gt;11&lt;/td&gt;
&lt;td class="gt_row gt_right"&gt;0.55&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td class="gt_row gt_right"&gt;1987&lt;/td&gt;
&lt;td class="gt_row gt_right"&gt;12&lt;/td&gt;
&lt;td class="gt_row gt_right"&gt;0.60&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td class="gt_row gt_right"&gt;1987&lt;/td&gt;
&lt;td class="gt_row gt_right"&gt;13&lt;/td&gt;
&lt;td class="gt_row gt_right"&gt;0.65&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td class="gt_row gt_right"&gt;1988&lt;/td&gt;
&lt;td class="gt_row gt_right"&gt;14&lt;/td&gt;
&lt;td class="gt_row gt_right"&gt;0.70&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td class="gt_row gt_right"&gt;1990&lt;/td&gt;
&lt;td class="gt_row gt_right"&gt;15&lt;/td&gt;
&lt;td class="gt_row gt_right"&gt;0.75&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td class="gt_row gt_right"&gt;1993&lt;/td&gt;
&lt;td class="gt_row gt_right"&gt;16&lt;/td&gt;
&lt;td class="gt_row gt_right"&gt;0.80&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td class="gt_row gt_right"&gt;1994&lt;/td&gt;
&lt;td class="gt_row gt_right"&gt;17&lt;/td&gt;
&lt;td class="gt_row gt_right"&gt;0.85&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td class="gt_row gt_right"&gt;1996&lt;/td&gt;
&lt;td class="gt_row gt_right"&gt;18&lt;/td&gt;
&lt;td class="gt_row gt_right"&gt;0.90&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td class="gt_row gt_right"&gt;1997&lt;/td&gt;
&lt;td class="gt_row gt_right"&gt;19&lt;/td&gt;
&lt;td class="gt_row gt_right"&gt;0.95&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td class="gt_row gt_right"&gt;2008&lt;/td&gt;
&lt;td class="gt_row gt_right"&gt;20&lt;/td&gt;
&lt;td class="gt_row gt_right"&gt;1.00&lt;/td&gt;&lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;With this we basically have a look-up table of quantiles. It turns out that we can compute a few common quantiles, like the median and two other quartiles. Based on this table, 50% of the data come before 1977. We can compute these quantiles in R with &lt;code&gt;quantile(data, p, type=1)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img data-src="/images/quantiles-fig7.png" class="uk-align-center" width="80%" height="" alt="" uk-img&gt;&lt;/p&gt;
&lt;p&gt;But with this method, computing quantiles relies on the step function nature of the data. Quantiles are just rounded up, which isn&amp;#8217;t very accurate or &amp;#8220;sophisticated&amp;#8221;, and leads to jumps in quantiles.  Let&amp;#8217;s move onto a more useful&amp;nbsp;estimator.&lt;/p&gt;
&lt;h4&gt;Linear&amp;nbsp;Interpolation&lt;/h4&gt;
&lt;p&gt;One of the next simplest quantile estimators for a sample of data is linear interpolation. Under this method, we draw a line between each point and compute &lt;span class="math"&gt;\(x_p\)&lt;/span&gt; based on evaluating the line between two&amp;nbsp;p-quantiles.&lt;/p&gt;
&lt;p&gt;Let&amp;#8217;s say we wanted to compute the 97.5th-quantile. We know the 95th-quantile is 1997 and the 100th-quantile is 2008. With the above method, the 97.5th-quantile would be 2008. But we can try to more accurately estimate the population by interpolating between the two years. Consider the following equation to&amp;nbsp;interpolate:&lt;/p&gt;
&lt;div class="math"&gt;$$
x_p = X^n \frac{p_{k+1} - p}{p_{k+1} - p_k} + X^{n+1} \frac{p - p_k}{p_{k+1} - p_k}
$$&lt;/div&gt;
&lt;p&gt;The equation between 1997 and 2008 is like: &lt;span class="math"&gt;\(x_p = 1997(1 - p)/(1 - 0.95) + 2008(p - 0.95)/(1 - 0.95)\)&lt;/span&gt;, where &lt;span class="math"&gt;\(p \in [0.95, 1.0]\)&lt;/span&gt;. Thus, for the 97.5th-quantile, &lt;span class="math"&gt;\(x_p = 2002.5\)&lt;/span&gt;. Similarly, with the right values from the table above, &lt;span class="math"&gt;\(Q(0.475) = x_p = 1976.5\)&lt;/span&gt; We can confirm this in&amp;nbsp;R:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;quantile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;year&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;0.475&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;0.975&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="code-output"&gt;
 47.5%  97.5% 
1976.5 2002.5
&lt;/pre&gt;

&lt;p&gt;We can visualize both of these calculations like&amp;nbsp;so:&lt;/p&gt;
&lt;p&gt;&lt;img data-src="/images/quantiles-fig8.png" class="uk-align-center" width="80%" height="" alt="" uk-img&gt;&lt;/p&gt;
&lt;p&gt;We can now flip the axes to illustrate the functional nature (i.e. inverting the function) of this interpolation&amp;nbsp;method:&lt;/p&gt;
&lt;p&gt;&lt;img data-src="/images/quantiles-fig9.png" class="uk-align-center" width="80%" height="" alt="" uk-img&gt;&lt;/p&gt;
&lt;p&gt;I find it very fascinating to see the lower end of the data start to make itself clear. What before we saw as the quantiles &amp;#8220;starting&amp;#8221; at 0.05, we now see as just the result of the quantiles evaluating to the same value&amp;nbsp;(1951).&lt;/p&gt;
&lt;h4&gt;Mode-based&amp;nbsp;Estimate&lt;/h4&gt;
&lt;p&gt;Another way to write the above linear interpolation equation is like the&amp;nbsp;following:&lt;/p&gt;
&lt;div class="math"&gt;$$
x_p = x_{\lfloor h \rfloor} + (h - \lfloor h \rfloor)(x_{\lceil h \rceil} - x_{\lfloor h \rfloor})
$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(h = Np\)&lt;/span&gt;. For example, for 20 data points and the quantile 0.55, we have &lt;span class="math"&gt;\(x_p=1979\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This formula has the benefit of being slightly more general, and is the form that five commonly used estimators use. One of these is the mode-based estimator. This is the default in R when we use the &lt;code&gt;quantile&lt;/code&gt; function, so it&amp;#8217;s very important we understand&amp;nbsp;it.&lt;/p&gt;
&lt;p&gt;The mode-based estimator has &lt;span class="math"&gt;\(h = (N - 1)p + 1\)&lt;/span&gt;. Thus, when &lt;span class="math"&gt;\(p=0.55\)&lt;/span&gt; we have &lt;span class="math"&gt;\(x_p = 1982.6\)&lt;/span&gt;. We can confirm this with&amp;nbsp;R:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;quantile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;year&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;0.55&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# type = 7&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="code-output"&gt;
   55%
1982.6
&lt;/pre&gt;

&lt;p&gt;It&amp;#8217;s amazing how different this value is, a full three and a half years different from the linear interpolation&amp;nbsp;method!&lt;/p&gt;
&lt;p&gt;&lt;img data-src="/images/quantiles-fig10.png" class="uk-align-center" width="80%" height="" alt="" uk-img&gt;&lt;/p&gt;
&lt;p&gt;This estimation method takes care of the lower end of the data phenomenon that we ran into with linear interpolation. Mode-based estimation also appears somewhat&amp;nbsp;smoother.&lt;/p&gt;
&lt;p&gt;Hyndman and Fan (1996) outlined six characteristics that a sample quantile estimator should possess. The mode-based estimator satisfied five of six. Although it is distribution-free and generally a fine method to use, it is not unbiased. The best contender in my opinion is the median-based estimate (type 8 in R) since it would be approximately median&amp;nbsp;unbiased.&lt;/p&gt;
&lt;h3&gt;Estimation&amp;nbsp;Evaluation&lt;/h3&gt;
&lt;p&gt;Just for fun, we&amp;#8217;ll compare how the quantile estimators evaluate when generalized to the remaining top 80&amp;nbsp;papers.&lt;/p&gt;
&lt;p&gt;For example purposes, we&amp;#8217;ll consider only papers published after 1940. Here are the linear interpolation sample quantiles for the top 20 papers (blue) and all top 100 papers&amp;nbsp;(gray):&lt;/p&gt;
&lt;p&gt;&lt;img data-src="/images/quantiles-fig11.png" class="uk-align-center" width="80%" height="" alt="" uk-img&gt;&lt;/p&gt;
&lt;p&gt;It looks like when we have more data, the quantile line is more linear, with fewer jumps. This is because we have more data to fill in some of those date gaps that showed up in the smaller sample. Remember that points here denote percentiles, not sample&amp;nbsp;data.&lt;/p&gt;
&lt;p&gt;It&amp;#8217;s not incredibly useful in this situation, but we can also visualize a Q-Q plot to visualize the differences between the two sample&amp;nbsp;distributions:&lt;/p&gt;
&lt;p&gt;&lt;img data-src="/images/quantiles-fig12.png" class="uk-align-center" width="80%" height="" alt="" uk-img&gt;&lt;/p&gt;
&lt;p&gt;The dashed line is &lt;span class="math"&gt;\(y=x\)&lt;/span&gt;. Based on this image, we can tell that the two sample distributions are fairly similar, but not exactly the same. Each point represents a percentile (thus, 100 points shown). Areas of the graph where the points line up on the &lt;span class="math"&gt;\(y=x\)&lt;/span&gt; line indicate points in the distributions that have the same proportion of data below them. For example, you can see that 1975 or so is one of these points: about the same amount of data is before 1975 in both distributions. Areas where the line is steeper than &lt;span class="math"&gt;\(y=x\)&lt;/span&gt; indicate areas where the data in the vertical distribution is more dispersed than the horizontal. Likewise, those flat parts indicate the data is more dispersed in the horizontal&amp;nbsp;distribution.&lt;/p&gt;
&lt;p&gt;If we considered the top 100 papers as the theoretical population, we might choose not to use the top 20 papers to estimate quantiles of the&amp;nbsp;population.&lt;/p&gt;
&lt;p&gt;To round everything out, let&amp;#8217;s calculate the sample median of the top 20 papers using a few different methods and compare that to our &amp;#8220;population&amp;#8221; median (from all 100&amp;nbsp;papers):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;quantile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;all_paper_years&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;type&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="code-output"&gt;
 50%
1983
&lt;/pre&gt;

&lt;p&gt;This is our &amp;#8220;population&amp;#8221; median (I use type 1 because this is avoids any interpolation at all of the quantiles). Now, our sample median&amp;nbsp;estimates:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;types&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;quantiles&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;sapply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;types&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nf"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="nf"&gt;quantile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;top_20_paper_years&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;type&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;names&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;F&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="nf"&gt;names&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;quantiles&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;types&lt;/span&gt;
&lt;span class="n"&gt;quantiles&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="code-output"&gt;
   1    4    7    8
1977 1977 1978 1978
&lt;/pre&gt;

&lt;p&gt;As you can see, types 7 and 8 are closest to our &amp;#8220;population&amp;#8221; median by a full year. If we are concerned about an area with more dispersion in the sample data, for example at &lt;span class="math"&gt;\(p=0.55\)&lt;/span&gt;, then we actually see type 8 with an estimation error of only ten months. Type 7 has an error of 17 months, and types 4 and 1 have an error of five years! Clearly the latter are not particularly (and relatively) good (unbiased) estimators in this&amp;nbsp;case.&lt;/p&gt;
&lt;h1&gt;Review&lt;/h1&gt;
&lt;p&gt;A few key takeaways from this&amp;nbsp;article:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A quantile is the value that corresponds to a specified proportion of the sample data or&amp;nbsp;population.&lt;/li&gt;
&lt;li&gt;We usually rely on estimating quantiles with a sample&amp;nbsp;estimator.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;quantile&lt;/code&gt; function in R provides nine different quantile sample estimator functions for the user to choose from. The default is mode-based estimation, which may not be the best estimator for your&amp;nbsp;problem.&lt;/li&gt;
&lt;li&gt;Consider a type 1 (inverse &lt;span class="caps"&gt;CDF&lt;/span&gt;) estimator if you want no interpolation of quantiles; type 4 (linear interpolation) if you want to be able to easily explain the mechanics of the interpolation; type 7 (mode-based estimation) if you want your results to match anyone who uses the default in R; and type 8 (median-based estimation) if you&amp;#8217;d like to push for a more unbiased&amp;nbsp;estimator.&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="meta"></category><category term="Statistics"></category><category term="Rlang"></category></entry><entry><title>Replication: A room with a greenÂ view</title><link href="/posts/2022/Jul/replication-a-room-with-a-green-view/" rel="alternate"></link><published>2022-07-31T00:00:00-07:00</published><updated>2022-07-31T00:00:00-07:00</updated><author><name>Alex Liebscher</name></author><id>tag:None,2022-07-31:/posts/2022/Jul/replication-a-room-with-a-green-view/</id><summary type="html">&lt;p&gt;Reproducing &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; expanding on the results of a recent article on the link between exposure to greenspace and mental&amp;nbsp;well-being&lt;/p&gt;</summary><content type="html">&lt;p&gt;Another article lamenting the &lt;a href="https://en.wikipedia.org/wiki/Replication_crisis"&gt;Replication Crisis&lt;/a&gt; within the social sciences would be redundant and unnecessary. For those who are unfamiliar though: the replication crisis is a recent phenomenon in the social sciences, especially psychology, where a large number of scientific research articles aren&amp;#8217;t able to have their results found again by independent research teams. This can be because of how the research was conducted (e.g. due to their methods), but also some researcher have been found to have fabricated their data in order to produce favorable&amp;nbsp;results.&lt;/p&gt;
&lt;p&gt;Currently, I don&amp;#8217;t have the focus to plan out and rerun in full any particular research article. But there&amp;#8217;s a number of other ways we can validate or test published research. This is especially, or only, true if that article has made their data publicly accessible. For example, we can use the data they&amp;#8217;ve collected and published to try to reproduce the results they report. Or, we could apply different methods for analyzing that data to see if the results hold&amp;nbsp;up.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;In this article I&amp;#8217;m doing both of these with a paper I recently found that ran a survey to assess the relationship between being outdoors and mental well-being.&lt;/strong&gt; The authors published their data, and the article is open access. We&amp;#8217;re going to download their data, try to reproduce their results, and compare those results to alternative&amp;nbsp;methods.&lt;/p&gt;
&lt;h2&gt;Article: &lt;em&gt;A room with a green&amp;nbsp;view&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img uk-img data-src="/images/window-overlooking-green-pasture.png" width="50%" style="margin-left:25%;" /&gt;&lt;/p&gt;
&lt;p&gt;The article we&amp;#8217;re looking at is titled &amp;#8220;A room with a green view: the importance of nearby nature for mental health during the &lt;span class="caps"&gt;COVID&lt;/span&gt;-19 pandemic&amp;#8221; (2021) by Masashi Soga, Maldwyn Evans, Kazuaki Tsuchiya, and Yuya Fukano. It was submitted to the journal &lt;em&gt;Ecological Applications&lt;/em&gt; (&lt;a href="https://en.wikipedia.org/wiki/Impact_factor"&gt;&lt;span class="caps"&gt;IF&lt;/span&gt;&lt;/a&gt;: 6.1) in fall 2020, and published in March&amp;nbsp;2021.&lt;/p&gt;
&lt;p&gt;The authors report on a survey they conducted in June 2020 of a sample of 3,000 people in Tokyo, assessing their mental well-being, their access and exposure to the natural world, and a set of lifestyle&amp;nbsp;behaviors.&lt;/p&gt;
&lt;p&gt;The authors claim that their results demonstrate that the experiences of nearby nature, including just viewing nature out their windows at home, can help prevent poor mental helath during stressful time periods. They explain the positive relationships they found in three ways: direct multi-sensory interactions with nature improve mental health; greenspace use may encourage physical activity, which in turn improves mental health; and greenspace use promotes interaction with one&amp;#8217;s community, which in turn improves mental&amp;nbsp;health.&lt;/p&gt;
&lt;p&gt;The article is of some value to researchers in ecology, but I think it is of greater value for policymakers and those in the psychological sciences as additional evidence that greenspaces are a meaningful and straightforward intervention to improve mental&amp;nbsp;well-being.&lt;/p&gt;
&lt;h2&gt;Replication of published&amp;nbsp;results&lt;/h2&gt;
&lt;p&gt;I wasn&amp;#8217;t able to find any analysis code published by the authors, which has revealed a couple ambiguities in their paper. Nonetheless, I was mostly able to reproduce the results that they reported with the data they made&amp;nbsp;available.&lt;/p&gt;
&lt;p&gt;First off, &lt;strong&gt;I was able to successfully reproduce all of their descriptive statistics of the data&lt;/strong&gt;. This importantly included the proportion of greenspace use frequencies and ability to view nature out people&amp;#8217;s windows. For example, the average respondant spent an hour in greenspaces per week in the month leading up to the survey. 81% said they had a view of nature from a room in their house they spent much of their time. The authors claimed there was a correlation of &lt;span class="math"&gt;\(r=0.69\)&lt;/span&gt; between greenspace use frequency and duration, which I was also able to&amp;nbsp;replicate.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;I was only somewhat able to replicate their primary findings&lt;/strong&gt; between their five measures of mental well-being and their measures of greenspace use and nature view. Some snags I ran into&amp;nbsp;included:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Why they used corrected &lt;span class="caps"&gt;AIC&lt;/span&gt;, when they have only a fraction of the number of parameters compared to number of observations. Although corrected &lt;span class="caps"&gt;AIC&lt;/span&gt; is well-known, it seems to me like they just used the defaults of the modeling package they&amp;nbsp;used.&lt;/li&gt;
&lt;li&gt;How did they come up with a delta of 6 for corrected &lt;span class="caps"&gt;AIC&lt;/span&gt; as their model inclusion&amp;nbsp;criterium?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Despite the things I took for granted, our main figures look very similar, as do our tables of&amp;nbsp;results.&lt;/p&gt;
&lt;p&gt;&lt;img data-src="/images/pos-mental-health-coefs.png" width="100%" height="" uk-img/&gt;
&lt;img data-src="/images/neg-mental-health-coefs.png" width="100%" height="" uk-img/&gt;&lt;/p&gt;
&lt;p&gt;Of note, I did not include the &lt;span class="caps"&gt;NVDI&lt;/span&gt; data which they said they included because I do not have the software. This could be the reason why our results differ. It&amp;#8217;s unclear why they didn&amp;#8217;t include this variable in their shared data &lt;span class="caps"&gt;CSV&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Our results differ in these notable ways: the authors&amp;#8217; coefficients for Green View seem larger than mine, some of the magnitudes of my coefficients for Green View and Greenspace View do not align with the authors&amp;#8217;, and the authors&amp;#8217; CIs for Housing Type factor levels seem tighter than&amp;nbsp;mine.&lt;/p&gt;
&lt;h2&gt;Multiverse&lt;/h2&gt;
&lt;p&gt;While I was on BetterUp&amp;#8217;s Summer Break, I read a lot. One thing I read into was multiverse analysis. This is a concept thought up independently by multiple research groups. Its goal is to reduce the arbitrariness in statistical modeling by accounting for a variety of operationalizations in your model setup. This helps prevent &amp;#8220;edge of the knife&amp;#8221; results (regarding the fragility of one&amp;#8217;s&amp;nbsp;results/conclusions).&lt;/p&gt;
&lt;h3&gt;Processing&amp;nbsp;Choices&lt;/h3&gt;
&lt;p&gt;One of the first steps in a multiverse analysis is to assess the possible forking paths of decisions a researcher might make when doing their analysis. Once these paths are identified, we can descirbe them such that we can essentially evaluate each path. Having results for all possible permutations of the data slicing and dicing leaves us with a more robust way to view the results in the face of researcher&amp;nbsp;decisions.&lt;/p&gt;
&lt;p&gt;While it&amp;#8217;s arguable that their analysis is more robust because they computed a model average, this model average can only account for arbitrary inclusion or exclusion of predictors. A multiverse on the other hand, can include decisions about variable transformation (&lt;span class="caps"&gt;IV&lt;/span&gt; and &lt;span class="caps"&gt;DV&lt;/span&gt;), outlier exclusion, and nonlinear transformations (e.g.&amp;nbsp;interactions).&lt;/p&gt;
&lt;p&gt;Here are some of the processing choices I came up&amp;nbsp;with:&lt;/p&gt;
&lt;ol&gt;
    &lt;li&gt;
    Colinearity of nature predictors
    &lt;ol type="a"&gt;
        &lt;li&gt;&lt;span class="caps"&gt;CN1&lt;/span&gt;: Use only Greenspace Use&amp;nbsp;Frequency&lt;/li&gt;
        &lt;li&gt;&lt;span class="caps"&gt;CN2&lt;/span&gt;: Use both Greenspace Use Frequency and Nature&amp;nbsp;View&lt;/li&gt;
    &lt;/ol&gt;
    &lt;/li&gt;
    &lt;li&gt;
    Greenspace use log-transformed
    &lt;ol type="a"&gt;
        &lt;li&gt;&lt;span class="caps"&gt;GSL1&lt;/span&gt;:&amp;nbsp;Identity&lt;/li&gt;
        &lt;li&gt;&lt;span class="caps"&gt;GSL2&lt;/span&gt;: Greenspace Use Frequency&amp;nbsp;log-transformed&lt;/li&gt;
    &lt;/ol&gt;
    &lt;/li&gt;
    &lt;li&gt;
    Exclusion of &amp;#8220;Other&amp;#8221; Housing Types
    &lt;ol type="a"&gt;
        &lt;li&gt;&lt;span class="caps"&gt;EXHT1&lt;/span&gt;: No&amp;nbsp;exclusion&lt;/li&gt;
        &lt;li&gt;&lt;span class="caps"&gt;EXHT2&lt;/span&gt;: Exclude respondents with Housing Type of&amp;nbsp;&amp;#8220;Other&amp;#8221;&lt;/li&gt;
    &lt;/ol&gt;
    &lt;/li&gt;
    &lt;li&gt;
    Including only full-time workers
    &lt;ol type="a"&gt;
        &lt;li&gt;&lt;span class="caps"&gt;FTW1&lt;/span&gt;:&amp;nbsp;Everyone&lt;/li&gt;
        &lt;li&gt;&lt;span class="caps"&gt;FTW2&lt;/span&gt;: Only individuals who work 5 or more days per&amp;nbsp;week&lt;/li&gt;
    &lt;/ol&gt;
    &lt;/li&gt;
    &lt;li&gt;
    Presence of demographic predictors
    &lt;ol type="a"&gt;
        &lt;li&gt;&lt;span class="caps"&gt;DP1&lt;/span&gt;: Not&amp;nbsp;present&lt;/li&gt;
        &lt;li&gt;&lt;span class="caps"&gt;DP2&lt;/span&gt;: Include age and&amp;nbsp;sex&lt;/li&gt;
    &lt;/ol&gt;
    &lt;/li&gt;
    &lt;li&gt;
    Assessment of Age
    &lt;ol type="a"&gt;
        &lt;li&gt;&lt;span class="caps"&gt;AB1&lt;/span&gt;: Age&amp;nbsp;z-scored&lt;/li&gt;
        &lt;li&gt;&lt;span class="caps"&gt;AB2&lt;/span&gt;: Age commonly&amp;nbsp;binned&lt;/li&gt;
    &lt;/ol&gt;
    &lt;/li&gt;
    &lt;li&gt;
    Presence of income predictors
    &lt;ol type="a"&gt;
        &lt;li&gt;&lt;span class="caps"&gt;IP1&lt;/span&gt;: Not&amp;nbsp;present&lt;/li&gt;
        &lt;li&gt;&lt;span class="caps"&gt;IP2&lt;/span&gt;: Include income and impact of &lt;span class="caps"&gt;COVID&lt;/span&gt;-19 on&amp;nbsp;income&lt;/li&gt;
    &lt;/ol&gt;
    &lt;/li&gt;
    &lt;li&gt;
    Income log-transformed
    &lt;ol type="a"&gt;
        &lt;li&gt;&lt;span class="caps"&gt;ILP1&lt;/span&gt;:&amp;nbsp;Identity&lt;/li&gt;
        &lt;li&gt;&lt;span class="caps"&gt;ILP2&lt;/span&gt;: Income is&amp;nbsp;log-transformed&lt;/li&gt;
    &lt;/ol&gt;
    &lt;/li&gt;
    &lt;li&gt;
    Presence of lifestyle predictors
    &lt;ol type="a"&gt;
        &lt;li&gt;&lt;span class="caps"&gt;LP1&lt;/span&gt;: Not&amp;nbsp;present&lt;/li&gt;
        &lt;li&gt;&lt;span class="caps"&gt;LP2&lt;/span&gt;: Include number of children, number of working days per week, presence of a pet, alchol drinking frequency, smoking frequency, and housing&amp;nbsp;type&lt;/li&gt;
    &lt;/ol&gt;
    &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This is equivalent to &lt;span class="math"&gt;\(2^9\)&lt;/span&gt; combinations or branches of analysis&amp;nbsp;logic.&lt;/p&gt;
&lt;p&gt;I used the &lt;code&gt;multiverse&lt;/code&gt; package to accomplish the branching logic and looping necessary to traverse all these combinations. This package actually seemed to work great. I was daunted when I started this that I&amp;#8217;d have to write all the logic myself, but the package fits seemlessly with &lt;code&gt;dplyr&lt;/code&gt; and they make it really easy to define all the different decisions. It took a little adjusting my code to fit the paradigm, but it was worth the startup&amp;nbsp;cost.&lt;/p&gt;
&lt;h3&gt;Results&lt;/h3&gt;
&lt;p&gt;Multiverse analyses aren&amp;#8217;t the kind of analysis where you get a single number back and try to make a decision based off that. Instead, you rely on visual inspection (usually of plots, but if you&amp;#8217;re into it tables work too) and human pattern recognition to measure your trust in the&amp;nbsp;hypotheses.&lt;/p&gt;
&lt;p&gt;For example, in the original analyses, the authors write in their&amp;nbsp;results:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Both greenspace use and green view were significantly positively associated with an increase in self-esteem, life satisfaction, and happiness and negatively associated with loneliness and depression and&amp;nbsp;anxiety&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Although they don&amp;#8217;t include p-values here, this result has five p-values sitting write behind it (for the five DVs). Instead of making a judgement based off just a single p-value, we can plot the p-values for all of our branching decisions. In this case, we have our five DVs, and for each the p-value of the average marginal effect of Greenspace&amp;nbsp;Use:&lt;/p&gt;
&lt;p&gt;&lt;img data-src="/images/pvalue-curves.png" width="80%" height="" style="margin-left:10%;" uk-img/&gt;&lt;/p&gt;
&lt;p&gt;This looks very promising! Notice how all of our analyses have a p-value on this marginal effect of less than 0.05. Now, 0.05 is also an arbitrary decision, but we can see here that most of our results are even less than a p-value of 0.01. This tells us that basically no matter how we operationalize the problem, according to our processing decisions above, we routinely believe that we can reject the null that there is no effect of the average marginal effect of Greenspace Use on each of our five mental health&amp;nbsp;signals.&lt;/p&gt;
&lt;p&gt;In laymans terms, Greenspace Use seems to have a strong relationship with mental health no matter how we set up those&amp;nbsp;models.&lt;/p&gt;
&lt;p&gt;Another way to visualize a multiverse analysis is a specification curve (&lt;a href="https://dx.doi.org/10.2139/ssrn.2694998"&gt;Simonsohn et al 2019&lt;/a&gt;). Here we again have our five DVs of interest, and for each, our average marginal effect sorted in ascending order, and colored whether it obtained&amp;nbsp;significance.&lt;/p&gt;
&lt;p&gt;&lt;img data-src="/images/specification-curves.png" width="80%" height="" style="margin-left:10%;"  uk-img/&gt;&lt;/p&gt;
&lt;p&gt;We see that virtually any arbitrary analytical decision renders the effect significant, providing massive support for rejecting the original&amp;nbsp;null.&lt;/p&gt;
&lt;h2&gt;Wrap&amp;nbsp;Up&lt;/h2&gt;
&lt;p&gt;With the data the authors provided, and the rough analytical framework they provided, we have strong evidence that the effect they identified exists. We&amp;#8217;ve come to this conclusion through a multiverse analysis which showed that among many arbitrary analytical decisions, we consistently were able to reject the null that there was no effect of Greenspace Use on our mental&amp;nbsp;health.&lt;/p&gt;
&lt;p&gt;Now, this might look good on paper but there are a few caveats. First, we were unable to include the &lt;span class="caps"&gt;NVDI&lt;/span&gt; data that the authors included. This seems like a substantial lack, and could easily make the story more complex. Second, we are &lt;em&gt;not&lt;/em&gt; making any new decisions about estimand: meaning, the effect of interest is the same as in the original paper. We aren&amp;#8217;t looking a new DVs, or new ways of measuring Greenspace Use. Third, we&amp;#8217;ve tested only linear regression models. While we could investigate other model structures, we don&amp;#8217;t in order to save&amp;nbsp;time.&lt;/p&gt;
&lt;p&gt;We also haven&amp;#8217;t discussed effect size. Just because we found a bunch of significant results, doesn&amp;#8217;t mean the effect we see has any real world meaning. By my judgement, an average marginal effect of less than 1 in this case indicates the effect is rather small. Not negligible, but not transformation. On the other hand though, we&amp;#8217;re measuring differences in mental health, which are subject to an infinitude of influences, so even a small impact can be&amp;nbsp;meaningful.&lt;/p&gt;
&lt;p&gt;We also have left all the theoretical implications to the authors, which I am somewhat underwhelmed by. For example, the authors question &lt;em&gt;why&lt;/em&gt; Greenspace Use might have this impact on mental health. They posit three&amp;nbsp;answers:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;There are several possible pathways through which greenspace use promotes mental health outcomes. The first, and most direct one, is the added health benefits of direct interactions with nature. &amp;#8230; Second, greenspace use is likely to encourage people to undertake physical exercise (so-called âgreen exerciseâ), which in turn contributes to improving mental health. Third, urban greenspace provides opportunities to interact with other members of local communities (e.g., friends), which is likely to ameliorate loneliness and improve&amp;nbsp;well-being.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The authors suggest that longitudinal studies be conducted in order to tease out&amp;nbsp;causality.&lt;/p&gt;
&lt;h3&gt;Lessons&lt;/h3&gt;
&lt;p&gt;All code and data available at &lt;a href="https://github.com/liebscher/replication-a-room-with-a-green-view"&gt;https://github.com/liebscher/replication-a-room-with-a-green-view&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here are a couple lessons I learned while doing this&amp;nbsp;project:&lt;/p&gt;
&lt;ol&gt;
    &lt;li&gt;Open Science is a good thing. I cannot think of one reason it could be bad. More researchers need to publish all of their data, and code too. Github is indispensable, but there are other platforms available&amp;nbsp;too.&lt;/li&gt;
    &lt;li&gt;After getting all my results, I wanted to put my work on Github. In the process, I accidentally deleted all my code. This is a reminder to start your repository before you begin coding, and to commit your progress consistently. I was a little bummed to have done this, but life moves on and I think my second version was&amp;nbsp;better.&lt;/li&gt;
    &lt;li&gt;I ended up thinking of more analytical branches as I went. This doesn&amp;#8217;t seem like the &amp;#8220;best&amp;#8221; way to go about constructing the set of&amp;nbsp;branches&lt;/li&gt;
    &lt;li&gt;Multiverse analyses can be valuable if you feel confident in your estimand, your data collection, and your theory. If those aren&amp;#8217;t settled though, you&amp;#8217;re just dressing up a fundamentally wrong&amp;nbsp;argument.&lt;/li&gt;
&lt;/ol&gt;

&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="meta"></category><category term="critique"></category><category term="statistics"></category><category term="rlang"></category><category term="psychology"></category><category term="ecology"></category></entry><entry><title>Highlights from PyConÂ 2022</title><link href="/posts/2022/May/pycon-2022-highlights/" rel="alternate"></link><published>2022-05-16T00:00:00-07:00</published><updated>2022-05-16T00:00:00-07:00</updated><author><name>Alex Liebscher</name></author><id>tag:None,2022-05-16:/posts/2022/May/pycon-2022-highlights/</id><summary type="html">&lt;p&gt;Lessons from 18 talks at this year&amp;#8217;s three days of&amp;nbsp;Pycon&lt;/p&gt;</summary><content type="html">&lt;p&gt;I was fortunate enough to attend PyCon 2022 this year in Salt Lake City (thanks BetterUp!). This is the annual Python conference, put on by the Python Software Foundation. It was my first time going and I had few expectations going into it. I really enjoyed my time, and learned a number of new things and ways of thinking. Here are some highlights and learnings from a subset of the talks I&amp;nbsp;attended.&lt;/p&gt;
&lt;h2&gt;Day&amp;nbsp;1&lt;/h2&gt;
&lt;table class="uk-table"&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th class="uk-width-1-6"&gt;Speaker&lt;/th&gt;
            &lt;th class="uk-width-1-5"&gt;Topic&lt;/th&gt;
            &lt;th&gt;Notes&lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;Åukasz Langa&lt;/td&gt;
            &lt;td&gt;Type Annotations&lt;/td&gt;
            &lt;td&gt;This was a pretty technical deep-dive into Python&amp;#8217;s typing system. It was a good reminder to me that this is a (useful) feature in Python that is here to stay. Check out &lt;a href="http://mypy-lang.org/" target="_new"&gt;mypy&lt;/a&gt; for implementation details. This was my first time hearing of Åukasz, who is a core developer of Python and known by many.&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;a href="https://treyhunner.com/" target="_new"&gt;Trey Hunner&lt;/a&gt;&lt;/td&gt;
            &lt;td&gt;Python Oddities Explained&lt;/td&gt;
            &lt;td&gt;A good reminder that Python is quirky, and to always be on the lookout for things that didn&amp;#8217;t match your expectations. See also &lt;a href="https://github.com/pablogsal/python-horror-show" target="_new"&gt;Python&amp;#8217;s Horror Show&lt;/a&gt; from Pablo S (talk below)&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;a href="https://github.com/brandtbucher/" target="_new"&gt;Brandt Bucher&lt;/a&gt;&lt;/td&gt;
            &lt;td&gt;Structural Pattern Matching&lt;/td&gt;
            &lt;td&gt;&lt;a href="https://peps.python.org/pep-0636/" target="_new"&gt;Pattern matching&lt;/a&gt; (new &lt;code&gt;match&lt;/code&gt; keyword) seems like a neat new feature in Python 3.11. Not sure yet how I&amp;#8217;d leverage it but I&amp;#8217;m happy to know it will exist. It may look like a switch statement, but it&amp;#8217;s not! It&amp;#8217;s for flow control and unpacking variables. It&amp;#8217;s more powerful and flexible than a switch statement, but also serves a slightly different purpose.&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;Reuven Lerner&lt;/td&gt;
            &lt;td&gt;Understanding Python attributes&lt;/td&gt;
            &lt;td&gt;
                &lt;p&gt;Ask yourself, how does a class object, an instance, an instance variable, an attribute, and a descriptor all relate? This is surprisingly complex question, in my opinion. It&amp;#8217;s one of those oddly confusing computer science-y things (similar to the reasons why I didn&amp;#8217;t want to study &lt;span class="caps"&gt;CS&lt;/span&gt;). Class objects support two kinds of operations: attribute references and instantiation. With the former, you can access class attributes and variables. With the latter, once we have an instance object, we can only perform attribute reference. There are two instance attribute types: data attributes (sometimes called instance variables or class members) and methods. Instance variables are data unique for an instance, and class variables are data shared by all instances of the class. But what about a&amp;nbsp;descriptor?&lt;/p&gt;
                &lt;blockquote&gt;A descriptor is what we call any object that defines `__get__()`, `__set__()`, or `__delete__()` &amp;#8230; The main motivation for descriptors is to provide a hook allowing objects stored in class variables to control what happens during attribute lookup.&lt;/blockquote&gt;
                &lt;p&gt;If you&amp;#8217;re thoroughly confused, I understand. I recommend the Python docs &lt;a href="https://docs.python.org/3/tutorial/classes.html" target="_new"&gt;tutorial on classes&lt;/a&gt; as a starting point for&amp;nbsp;clarification.&lt;/p&gt;
            &lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;Paul Ganssle&lt;/td&gt;
            &lt;td&gt;&lt;a href="https://ganssle.io/talks/" target="_new"&gt;What to Do When the Bug Is in Someone Else&amp;#8217;s Code&lt;/a&gt;&lt;/td&gt;
            &lt;td&gt;Somewhat relevant to something I&amp;#8217;ve been doing, a good new way to look at &lt;span class="caps"&gt;OSS&lt;/span&gt; and how to work with it. The speaker discussed five ways that you can overcome bugs in code that you don&amp;#8217;t maintain. In order of best to worst: patching upstream (fix the bug, then make a &lt;span class="caps"&gt;PR&lt;/span&gt; on the project), wrapper functions (encapsulate buggy code in updated code), monkey patching (assign the global variable a fixed version), vendoring (clone a copy, then patch it locally), and maintaining a fork (fork and patch a copy of the project). I&amp;#8217;m guilty of some of the worse behaviors, but now I&amp;#8217;ve got some perspective on why they&amp;#8217;re &amp;#8220;bad&amp;#8221; that I didn&amp;#8217;t have before.&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;Nir Barazida&lt;/td&gt;
            &lt;td&gt;Dock your Jupyter Notebook&lt;/td&gt;
            &lt;td&gt;This talk proposed hosting your Jupyter-based data science and research projects in Docker images. I&amp;#8217;d thought of running scripts in containers, but this was specifically about running notebooks. The speaker introduced &lt;a href="https://github.com/jupyter/docker-stacks" target="_new"&gt;docker-stacks&lt;/a&gt;. I thought this was all very fascinating for creating reproducible research. However, I tried to quickly implement this with my own work and it wasn&amp;#8217;t so easy. I still find Docker tricky as soon as you want to do even the slightest customizations to the image and runtime. Also, Conda overcomes many barriers to reproducibility and Docker seems to only contribute marginally.&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;Maria Jose Molina Contreras&lt;/td&gt;
            &lt;td&gt;Creating an indoor air quality monitoring and predictive system&lt;/td&gt;
            &lt;td&gt;This was an exposition of a data science project. The speaker basically linked up some &lt;span class="caps"&gt;CO2&lt;/span&gt; and climate sensors to a microcontroller and slapped a prediction algorithm on top. A couple learnings. First, open the windows, get some air inside your room/office! Second, machine learning projects don&amp;#8217;t have to be crazy complex. You can get detailed, but it&amp;#8217;s not necessary to make a point. The most interesting projects are the ones with real world consequences, not just the titanic dataset or housing prices. Third, so much effort goes into everything that sets up any prediction model. Always remember that.&lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1&gt;Day&amp;nbsp;2&lt;/h1&gt;
&lt;table class="uk-table"&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th class="uk-width-1-6"&gt;Speaker&lt;/th&gt;
            &lt;th class="uk-width-1-5"&gt;Title&lt;/th&gt;
            &lt;th&gt;Notes&lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;a href="https://sissaoun.github.io/" target="_new"&gt;Sara Issaoun&lt;/a&gt;&lt;/td&gt;
            &lt;td&gt;&lt;/td&gt;
            &lt;td&gt;Hands down one of the best talks of the weekend. Dr. Issaoun&amp;#8217;s talk was an inspiration, and for an extremely complex subject (astrophysics, astronomy, blackholes, etc.) she made the audience (or at least me) feel like both experts and curious children. If I had to pick one talk to re-attend, this would be the one. It was the only one where I went up to talk to talk to the speaker at the end.&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;Peter Wang&lt;/td&gt;
            &lt;td&gt;&lt;/td&gt;
            &lt;td&gt;Peter gave an introduction to and live-demo of &lt;a href="https://pyscript.net/" target="_new"&gt;PyScript&lt;/a&gt;, &amp;#8220;a framework that allows users to create rich Python applications in the browser using &lt;span class="caps"&gt;HTML&lt;/span&gt;âs interface.&amp;#8221; This was another extremely inspiring talk. I don&amp;#8217;t know exactly how I would use Python-in-the-browser but I can imagine it will open a world of possibilities, similar to Node. I&amp;#8217;m very eager to see where this project goes, and perhaps sometime soon I&amp;#8217;ll give it a try and write up my experience.&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;Fred Phillips&lt;/td&gt;
            &lt;td&gt;Hooking into Imports&lt;/td&gt;
            &lt;td&gt;This speaker introduced the idea of &amp;#8220;hooking&amp;#8221; into the Python import process. For example, it might be necessary to create a blocklist of packages that can&amp;#8217;t be imported; or load package code from a remote database. In both cases, it can be helpful to modify and overload the default package search and execution process. I currently have no use case for it, but it was good to learn about what&amp;#8217;s happening under the hood. I had never considered the process with which Python resolves and loads the modules we import.&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;Antoine Toubhans&lt;/td&gt;
            &lt;td&gt;Flexible &lt;span class="caps"&gt;ML&lt;/span&gt; Experiment Tracking&lt;/td&gt;
            &lt;td&gt;Data Version Control (&lt;span class="caps"&gt;DVC&lt;/span&gt;) is something I&amp;#8217;ve been thinking about for years, and just haven&amp;#8217;t tried it out yet. It seems like it could help solve a number of common data science and machine learning problems. I just need to learn it. I&amp;#8217;ve decided to try it out in a current project I&amp;#8217;m working on. There is a learning curve, but not near as confusing as something like Docker; more comparable to git. So far I&amp;#8217;m finding some value in the experiment tracking functionality. I don&amp;#8217;t really want to implement my own visualizations to &lt;a href="https://dvc.org/doc/use-cases/experiment-tracking" traget="_new"&gt;track experiments&lt;/a&gt;, as was proposed in this talk (with streamlit), and I&amp;#8217;m hoping the maintainers of the &lt;span class="caps"&gt;DVC&lt;/span&gt; library add in more viz tools.&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;Ryan Kuhl&lt;/td&gt;
            &lt;td&gt;GraphQL&lt;/td&gt;
            &lt;td&gt;I had heard of GraphQL before but haven&amp;#8217;t spent anytime learning about it. Therefore I thought this would be a good crash-course on the tool, but it wasn&amp;#8217;t. GraphQL is a way to query any database (although it works well for graph DBs especially). My takeaways could be summed up as, this is a fascinating technology with a clear implementational benefit over other querying languages, but it seems highly engineered, clunky, and probably only worth it if you&amp;#8217;re dealing with a lot of data in a production environment.&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;a href="https://github.com/pablogsal" target="_new"&gt;Pablo Galindo Salgado&lt;/a&gt;&lt;/td&gt;
            &lt;td&gt;Making Python better w/ Errors&lt;/td&gt;
            &lt;td&gt;Python 3.11 and 3.12 will contain some big makeovers in the traceback and error diagnostic capabilities in Python. I&amp;#8217;m looking forward to having clearer and more specific errors and exceptions.&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;Kelly Schuster, Sean Paredes&lt;/td&gt;
            &lt;td&gt;Python like a 12-year-old&lt;/td&gt;
            &lt;td&gt;The speakers were two Python educators, who have experience teaching Python to middle schoolers. Their top learnings from working with these kids: Be curious! Take risks! Kids think broadly, unlike us adults. Engage all senses, not just what you can see (e.g. build something with your hands). Always be on the lookout for unexpected behavior. When solving a problem, think: what&amp;#8217;s the worst way to do this? (to force yourself to think in others&amp;#8217; shoes.)&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;Jeremiah Paige&lt;/td&gt;
            &lt;td&gt;Intro to Introspection&lt;/td&gt;
            &lt;td&gt;Python has many tools available for debugging, and although Python can be frustrating with its lack of verbosity sometimes, we must remember what we have at our disposal to dissect issues. For example, Python has the &lt;code&gt;&lt;a href="https://docs.python.org/3/library/inspect.html" target="_new"&gt;inspect&lt;/a&gt;&lt;/code&gt; built-in module to help diagnose runtime issues.&lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2&gt;Day&amp;nbsp;3&lt;/h2&gt;
&lt;table class="uk-table"&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th class="uk-width-1-6"&gt;Speaker&lt;/th&gt;
            &lt;th class="uk-width-1-5"&gt;Title&lt;/th&gt;
            &lt;th&gt;Notes&lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;Joseph Lucas&lt;/td&gt;
            &lt;td&gt;Serialization&lt;/td&gt;
            &lt;td&gt;This talk introduced serialization and ways to do this in Python. Serialization refers to breaking down data and objects (in Python, or elsewhere) and storing it with the intent to deserialize it later for use. The speaker first talked about the built-in &lt;a href="https://docs.python.org/3/library/pickle.html" target="_new"&gt;pickle&lt;/a&gt; module. This is a Python-specific module for serializing objects and data structures in a compressed byte-stream. Unlike &lt;span class="caps"&gt;JSON&lt;/span&gt; (another serialization format), pickles are binary, not unicode; pickles are not human readable; pickles can represent a wide number of Python objects; and deserializing pickles can pose an execution safety risk, unlike &lt;span class="caps"&gt;JSON&lt;/span&gt;. If we&amp;#8217;re trying to serialize an object that pickle cannot handle, there is the &lt;a href="https://pypi.org/project/dill/" target="_new"&gt;dill&lt;/a&gt; module.&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;Tetsuya &amp;#8220;Jesse&amp;#8221; Hirata&lt;/td&gt;
            &lt;td&gt;Productionize Research-Oriented Code&lt;/td&gt;
            &lt;td&gt;I went into this thinking it would cover how to write research code for your production engineers. Instead, it was how to read research code from your researchers as an engineer. This was a pleasant surprise, and was almost sort of a lesson in empathy. This reinforced my belief that my code and work is most valuable when it can be (quickly and easily) taken to a place of impact. Some reminders for myself: write clean code, and document it; modularize when possible; separate loading, cleaning, processing, and modeling; if you have time, look for ways to refactor.&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;Cillian Kieran&lt;/td&gt;
            &lt;td&gt;Open-Source Tools For Data Privacy&lt;/td&gt;
            &lt;td&gt;The &lt;a href="https://ethyca.com/fides/" target="_new"&gt;Ethyca fida&lt;/a&gt; folks have laid out a really appealing taxonomy of data privacy in the way of privacy-as-code. I love data privacy, and I lvoe categorizing things, especially data, so this is like candy to me. I don&amp;#8217;t know if I would try this out in reality, as it seems like a pretty heavy lift, but maybe one day.&lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;</content><category term="meta"></category><category term="Python"></category></entry><entry><title>Oddities and Surprises: Unusual Behaviors in R andÂ Python</title><link href="/posts/2022/May/oddities-and-surprises/" rel="alternate"></link><published>2022-05-13T00:00:00-07:00</published><updated>2022-05-13T00:00:00-07:00</updated><author><name>Alex Liebscher</name></author><id>tag:None,2022-05-13:/posts/2022/May/oddities-and-surprises/</id><summary type="html">&lt;p&gt;Questioning the developers of our favorite&amp;nbsp;languages&lt;/p&gt;</summary><content type="html">&lt;p&gt;This document is a collection of unexpected behaviors in R and Python. It&amp;#8217;s inspired by &lt;a href="https://treyhunner.com/python-oddities/resources.html"&gt;Python Oddities&lt;/a&gt; and &lt;a href="https://github.com/pablogsal/python-horror-show"&gt;Python&amp;#8217;s horror show&lt;/a&gt;. I will add to it over&amp;nbsp;time.&lt;/p&gt;
&lt;h1&gt;Python&lt;/h1&gt;
&lt;h2&gt;String&amp;nbsp;Strip&lt;/h2&gt;
&lt;h3&gt;Behavior&lt;/h3&gt;
&lt;p&gt;Let&amp;#8217;s say we have a&amp;nbsp;string:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;sentence&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;I love when monkeys write&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;When we run &lt;code&gt;sentence.rstrip('new york times')&lt;/code&gt;, as if we wanted to remove any case of &amp;#8220;new york times&amp;#8221; from the string, we would expect the string to go unchanged. After all, &amp;#8220;new york times&amp;#8221; is not mentioned anywhere in &lt;code&gt;sentence&lt;/code&gt;. Instead though, we get&amp;nbsp;this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;sentence&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rstrip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;new york times&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="code-output"&gt;'I love wh'&lt;/pre&gt;

&lt;h3&gt;Explanation&lt;/h3&gt;
&lt;p&gt;Unlike how one might guess based on their names, Python&amp;#8217;s &lt;code&gt;rstrip&lt;/code&gt; and similar string methods don&amp;#8217;t remove &lt;em&gt;suffixes&lt;/em&gt;. Instead, they remove trailing characters in the list provided. And as it turns out, &amp;#8220;New York Times&amp;#8221; is &lt;a href="https://en.wikipedia.org/wiki/Anagram"&gt;an anagram&lt;/a&gt; of &amp;#8220;monkeys write&amp;#8221; (plus the &amp;#8220;en&amp;#8221; in &amp;#8220;when&amp;#8221;). Therefore all of the characters in the former get found in the&amp;nbsp;latter.&lt;/p&gt;
&lt;p&gt;From the&amp;nbsp;documentation,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The chars argument is not a suffix; rather, all combinations of its values are&amp;nbsp;stripped&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;What&amp;#8217;s the &amp;#8220;right&amp;#8221; way to remove a suffix? The docs recommend &lt;a href="https://docs.python.org/3/library/stdtypes.html#str.removesuffix"&gt;str.removesuffix()&lt;/a&gt;. This is only implemented in Python 3.9 and&amp;nbsp;above.&lt;/p&gt;
&lt;h2&gt;Nested For&amp;nbsp;Loops&lt;/h2&gt;
&lt;h3&gt;Behavior&lt;/h3&gt;
&lt;h3&gt;Explanation&lt;/h3&gt;
&lt;p&gt;https://peps.python.org/pep-0202/&amp;nbsp;https://spapas.github.io/2016/04/27/python-nested-list-comprehensions/&lt;/p&gt;</content><category term="meta"></category><category term="Python"></category><category term="Rlang"></category></entry><entry><title>Solving the GrecianÂ Computer</title><link href="/posts/2022/Mar/solving-grecian-computer/" rel="alternate"></link><published>2022-03-18T00:00:00-07:00</published><updated>2022-03-18T00:00:00-07:00</updated><author><name>Alex Liebscher</name></author><id>tag:None,2022-03-18:/posts/2022/Mar/solving-grecian-computer/</id><summary type="html">&lt;p&gt;Solving Grandma&amp;#8217;s puzzle with a little algorithmic&amp;nbsp;help&lt;/p&gt;</summary><content type="html">&lt;p&gt;My family celebrated my grandma&amp;#8217;s 88th birthday a couple weekends ago. She, however, was prepared to give out gifts to everyone else. Knowing what my family enjoys doing, she mail ordered (from a catalogue, yes) a puzzle for us all to solve. It&amp;#8217;s called the Grecian Computer puzzle. Unfortunately, I can&amp;#8217;t find any reliable background on it. Barnes and Noble &lt;a href="https://www.barnesandnoble.com/w/true-genius-greek-computer-2-wooden-brainteaser-puzzle-project-genius/1137585230"&gt;claims&lt;/a&gt; it was inspired by an astronomical tool found in an old Grecian shipwreck. I think it&amp;#8217;s just a puzzle someone came up with. However skeptical I am of its origins, the puzzle is hard, very hard. My sister, dad, and I took turns trying to solve it and none of us made any progress. Not that we&amp;#8217;re numerical geniuses, but collectively we were&amp;nbsp;stumped.&lt;/p&gt;
&lt;p&gt;To help you picture what we were up against, here is what the puzzle looks&amp;nbsp;like:&lt;/p&gt;
&lt;p&gt;&lt;img data-src="/images/grecian-computer-start.jpg" class="uk-align-center" width="90%" height="" alt="Starting position of the Grecian Computer puzzle" uk-img&gt;&lt;/p&gt;
&lt;p&gt;Each of the concentric circles rotates, and the goal is to turn these dials until each of the 12 columns add up to&amp;nbsp;42.&lt;/p&gt;
&lt;p&gt;The dials aren&amp;#8217;t purely concentric &amp;#8212; some of the spaces are &amp;#8220;blank&amp;#8221;, revealing the number on the dial&amp;nbsp;below.&lt;/p&gt;
&lt;p&gt;After turning the dials for a while without success, I took out a piece of paper and started writing out numbers. It took me a moment before realizing how to write them out: each dial being a &amp;#8220;layer&amp;#8221; and represented as a matrix. I could leave a cell empty to represent a hole in a dial. Here&amp;#8217;s one of my&amp;nbsp;notes:&lt;/p&gt;
&lt;p&gt;&lt;img data-src="/images/grecian-computer-notes.jpg" class="uk-align-center" width="60%" height="" alt="A piece of scratch paper with a collection of numbers written out to describe my thoughts while trying to represent the puzzle numerically" uk-img&gt;&lt;/p&gt;
&lt;p&gt;This started to make sense in my head and I noticed a couple patterns, but nothing meaningful enough to help solve the puzzle. I thought, Well since these are really just matrices, could we write an algorithm to solve&amp;nbsp;this?&lt;/p&gt;
&lt;p&gt;Let me first get it out of the way that yes, I solved the puzzle. Yes, I did it using a computer. No, I have no idea how to solve the puzzle without the computer&amp;#8217;s brute force. Puzzles are for meant to be fun, and reveling in Algorithm Land is my version of&amp;nbsp;fun.&lt;/p&gt;
&lt;p&gt;That made clear, I&amp;#8217;m sure the reader is most interested in the computational solution. Without further delay, I first will describe the solution, then my progress and&amp;nbsp;learnings.&lt;/p&gt;
&lt;h2&gt;Solution&lt;/h2&gt;
&lt;p&gt;I represented our puzzle as a 3-dimensional tensor. A tensor is a generalization of a scalar, a vector, or a matrix. A scalar is a 0-dimensional tensor, a vector 1-dimensional, and a matrix 2-dimensional. The puzzle has 5 dials, with 4 rows of numbers, and 12 columns. Naturally then, this is a tensor with shape&amp;nbsp;5x4x12.&lt;/p&gt;
&lt;p&gt;The solution is given to us: all columns must sum to 42. Mathematically, we can represent this solution as a vector of length 12, each value equal to&amp;nbsp;42.&lt;/p&gt;
&lt;p&gt;We have two unanswered questions. First, how do we represent those &amp;#8220;blank&amp;#8221; or &amp;#8220;empty&amp;#8221; cells? Second, how do we &amp;#8220;rotate&amp;#8221; the&amp;nbsp;dials?&lt;/p&gt;
&lt;p&gt;My answer to the first is that the &amp;#8220;empty&amp;#8221; cells in the tensor are set to 0. A zero signals, through masking and multiplication, that the value beneath is&amp;nbsp;used.&lt;/p&gt;
&lt;p&gt;My answer to the second is to &amp;#8220;rotate&amp;#8221; or &amp;#8220;roll&amp;#8221; a specific axis of the tensor. The matrix gets pushed to the right and the column that falls off gets placed on the&amp;nbsp;left.&lt;/p&gt;
&lt;p&gt;I&amp;#8217;m intentionally grounding this explanation in metaphors of the physical world. Without the metaphors, we would quickly become lost in&amp;nbsp;terminology.&lt;/p&gt;
&lt;p&gt;Another hidden difficulty in this is summing the columns. What&amp;#8217;s trivial for the human eye to pick up as a &amp;#8220;column&amp;#8221; is less so in this computational framework. I thought of this as working down, or outward. First, we consider the innermost or top dial. Suppose on a dial, &lt;span class="math"&gt;\(A\)&lt;/span&gt;, with two numbers (but four spaces) there&amp;nbsp;are&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;[0, 5, 0, 1]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then if we sum over each column we get the result: &lt;code&gt;[0, 5, 0, 1]&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Suppose there&amp;#8217;s a dial underneath, call this &lt;span class="math"&gt;\(B\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;[
  [1,1,1,1],
  [2,2,2,2]
]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;How would we add this with the innermost dial? Overlay the first dial onto the second. The first column would be 1 + 2 = 3. Second, 1 + 5 = 6. Third, 1 + 2 = 3. Fourth, 1 + 1 = 2. Resulting in &lt;code&gt;[3, 6, 3, 2]&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;I represented the innermost dial &lt;span class="math"&gt;\(A\)&lt;/span&gt; then&amp;nbsp;as&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;[
  [0, 0, 0, 0],
  [0, 5, 0, 1]
]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;To quickly compute the result of &lt;span class="math"&gt;\(A\)&lt;/span&gt; overlaid on &lt;span class="math"&gt;\(B\)&lt;/span&gt;,&amp;nbsp;solve:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;A + (A == 0)*B
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;where &lt;code&gt;(A == 0)&lt;/code&gt; becomes a masking matrix of 1s (&lt;code&gt;True&lt;/code&gt;) and 0s (&lt;code&gt;False&lt;/code&gt;). Thus this only uses values of &lt;span class="math"&gt;\(B\)&lt;/span&gt; where &lt;span class="math"&gt;\(A\)&lt;/span&gt; is empty. Bringing it all together, this sums the values of &lt;span class="math"&gt;\(B\)&lt;/span&gt; where &lt;span class="math"&gt;\(A\)&lt;/span&gt; is empty, with all values of &lt;span class="math"&gt;\(A\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We extend this for all five puzzle dials until we have something that looks&amp;nbsp;like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;game&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;game&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;game&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;game&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;game&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;game&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;game&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;game&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;game&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;game&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;game&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;game&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;game&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;game&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;game&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Here &lt;code&gt;game&lt;/code&gt; is just a copy of the game board after rotating it. Consider &lt;code&gt;game[4]&lt;/code&gt; as &lt;span class="math"&gt;\(A\)&lt;/span&gt; and &lt;code&gt;game[3]&lt;/code&gt; as &lt;span class="math"&gt;\(B\)&lt;/span&gt;. Now extend the logic to the next dial, say &lt;span class="math"&gt;\(C\)&lt;/span&gt;, where the values of &lt;span class="math"&gt;\(C\)&lt;/span&gt; are only relevant where we can see them (i.e. where the values of both &lt;span class="math"&gt;\(A\)&lt;/span&gt; and &lt;span class="math"&gt;\(B\)&lt;/span&gt; are&amp;nbsp;0).&lt;/p&gt;
&lt;p&gt;Now what about rotating the dials? Numpy has an &lt;code&gt;np.roll&lt;/code&gt; method that performs exactly what we need: rolling an axis of a tensor so that the &amp;#8220;columns&amp;#8221; are shifted, with the final column being carried over to the&amp;nbsp;front.&lt;/p&gt;
&lt;p&gt;Lastly, we must represent the state of the dial somehow. I did this with a vector of length 5, with each element representing the rotation (up to 12) of one of the five dials. The starting point of the game is arbitrary, and doesn&amp;#8217;t matter since we can brute force all combinations&amp;nbsp;anyways.&lt;/p&gt;
&lt;p&gt;We have a function for summing the puzzle and checking the&amp;nbsp;score:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;equals&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c1"&gt;# first roll the board so the dials are at the right positions&lt;/span&gt;
    &lt;span class="n"&gt;sub_copy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;roll_board&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# now sum all the columns, for each dial taking into account how it&amp;#39;s masked&lt;/span&gt;
    &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;sub_copy&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
        &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sub_copy&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;sub_copy&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
        &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sub_copy&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sub_copy&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;sub_copy&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
        &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sub_copy&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sub_copy&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sub_copy&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;sub_copy&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
        &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sub_copy&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sub_copy&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sub_copy&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sub_copy&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;sub_copy&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# do all our columsn equal our solution?&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;all&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;solution&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We also have a function for rotating the board, according to a given&amp;nbsp;state:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;roll_board&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c1"&gt;# first, create a copy of the game (so we don&amp;#39;t modify the original)&lt;/span&gt;
    &lt;span class="n"&gt;sub_copy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;game&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# for each of the 5 dials&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;offset&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;selected&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;game&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
        &lt;span class="c1"&gt;# roll the dial (on axis 2) and keep the new position in the copy&lt;/span&gt;
        &lt;span class="n"&gt;selected&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;roll&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;selected&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;offset&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;sub_copy&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;selected&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;sub_copy&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I saw this as a recursive problem, instead of writing out many loops. This recursive function looks like&amp;nbsp;this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nd"&gt;@tail_recursive&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;recursive&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tally&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="c1"&gt;# exit condition&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;equals&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tally&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# create a copy of the puzzle state that may be masked&lt;/span&gt;
    &lt;span class="n"&gt;masked&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c1"&gt;# we want to know if any of the dials have been exhausted&lt;/span&gt;
    &lt;span class="n"&gt;top&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argwhere&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;masked&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# if none have, turn the outermost dial one place&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;top&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;masked&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="c1"&gt;# recurse with one change in the state&lt;/span&gt;
        &lt;span class="n"&gt;recurse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;masked&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tally&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# if one of dials has been exhausted, we increment the dial&lt;/span&gt;
    &lt;span class="c1"&gt;# just greater than it, and wipe clean the dials less than it&lt;/span&gt;
    &lt;span class="n"&gt;top&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;top&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;masked&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;top&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="n"&gt;masked&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;top&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;

    &lt;span class="n"&gt;recurse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;masked&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tally&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We can initiate the algorithm and&amp;nbsp;solve:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;recursive&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="code-output"&gt;
(array([0, 5, 9, 2, 7]), 10871)
&lt;/pre&gt;

&lt;p&gt;It took 10,871 dial turns to reach the solution: [0, 5, 9, 2,&amp;nbsp;7].&lt;/p&gt;
&lt;p&gt;Here is what the puzzle looks like at this state, with all columns adding to&amp;nbsp;42:&lt;/p&gt;
&lt;p&gt;&lt;img data-src="/images/grecian-computer-end.jpg" class="uk-align-center" width="90%" height="" alt="Completed position of the Grecian Computer puzzle, with all five columns adding to 42" uk-img&gt;&lt;/p&gt;
&lt;h2&gt;Progress and&amp;nbsp;Learnings&lt;/h2&gt;
&lt;p&gt;I&amp;#8217;m glad to be able to share this solution, and I did my best to explain it simply. There was a handful of attempts to get here though, and it wasn&amp;#8217;t a straight&amp;nbsp;shot.&lt;/p&gt;
&lt;p&gt;First, I didn&amp;#8217;t immediately realize that I&amp;#8217;d need to mask numbers. I was confused why the columns weren&amp;#8217;t adding up digitally like they were on the puzzle. Eventually I understood that numbers on outer dials were being &amp;#8220;hidden&amp;#8221; by inner dials. I found that I could mask the outer dials by converting the inner dial to &amp;#8220;mask&amp;#8221; or &amp;#8220;no&amp;nbsp;mask&amp;#8221;.&lt;/p&gt;
&lt;p&gt;I also wasn&amp;#8217;t aware of the &lt;code&gt;np.roll&lt;/code&gt; method going into this. I thought I was going to have to&amp;#8230; roll my own. Luckily, it took me one or two Google searches to find the pre-built roll method. It did however take me some time to understand how best to use it in this case. Through trial-and-error, I rolled different tensors and tried matching the output with what I&amp;nbsp;expected.&lt;/p&gt;
&lt;p&gt;It was about this point that a &lt;a href="https://en.wikipedia.org/wiki/Recursion"&gt;recursive strategy&lt;/a&gt; came to mind. I prototyped a looping method, but it seemed too &amp;#8220;hardcoded&amp;#8221;, messy, and &amp;#8220;stateful.&amp;#8221; I have not taken any algorithms courses though. My understanding of algorithms, and recursion, is very&amp;nbsp;limited.&lt;/p&gt;
&lt;p&gt;I also realized around this time that I wouldn&amp;#8217;t be able to continue on without using simplified toy data. The real data was too complex, and if I wanted to prototype with only one or two layers I wouldn&amp;#8217;t have a known solution. So, I created a toy dataset of size 3x3x3 (3 dials, with 3 rows, and 3&amp;nbsp;columns).&lt;/p&gt;
&lt;p&gt;I came up with a first recursive attempt, but, surprisingly, it wasn&amp;#8217;t recursive enough. I was really only iterating down a single dimension, not all dimensions. So, many of the possible combinations weren&amp;#8217;t getting tried out. I paused and thought about what I needed in a recursive function. I tried&amp;nbsp;again.&lt;/p&gt;
&lt;p&gt;There were a couple updates to adapt my previous recursive function to the real data. The toy data was necessary for getting the mechanics down, but I needed to see how things would flow in reality. This included putting quite a few print statements in the recursive&amp;nbsp;function.&lt;/p&gt;
&lt;p&gt;As I was testing, I hit a &lt;a href="https://stackoverflow.com/questions/2401447/python-recursive-function-error-maximum-recursion-depth-exceeded"&gt;RecursionError&lt;/a&gt; saying I reached the maximum recursion depth. I had to Google what this meant. I went back to the drawing board to start fresh with some of the implementations I saw&amp;nbsp;online.&lt;/p&gt;
&lt;p&gt;I created a new function and again hit a recursion error. This forced me to go learn about &lt;a href="https://stackoverflow.com/questions/33923/what-is-tail-recursion"&gt;(non-)tail recursion&lt;/a&gt; and how Python handles this type of computation. I discovered that Guido in fact &lt;a href="https://neopythonic.blogspot.com/2009/04/tail-recursion-elimination.html"&gt;does not believe&lt;/a&gt; that Python should be designed with recursion in&amp;nbsp;mind:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Third, I don&amp;#8217;t believe in recursion as the basis of all programming. This is a fundamental belief of certain computer scientists, especially those who love Scheme and like to teach programming by starting with a &amp;#8220;cons&amp;#8221; cell and recursion. But to me, seeing recursion as the basis of everything else is just a nice theoretical approach to fundamental mathematics (&lt;a href="http://en.wikipedia.org/wiki/Turtles_all_the_way_down"&gt;turtles all the way down&lt;/a&gt;), not a day-to-day&amp;nbsp;tool.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So trying to use recursion had come back to bite me. I read that Python only allowed at most about 1000 levels deep in a recursive function. After this, the memory stack has too much to keep track of, and opts to fail. Next time I think I will favor an iterative approach, at least when using&amp;nbsp;Python.&lt;/p&gt;
&lt;p&gt;I found a snippet of code on &lt;a href="https://chrispenner.ca/posts/python-tail-recursion"&gt;a kind person&amp;#8217;s blog&lt;/a&gt; which catches any tail recursion error, if there is one. If it catches an error, it restarts a new stack of memory after closing out the original function. This allows the recursive function to continue on even if it hits Python&amp;#8217;s recursive limit. In my case, this was a good enough solution to carry onward. You can see how this decorator is used in my code above. I copy here the&amp;nbsp;snippet:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Recurse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ne"&gt;Exception&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;kwargs&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;recurse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="n"&gt;Recurse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;tail_recursive&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;decorated&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;except&lt;/span&gt; &lt;span class="n"&gt;Recurse&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;args&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;
                &lt;span class="n"&gt;kwargs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;
                &lt;span class="k"&gt;continue&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;decorated&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Eventually, I hit run and the function carried out for about 5 to 10 seconds. As I mentioned, after 10,871 iterations, the function reached an output. I determined what the board would be at that result and tested it out on the game in my hand.&amp;nbsp;Success!&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="meta"></category><category term="python"></category><category term="algorithms"></category></entry><entry><title>Linear Regression: ParameterÂ Estimation</title><link href="/posts/2022/Feb/linear-regression-parameters/" rel="alternate"></link><published>2022-02-21T00:00:00-08:00</published><updated>2022-02-21T00:00:00-08:00</updated><author><name>Alex Liebscher</name></author><id>tag:None,2022-02-21:/posts/2022/Feb/linear-regression-parameters/</id><summary type="html">&lt;p&gt;Three methods of fitting a line to&amp;nbsp;data&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;img class="uk-align-center" width="90%" height="" src="/images/regr-header.png" uk-img&gt;&lt;/p&gt;
&lt;div class="caption"&gt;
Image by Alex Liebscher
&lt;/div&gt;

&lt;p&gt;Truly understanding the workings of linear regression isn&amp;#8217;t as straightforward as an introductory stats class makes it out to be. Because of its utility and storied history, linear regression can now be understood in too many ways for most scientists and data-folks to fully&amp;nbsp;grasp.&lt;/p&gt;
&lt;p&gt;This article makes a chip away at this complexity by explaining three routine methods for estimating linear regression parameters. We&amp;#8217;ll discuss Least Squares, Gradient Descent, and Bayesian&amp;nbsp;estimation.&lt;/p&gt;
&lt;p&gt;We&amp;#8217;ll walk through each with code examples notebook-style and some sparse math to help illustrate what&amp;#8217;s happening. Before we get started, we&amp;#8217;ve got a few packages to load up. If you don&amp;#8217;t have them installed, create a &lt;code&gt;conda&lt;/code&gt; environment and &lt;code&gt;pip install&lt;/code&gt; them.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;scipy.optimize&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;minimize&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pymc3&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pm&lt;/span&gt;

&lt;span class="n"&gt;rng&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;default_rng&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;&lt;a name="s1"&gt;&lt;/a&gt;Creating our&amp;nbsp;Data&lt;/h2&gt;
&lt;p&gt;We will first create a fake dataset, also known as a simulated dataset. We do this so later on we can compare our model results to the true data generation process. Imagine a phenomenom that, when nothing happens, with an input of 0, the output is 5. Now imagine that when the process is input with -1, the output is 3; and when input with 1, the output is 7. We can build this simulation by specifying a line like &lt;span class="math"&gt;\(y = 5 + 2x + \epsilon\)&lt;/span&gt;, where &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt; is some random error on each output (for example &lt;span class="math"&gt;\(\pm 1\)&lt;/span&gt;).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;beta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="n"&gt;sigma&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;

&lt;span class="n"&gt;N&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;

&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;rng&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;beta&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;rng&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We can then plot these predictor values, ordered by which the data were&amp;nbsp;generated,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Index&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;X value&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="code-output"&gt;
    &lt;img src="/images/regression-fig1.svg" uk-svg &gt;
&lt;/pre&gt;

&lt;p&gt;and also related with the outcome&amp;nbsp;values,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;X&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;y&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="code-output"&gt;
    &lt;img src="/images/regression-fig2.svg" uk-svg &gt;
&lt;/pre&gt;

&lt;p&gt;The human eye easily picks up a pattern here. There&amp;#8217;re data that, when the horizontal variable increases, corresponds to an increase in the vertical variable. The pattern looks linear. There also appears to be some noise in the data; one data point doesn&amp;#8217;t relate to the next in an exactly predictable&amp;nbsp;way.&lt;/p&gt;
&lt;p&gt;You&amp;#8217;re likely familiar with this scenario. As taught (either through instruction or experience), you&amp;#8217;d next feel a craving to model this pattern using a linear regression model. This would help you describe, in numerical terms, exactly the pattern you see. If you&amp;#8217;re jumping the gun, you&amp;#8217;d probably dart your eyes to those p-values too. This article won&amp;#8217;t discuss p-values, but it will discuss how our coefficients are&amp;nbsp;determined.&lt;/p&gt;
&lt;h2&gt;Estimating &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; and &lt;span class="math"&gt;\(\beta\)&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;Given these simulated data, what can we do to understand what the pattern we&amp;nbsp;see?&lt;/p&gt;
&lt;h3&gt;Least&amp;nbsp;Squares&lt;/h3&gt;
&lt;p&gt;The first and most transparent method for understanding the relationship we see is called Least Squares, or Ordinary Least&amp;nbsp;Squares.&lt;/p&gt;
&lt;p&gt;It&amp;#8217;s called least squares because our goal with this method is to find the line which &lt;span style="color: red;"&gt;minimizes&lt;/span&gt; the &lt;span style="color: purple;"&gt;sum&lt;/span&gt; of the &lt;span style="color: orange;"&gt;squares&lt;/span&gt; of the residuals&amp;#8212;the &lt;span style="color: blue;"&gt;true outcome&lt;/span&gt; minus the &lt;span style="color: green;"&gt;model prediction&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$
\color{red}{\text{argmin}}_{\alpha, \beta} \color{purple}{\sum}_{i=1}^N \color{orange}{(}\color{blue}{y_i} - (\color{green}{\alpha + \beta x_i})\color{orange}{)^2}
$$&lt;/div&gt;
&lt;p&gt;The parameter values, or &lt;strong&gt;regression coefficients&lt;/strong&gt;, are defined as the best solution to this minimization&amp;nbsp;problem.&lt;/p&gt;
&lt;h4&gt;Algabraic&amp;nbsp;Solution&lt;/h4&gt;
&lt;p&gt;Our first stop is the algabraic solution to regression. By taking &lt;a href="https://seismo.berkeley.edu/~kirchner/eps_120/Toolkits/Toolkit_10.pdf"&gt;some calculus&lt;/a&gt; for granted, we arrive at a solution that requires nothing more than&amp;nbsp;algebra.&lt;/p&gt;
&lt;p&gt;In code, we can use our &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; vectors to compute the values for &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; and &lt;span class="math"&gt;\(\beta\)&lt;/span&gt; quite&amp;nbsp;easily,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;x_mean&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;beta_hat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;x_mean&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;x_mean&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;alpha_hat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;beta_hat&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;x_mean&lt;/span&gt;

&lt;span class="n"&gt;alpha_hat&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;beta_hat&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="code-output"&gt;(4.441, 2.789)&lt;/pre&gt;

&lt;p&gt;To visually assess the fit of this model, let&amp;#8217;s plot the fitted regression line on the data as well as the true data generation&amp;nbsp;line,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;xp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;beta&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;xp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;k&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;linestyle&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;--&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Truth&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha_hat&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;beta_hat&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;xp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;r&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;linestyle&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;-&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Estimated&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;upper left&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;X&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;y&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="code-output"&gt;
    &lt;img src="/images/regression-fig3.svg" uk-svg &gt;
&lt;/pre&gt;

&lt;p&gt;And that&amp;#8217;s it, we&amp;#8217;ve calculated a fitted line to the data at hand. As you can see, we came quite close to the true data generation line. Unfortunately, this method really only works if you only have a single predictor, which in many cases is too much of a&amp;nbsp;constraint.&lt;/p&gt;
&lt;h4&gt;Measuring&amp;nbsp;Error&lt;/h4&gt;
&lt;p&gt;It&amp;#8217;s common to assess the fit of the model by aggregating the &lt;em&gt;residuals&lt;/em&gt;. The residuals are the difference between either the true in-sample outcomes or a set of out-of-sample outcomes, and the predicted outcomes for those observations. Since we&amp;#8217;re not working with any out-of-sample data (like a testing or hold-out set), let&amp;#8217;s just calculate the in-sample&amp;nbsp;residuals:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;y_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;alpha_hat&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;beta_hat&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;

&lt;span class="n"&gt;residuals&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With this &lt;code&gt;residuals&lt;/code&gt; vector, we can compute the Mean Squared Error (&lt;span class="math"&gt;\(\text{MSE} = \frac{1}{df}\sum_{i=0}^n(residuals_i^2)\)&lt;/span&gt;), Root Mean Squared Error (&lt;span class="math"&gt;\(\text{RMSE} = \sqrt{\text{MSE}}\)&lt;/span&gt;), or Mean Absolute Error. &lt;a href="http://zerospectrum.com/2019/06/02/mae-vs-mse-vs-rmse/"&gt;Each of these&lt;/a&gt; is a method for quantitatively assessing how well the model fit the data. If we had a test set of data, we could assess how well the model fits new data, i.e. its ability to&amp;nbsp;generalize.&lt;/p&gt;
&lt;p&gt;The error in the model &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt; is assumed to be normally distributed, and there is in fact a strong relationship between how the variance of a Gaussian sample can be computed and the formula for the &lt;span class="caps"&gt;MSE&lt;/span&gt;. Both are drawn from the idea that the squared difference between the true value (the true &lt;span class="math"&gt;\(x\)&lt;/span&gt; or true &lt;span class="math"&gt;\(y\)&lt;/span&gt;) and the mean or predicted value (&lt;span class="math"&gt;\(\mu\)&lt;/span&gt; or &lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt;) is a meaningful quantity of dispersion. The general form for either the sample variance or the &lt;span class="caps"&gt;MSE&lt;/span&gt;&amp;nbsp;is&lt;/p&gt;
&lt;div class="math"&gt;$$
\text{Var}(\theta) = \text{MSE}(\theta) = \mathbb{E}_\theta[(\theta - \hat{\theta})^2]
$$&lt;/div&gt;
&lt;p&gt;which can be defined then for either the sample&amp;nbsp;variance&lt;/p&gt;
&lt;div class="math"&gt;$$
\text{Var}(X) = \mathbb{E}_X[(X - \mu)^2] = \frac{1}{n}\sum_{i=0}^n (x_i - \mu)^2
$$&lt;/div&gt;
&lt;p&gt;or the &lt;span class="caps"&gt;MSE&lt;/span&gt;, where &lt;span class="math"&gt;\(p\)&lt;/span&gt; is the number of model parameters (and the 1 indicates a degree of freedom for the&amp;nbsp;intercept)&lt;/p&gt;
&lt;div class="math"&gt;$$
\text{MSE}(y) = \mathbb{E}_y[(y - \hat{y})^2] = \frac{1}{n-p-1}\sum_{i=0}{n} (y_i - \hat{y_i})^2
$$&lt;/div&gt;
&lt;p&gt;The ability of the model to minimize the squared error is also closely related to how well the model captures variance in the data. This is known as the &lt;a href="https://en.wikipedia.org/wiki/Coefficient_of_determination"&gt;coefficient of determination&lt;/a&gt; &lt;span class="math"&gt;\(r^2\)&lt;/span&gt;, and can be computed&amp;nbsp;like&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;y_pred&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="code-output"&gt;
0.3728
&lt;/pre&gt;

&lt;p&gt;This says that, on a scale from 0 to 1, the model captures about 37% of the variance in the response data. Normally we&amp;#8217;d need to place this in the context of other studies or research to decide if it&amp;#8217;s good or bad, but here we know the true data generation process. It&amp;#8217;s lower than I would have expected, but still it&amp;#8217;s good to see that the model captures some degree of&amp;nbsp;variance.&lt;/p&gt;
&lt;h4&gt;Linear Algebraic&amp;nbsp;Solution&lt;/h4&gt;
&lt;p&gt;The algabraic method is good pedalogically, but suffers at estimation when we want more than a single predictor and two parameters. So we&amp;#8217;ll graduate to a new method, which is actually equivalent to the algabraic method. We&amp;#8217;ll start by putting our intercept data (just 1&amp;#8217;s &lt;sup class="uk-link" uk-tooltip="Why 1's? To estimate a constant intercept, the data shouldn't change the estimate when multiplied. The one number which satisfies that identity mapping is 1."&gt;â³ï¸&lt;/sup&gt;) and &lt;span class="math"&gt;\(X\)&lt;/span&gt; data into a matrix &lt;span class="math"&gt;\(M\)&lt;/span&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;M&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;
&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="code-output"&gt;
[[1.        , 0.77395605],
 [1.        , 0.43887844],
 [1.        , 0.85859792],
 [1.        , 0.69736803],
 [1.        , 0.09417735]]
&lt;/pre&gt;

&lt;p&gt;From this we multiply the transpose of this matrix &lt;span class="math"&gt;\(M\)&lt;/span&gt; by &lt;span class="math"&gt;\(M\)&lt;/span&gt; itself. If &lt;span class="math"&gt;\(M\)&lt;/span&gt; is originally &lt;span class="math"&gt;\(N\)&lt;/span&gt; by &lt;span class="math"&gt;\(2\)&lt;/span&gt;, then this creates a &lt;span class="math"&gt;\(2\)&lt;/span&gt; by &lt;span class="math"&gt;\(2\)&lt;/span&gt; matrix. We then invert this matrix. There are tricks to doing this for models with many parameters, such as the &lt;span class="caps"&gt;LU&lt;/span&gt; decomposition or Cholesky decomposition, which I won&amp;#8217;t go into detail about here. With the inverse, we multiply this by &lt;span class="math"&gt;\(M\)&lt;/span&gt; transpose to get an &lt;span class="math"&gt;\(2 \times N\)&lt;/span&gt;. Lastly, this is multiplied by &lt;span class="math"&gt;\(\bf{y}\)&lt;/span&gt; to get our parameter solution, a &lt;span class="math"&gt;\(2 \times 1\)&lt;/span&gt;&amp;nbsp;vector.&lt;/p&gt;
&lt;div class="math"&gt;$$
\require{boldsymbol}
\boldsymbol{\hat{\beta}} = (M^TM)^{-1}M^T\boldsymbol{y}
$$&lt;/div&gt;
&lt;p&gt;in one line of Python this looks&amp;nbsp;like&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This is an excellent progression from the algabraic solution we just had because this allows us to find the parameter values for an arbitrarily large model, constrained only by our ability to take the inverse of a potentially large matrix. Luckily, as I mentioned, modern methods have tricks to increase the power of this&amp;nbsp;method.&lt;/p&gt;
&lt;p&gt;This matrix multiplication method is what R is doing when you call &lt;code&gt;lm()&lt;/code&gt; with a formula and a data frame. Granted, under the hood of this function there&amp;#8217;re layers of optimization and numerical tricks, but it&amp;#8217;s conceptually the same. For an excellent dive into the machinery behind &lt;code&gt;lm&lt;/code&gt; I highly recommend &lt;a href="https://madrury.github.io/jekyll/update/statistics/2016/07/20/lm-in-R.html"&gt;this blog article&lt;/a&gt; on the&amp;nbsp;matter.&lt;/p&gt;
&lt;h3&gt;Gradient&amp;nbsp;Descent&lt;/h3&gt;
&lt;p&gt;Least squares is sufficient for your typical linear regression model, and has the great benefit of having an analytical solution. But other regression models require other forms of finding the best parameter&amp;nbsp;values.&lt;/p&gt;
&lt;p&gt;One such alternative form is Gradient&amp;nbsp;Descent.&lt;/p&gt;
&lt;p&gt;First, a function to calculate the mean squared&amp;nbsp;error.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;mean_squared_error&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Beta&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;residuals&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Beta&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;residuals&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We need to set uniform random initial values for our parameters &lt;sup class="uk-link" uk-tooltip="True, you could start with any values between -Inf and Inf, but most models don't have parameter estimates that large (if they do, you might consider transforming/scaling your data."&gt;â³ï¸&lt;/sup&gt;, &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; and &lt;span class="math"&gt;\(\beta\)&lt;/span&gt;, or in this case &lt;span class="math"&gt;\(\hat{\beta}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;beta_hat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;rng&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,))&lt;/span&gt;
&lt;span class="n"&gt;beta_hat&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="code-output"&gt;
[0.437, 0.832]
&lt;/pre&gt;

&lt;p&gt;This means that without fitting the model to our data, we believe the intercept is 0.437 and the slope is 0.832. Compared to the true values (5 and 2, respectively), this is a very bad model. But that&amp;#8217;s expected; we haven&amp;#8217;t fit the model. How do we use the data to find parameter values that generalize better to the&amp;nbsp;data?&lt;/p&gt;
&lt;p&gt;The basic idea is to measure how bad the current parameter values are, and over many iterations, slowly adjust these parameter values in a better direction. Commonly, this is depicted like a person walking down a mountain in the dark with just a flashlight. The person can&amp;#8217;t see exactly where the bottom of the mountain is, but with their flashlight, they can look around them and move in a direction that takes them down. Sometimes, if the mountain is rocky, they&amp;#8217;ll hit a plateau or even start going uphill again. This is called a &lt;em&gt;local minimum&lt;/em&gt;. They really should get to the &lt;em&gt;global minimum&lt;/em&gt;. How can we prevent hitting local minima and get to the global minima? There&amp;#8217;s no perfect answer, but two answers do come up first: step size (how long are the hiker&amp;#8217;s legs?) and number of iterations (how many steps does the hiker take before they stop walking and throw their hands&amp;nbsp;up?).&lt;/p&gt;
&lt;p&gt;The step size, also called the &lt;em&gt;learning rate&lt;/em&gt;, is usually very small. The smaller it is, the more likely it is the hiker won&amp;#8217;t miss the right trail down; the larger it is, the faster they could reach the bottom, if they don&amp;#8217;t hit a local minima on their way. In our case, we&amp;#8217;ll set our learning rate to 0.01. This was chosen by running the cell a few times with values from 0.1 to 0.00001 and seeing how much progress we made down the mountain (i.e. did we barely move from the randomly set parameter values, or did they overshoot the true&amp;nbsp;model?).&lt;/p&gt;
&lt;p&gt;The number of iterations is somewhat inversely related to our step size. If you don&amp;#8217;t take very big steps, you&amp;#8217;ll need more steps to get to the bottom of the mountain. If you take big steps, you&amp;#8217;ll need fewer. We chose 1000 steps here, found by trial and&amp;nbsp;error.&lt;/p&gt;
&lt;p&gt;How do we decide which direction to step in? By computing the &lt;em&gt;gradient&lt;/em&gt; of the least squares error function. Computing the gradient means taking the partial derivate of a function &lt;span class="math"&gt;\(\mathcal{f}\)&lt;/span&gt; at values &lt;span class="math"&gt;\(p\)&lt;/span&gt;, written as &lt;span class="math"&gt;\(\nabla \mathcal{f}(p)\)&lt;/span&gt;. In our case of least squares errors, &lt;span class="math"&gt;\(\mathcal{f}(p) = \mathcal{f}(\beta) = \sum_{i=0}^N E_i(\beta)\)&lt;/span&gt; where &lt;span class="math"&gt;\(E_i(\beta)\)&lt;/span&gt; is the error of our model at the &lt;span class="math"&gt;\(i\)&lt;/span&gt; observation given the parameters &lt;span class="math"&gt;\(\beta\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In notation, we have the least squares error function,&amp;nbsp;and &lt;/p&gt;
&lt;div class="math"&gt;$$
\mathcal{f}(\beta) = \sum_{i=0}^N (y_i - \beta \, X_i)^2
$$&lt;/div&gt;
&lt;p&gt;The gradient with respect to &lt;span class="math"&gt;\(\beta\)&lt;/span&gt; (our only parameter)&amp;nbsp;is&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
\nabla_\beta \; \mathcal{f}(\beta) &amp;amp;= \frac{\partial}{\partial \beta} \; \mathcal{f} \\
&amp;amp;= \sum_{i=0}^N 2(y_i - \beta \, X_i)(-X_i) \\
&amp;amp;= -2 \sum_{i=0}^N (y_i - \hat{y_i})(X_i) \\
&amp;amp;= -2 \sum_{i=0}^N r_i X_i \\
&amp;amp;= -2 \, X^T \, \boldsymbol{r}
\end{align}
$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(r_i\)&lt;/span&gt; is the residual value for the &lt;span class="math"&gt;\(i\)&lt;/span&gt;th observation. We can then scale this down according to our step size and subtract it from our current parameter values to move in the &amp;#8220;downhill&amp;#8221;&amp;nbsp;direction.&lt;/p&gt;
&lt;p&gt;We would write this&amp;nbsp;like&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;beta_hat_copy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;beta_hat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;copy&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;learning_rate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.01&lt;/span&gt;

&lt;span class="n"&gt;history&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;gradient&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[],&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;beta_hat&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[],&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;mse&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[]}&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="n"&gt;residuals&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_mtx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;beta_hat_copy&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;gradient&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_mtx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;residuals&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;beta_hat_copy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;beta_hat_copy&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;gradient&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;history&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;gradient&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gradient&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;history&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;beta_hat&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;beta_hat_copy&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;history&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;mse&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mean_squared_error&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;beta_hat_copy&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;history&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;beta_hat&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="code-output"&gt;
[4.898, 2.244]
&lt;/pre&gt;

&lt;p&gt;And then we can plot the altitude of our hiker as they traversed down the&amp;nbsp;mountain&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;MSE:&lt;/span&gt;&lt;span class="se"&gt;\t&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;round&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;history&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;mse&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Iteration&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Mean Squared Error&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;history&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;mse&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="code-output"&gt;
MSE:     0.163

&lt;img src="/images/regression-fig5.svg" uk-svg &gt;
&lt;/pre&gt;

&lt;p&gt;As you can see, gradient descent looks like it found the bottom of the mountain! The final parameter values it determined were &lt;span class="math"&gt;\(\alpha = 4.898\)&lt;/span&gt; and &lt;span class="math"&gt;\(\beta = 2.244\)&lt;/span&gt;, which is close to the true values of 5 and&amp;nbsp;2.&lt;/p&gt;
&lt;p&gt;How does the fitted model look compared to our sample and true data generation&amp;nbsp;line?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;xp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;beta&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;xp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;k&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;linestyle&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;--&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Truth&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;history&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;beta_hat&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]),&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;r&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;linestyle&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;-&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Estimated&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;upper left&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;X&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;y&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="code-output"&gt;
    &lt;img src="/images/regression-fig6.svg" uk-svg &gt;
&lt;/pre&gt;

&lt;p&gt;Not too bad, given the low number of&amp;nbsp;observations.&lt;/p&gt;
&lt;p&gt;Although this is intuitive, researchers have developed far more efficient and less problematic methods for large datasets and models with many parameters. Gradient descent can &lt;a href="https://stats.stackexchange.com/questions/278755/why-use-gradient-descent-for-linear-regression-when-a-closed-form-math-solution"&gt;out-perform least squares&lt;/a&gt; in a number of situations, which is part of the reason why I included it here. Bonus concept: this is essentially the backdown of most modern &lt;span class="caps"&gt;AI&lt;/span&gt;, being a critical ingredient of neural networks and forming &lt;a href="https://brilliant.org/wiki/backpropagation/"&gt;the workhorse of backpropogation&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Bayes&lt;/h3&gt;
&lt;p&gt;The third and last method of parameter estimation I&amp;#8217;ll talk about is the Bayesian&amp;nbsp;form.&lt;/p&gt;
&lt;p&gt;Unlike the past two models, Bayesian estimation puts a lot more emphasis on building the right model. This includes the distributions of parameters and the way they relate, which in frequentist linear regression seems typically taught as assumed or&amp;nbsp;given.&lt;/p&gt;
&lt;p&gt;A Bayesian linear model has two to three key components: the outcome distribution, the linear model, and if necessary, the variance parameter. In some linear models, the last two are combined. For example, in a Poisson regression there&amp;#8217;s only a single parameter, &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;, whereas in a Gaussian regression there&amp;#8217;re both &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; and &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The first component though is the outcome distribution. If we&amp;#8217;re modeling a phenomenon that is real-valued and continuous, and tends to not be too skewed or dispersed, then a Gaussian outcome distribution might be a good first attempt. If we&amp;#8217;re modeling a phenomenon that consists of positive, integer count data, then a Poisson distribution might be most appropriate. There are a plethora of options here, but we&amp;#8217;ll stick with the Gaussian&amp;nbsp;example.&lt;/p&gt;
&lt;p&gt;The Gaussian distribution is parameterized by &lt;span class="math"&gt;\(\mathcal{N}(\mu, \sigma)\)&lt;/span&gt;. Instead of finding the one value that maximizes some function for both of these parameters, we&amp;#8217;ll consider each of these parameters as functions of other values &lt;sup class="uk-link" uk-tooltip="I recognize this is confusing, put another way, these mu and sigma values are not computed directly, and they're not even values. They're distributions, and we compute them through other, more descriptive parameters."&gt;â³ï¸&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;It&amp;#8217;s called a linear model because in this case, we&amp;#8217;ll define &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; as the &lt;strong&gt;function of a linear combination of other parameters&lt;/strong&gt;. For example, we might say &lt;span class="math"&gt;\(\mu = 5\)&lt;/span&gt;. This is a linear combination of exactly one value and a terrible model because it always will predict something around the value 5. We could add a parameter though, and fit the parameter to the data, like &lt;span class="math"&gt;\(\mu = \alpha\)&lt;/span&gt;. Then with &lt;em&gt;only&lt;/em&gt; our outcome data we&amp;#8217;d figure out what &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; could be. But if we want to use other data to predict our outcome, we need to include that, like &lt;span class="math"&gt;\(\mu = \alpha + \beta \, X\)&lt;/span&gt;. Now we have two parameters to fit, and our new one will be fit so as to reflect the relationship between our outcome and &lt;span class="math"&gt;\(X\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;As you can see though, &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; is just a linear combination. In other models, like the Poisson or a binomial/logistic, we&amp;#8217;d need to transform &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; so that it lines up with what the outcome distribution expects for parameters. For example, in the binomial model we&amp;#8217;d need &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; to be a probability, which wouldn&amp;#8217;t line up if we just let it vary as high or low as &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;, &lt;span class="math"&gt;\(\beta\)&lt;/span&gt;, and &lt;span class="math"&gt;\(X\)&lt;/span&gt; want it to go. Therefore we&amp;#8217;d need a &lt;a href="https://en.wikipedia.org/wiki/Logit"&gt;logit function&lt;/a&gt; to constrain those values back to the model parameter&amp;nbsp;space.&lt;/p&gt;
&lt;p&gt;But what are &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; and &lt;span class="math"&gt;\(\beta\)&lt;/span&gt;? They are parameters we must estimate. How? By letting them reflect certain distributions. Which distributions? Well, we must consider what values these parameters can take on. What&amp;#8217;s reasonable given our problem? Let&amp;#8217;s say we know from theory that our data very rarely exceed -50 or 50. That means if there&amp;#8217;s no relationship between our outcome and &lt;span class="math"&gt;\(X\)&lt;/span&gt;, we might expect &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; to be approximated by &lt;span class="math"&gt;\(\mathcal{N}(0, 20)\)&lt;/span&gt;. Why 20? In a normal distribution with mean 0, there&amp;#8217;s about 99% of the density between -50 and 50 with a standard deviation of 20. What about &lt;span class="math"&gt;\(\beta\)&lt;/span&gt; though? Suppose we don&amp;#8217;t expect the rate of change between &lt;span class="math"&gt;\(X\)&lt;/span&gt; and &lt;span class="math"&gt;\(y\)&lt;/span&gt; to be more than 5 units. So if &lt;span class="math"&gt;\(X\)&lt;/span&gt; increases by 1 unit, we shouldn&amp;#8217;t expect &lt;span class="math"&gt;\(y\)&lt;/span&gt; to increase by more than 5 or decrease by more than -5. Therefore, &lt;span class="math"&gt;\(\beta\)&lt;/span&gt; may be approximated by &lt;span class="math"&gt;\(\mathcal{N}(0, 2)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The other component we know of is &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;. We don&amp;#8217;t really think &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt; will vary with any data, so we can represent &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt; as a value or a space of values (a distribution). For example, in &lt;span class="math"&gt;\(\mathcal{N}(\mu, \sigma)\)&lt;/span&gt; we could say &lt;span class="math"&gt;\(\sigma=2\)&lt;/span&gt; if we knew that the variance of the Gaussian was 2, or we could say &lt;span class="math"&gt;\(\sigma \sim \text{Exp}(1)\)&lt;/span&gt;, as in &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt; reflects values from an exponential distribution with rate&amp;nbsp;1.&lt;/p&gt;
&lt;p&gt;Bringing it all together, we can write out our linear model like&amp;nbsp;so
&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
y &amp;amp; \sim \mathcal{N}(\mu, \sigma)\\
\mu &amp;amp; = \alpha + \beta X\\
\alpha &amp;amp; \sim \mathcal{N}(0, 20)\\
\beta &amp;amp; \sim \mathcal{N}(0, 2)\\
\sigma &amp;amp; \sim \text{Exp}(1)
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;At the end of the day though, enough data will allow this model to fit very well and the distributions we used for &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;, &lt;span class="math"&gt;\(\beta\)&lt;/span&gt;, and &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt; will get washed out by the data. In other words, their only function will be to constrain the shape of the parameter, not necessary its&amp;nbsp;values.&lt;/p&gt;
&lt;p&gt;What does this model look like in&amp;nbsp;code?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;alpha&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;beta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;beta&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;sigma&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;HalfNormal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;sigma&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;mu&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="nd"&gt;@beta&lt;/span&gt;
    &lt;span class="n"&gt;y_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;y_pred&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;observed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;posterior&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;chains&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;return_inferencedata&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Running this cell fits the model and its parameters to the data. The model works by guessing random numbers in a related sequence for the parameters. As it guesses these numbers, it starts to figure out what the space of the parameter distribution looks like, or what the most likely numbers are in the distribution if you were to pull from it at&amp;nbsp;random.&lt;/p&gt;
&lt;p&gt;If we pull out the parameters, you&amp;#8217;ll find that they&amp;#8217;re each actually a vector. This is that set of numbers the model guessed and identified as best representative of the distribution. It contains the sampling noise that comes with observing a natural phenomenon. Everything is a little noisy after&amp;nbsp;all.&lt;/p&gt;
&lt;p&gt;If we want to get just one number for our parameters, we can take the mean of the parameter&amp;nbsp;vectors:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;posterior&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;posterior&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;posterior&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;posterior&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;beta&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;posterior&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;posterior&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="code-output"&gt;
[5.207, 1.734, 0.431]
&lt;/pre&gt;

&lt;p&gt;In other words, the model believes &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; is centered around 5.2, &lt;span class="math"&gt;\(\beta\)&lt;/span&gt; around 1.7, and our variance around 0.4. Compared to the true values of 5, 2, and 0.5, this is not bad given only 20&amp;nbsp;observations.&lt;/p&gt;
&lt;p&gt;Another way to see what the model found is by plotting the distributions of the parameters. Here they are with the &lt;em&gt;mode&lt;/em&gt; of the distribution overlaid in black. I picked the mode because this is then the &lt;a href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation"&gt;Maximum a Posteriori estimate&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;layout&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;tight&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sharey&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Estimated Parameter Values&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;posterior&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;posterior&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bins&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;lightblue&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vlines&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;round&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;110&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;black&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Posterior alpha values&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Frequency&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;posterior&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;posterior&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;beta&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bins&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;lightblue&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vlines&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;round&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;110&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;black&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Posterior beta values&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Frequency&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;posterior&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;posterior&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bins&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;lightblue&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vlines&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;round&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;110&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;black&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Posterior sigma values&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Frequency&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="code-output"&gt;
    &lt;img src="/images/regression-fig7.svg" uk-svg &gt;
&lt;/pre&gt;

&lt;p&gt;This may be confusing because compared to our previous two methods of parameter estimation where we had a single estimate for each parameter, now we see a histogram for each. Like I said, this is because a Bayesian model determines the values of parameters through random sampling. There was no random sampling in Least Squares and Gradient Descent. Because of the random sampling, we never know the exact values. This is unhelpful conceptually, but practically it provides us with concrete and sane estimates of &lt;em&gt;uncertainty&lt;/em&gt;. Now we can compute the single parameter values using the mean, median, or mode; and we can also easily compute how accurate those measures&amp;nbsp;are.&lt;/p&gt;
&lt;p&gt;Don&amp;#8217;t let the histograms make you believe anything conceptually different is happening: we&amp;#8217;re still figuring out what the parameters should be, now we just have more information about&amp;nbsp;each.&lt;/p&gt;
&lt;h4&gt;Measuring&amp;nbsp;Error&lt;/h4&gt;
&lt;p&gt;We&amp;#8217;ve estimated the parameters using Bayes and random sampling, and just like the previous two sections, we&amp;#8217;d like to measure the error of the model and see how closely it matched the true data&amp;nbsp;generator.&lt;/p&gt;
&lt;p&gt;We&amp;#8217;re going to do this a little different to get more utility out of the Bayesian&amp;nbsp;framework.&lt;/p&gt;
&lt;p&gt;For each point, we can calculate the &lt;em&gt;posterior predictive outcome&lt;/em&gt;. An outcome like this is what the model believes the outcome could be knowing the (un)certainty of the outcomes to begin&amp;nbsp;with.&lt;/p&gt;
&lt;p&gt;The posterior predictive accounts for all the uncertainty in &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;, &lt;span class="math"&gt;\(\beta\)&lt;/span&gt;, and &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt; to produce a distribution &lt;em&gt;for each observation&lt;/em&gt;. The uncertainty in the distributions get propogated through the linear model and into the outcome distribution. Through this propogation of uncertainty, each of our 20 observations can be given a distribution. This distribution says, &amp;#8220;Given a predictor value for the model, here are roughly the most likely outcomes values of that&amp;nbsp;predictor.&amp;#8221;&lt;/p&gt;
&lt;p&gt;In the following code chunk, we sample and compute the posterior predictive values for each observed predictor. This produces a matrix of 20 observations and many possible outcomes for each observation. We take the 95% interval of each of those points (plus the median), and plot the median point and credible intervals around each&amp;nbsp;point.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;post_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample_posterior_predictive&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;posterior&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;posterior&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nb"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nb"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)],&lt;/span&gt; &lt;span class="n"&gt;linestyle&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;--&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;zorder&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;quantile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;post_pred&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;y_pred&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;zorder&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;#2777b4&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;y_i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;point&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;post_pred&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;y_pred&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;q025&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;quantile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;point&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.025&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;q975&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;quantile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;point&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.975&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;q050&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;quantile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;point&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vlines&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;q025&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;q975&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;black&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;linewidth&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;zorder&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;True y value&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Posterior predicted value&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;suptitle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Posterior predicted values&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;with 95&lt;/span&gt;&lt;span class="si"&gt;% c&lt;/span&gt;&lt;span class="s2"&gt;redible intervals&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="code-output"&gt;
    &lt;img src="/images/regression-fig8.svg" uk-svg &gt;
&lt;/pre&gt;

&lt;p&gt;This is arguably the most interesting plot of the entire article! Here we see on the x-axis the true data generator value. On the y-axis is the posterior predictive outcome. Each point is what the model believes the outcome should be for that observation&amp;#8217;s predictors (which we don&amp;#8217;t see anything of in this plot). And then for each point there&amp;#8217;s also a representation of the uncertainty of that estimate. The estimate can be interpreted as saying, this observation&amp;#8217;s outcome is most likely to be this red point, but there&amp;#8217;s a 95% probability it falls somewhere in the line range.&amp;#8221; This is of course dependent on the model (so really, there&amp;#8217;s a 95% probability &lt;em&gt;given this model&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;A perfectly accurate model would have all the blue points exactly on the dashed line. A perfectly precise model would have very small error bars. A perfectly accurate and precise model would have the blue poitns right on the line and very small error&amp;nbsp;bars.&lt;/p&gt;
&lt;p&gt;We see all the error bars include the line at some point, which signals this model seems to be doing pretty&amp;nbsp;good!&lt;/p&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;We&amp;#8217;ve gone over three (really four) methods for estimating the parameters of a linear regression model. We had the least squares method (both formulated through algabra and matrices), gradient descent, and a Bayesian method. It&amp;#8217;s also possible to think of estimating linear regression parameters using &lt;a href="https://kaomorphism.com/socraticregression/ols.html"&gt;geometry&lt;/a&gt; and &lt;a href="https://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/readings/L02%20Linear%20Regression.pdf"&gt;calculus&lt;/a&gt;. Not too mention the many methods of regularization, like &lt;a href="https://en.wikipedia.org/wiki/Linear_regression#Maximum-likelihood_estimation_and_related_techniques"&gt;&lt;span class="caps"&gt;LASSO&lt;/span&gt; and ridge regression&lt;/a&gt;, and &lt;a href="https://en.wikipedia.org/wiki/Linear_regression#Other_estimation_techniques"&gt;robust estimation&lt;/a&gt;. This is all to say that linear regression is a topic with great complexity, making it a daunting, mysterious, and fruitful concept for data-folk to&amp;nbsp;study.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="meta"></category><category term="statistics"></category><category term="python"></category></entry><entry><title>2022 SiteÂ Redesign</title><link href="/posts/2022/Feb/site-redesign/" rel="alternate"></link><published>2022-02-20T00:00:00-08:00</published><updated>2022-02-20T00:00:00-08:00</updated><author><name>Alex Liebscher</name></author><id>tag:None,2022-02-20:/posts/2022/Feb/site-redesign/</id><summary type="html">&lt;p&gt;Redesigning and redeveloping my personal&amp;nbsp;site&lt;/p&gt;</summary><content type="html">&lt;p&gt;I&amp;#8217;ve set out on redesigning my personal site. Why? I find Jekyll fairly picky, unstable, and hard to work with. Moreover, in using the Jekyll theme that I was (&lt;a href="https://github.com/alshedivat/al-folio"&gt;al-folio&lt;/a&gt;), I found that to also be unstable, hard to keep up to date, and kind of clunky. I appreciate all the people that went in to building both of these tools, but neither has really served me as well as I had once hoped they&amp;nbsp;would.&lt;/p&gt;
&lt;p&gt;It&amp;#8217;s time to not only rework the site generation process, but to update the look of my site to better reflect how I feel and what I need to make the best content I&amp;nbsp;can.&lt;/p&gt;
&lt;p&gt;I was actually inspired to do this by the typographer &lt;a href="MiklÃ³s TÃ³tfalusi Kis"&gt;MiklÃ³s TÃ³tfalusi Kis&lt;/a&gt;. His design of the Janson typeface is beautiful and inspiring, and it forced me to reflect deeply on it. I discovered it in a book I recently started reading, &lt;a href="https://en.wikipedia.org/wiki/A_Garden_of_Earthly_Delights"&gt;A Garden of Earthly Delights&lt;/a&gt; by Joyce Carol Oates, in which it is the typefont the book is set&amp;nbsp;in.&lt;/p&gt;
&lt;p&gt;So I&amp;#8217;ve embarked on redesigning my site in a way that resembles this spacing, consistency, sophistication, freedom, and&amp;nbsp;elegance.&lt;/p&gt;
&lt;hr /&gt;

&lt;p&gt;I started out with the simple theme from &lt;a href="https://blog.getpelican.com/"&gt;Pelican&lt;/a&gt;, a Python static site generator. I pruned back the Pelican dependencies in the source &amp;#8212; the custom classes, specific element IDs, and some &lt;span class="caps"&gt;HTML&lt;/span&gt; that I wasn&amp;#8217;t interested&amp;nbsp;in.&lt;/p&gt;
&lt;p&gt;Then I added &lt;a href="https://getuikit.com/"&gt;&lt;span class="caps"&gt;UI&lt;/span&gt; Kit&lt;/a&gt; to the base template. I chose &lt;span class="caps"&gt;UI&lt;/span&gt; Kit because I recently discovered it as an alternative to Bulma. I really like Bulma, but &lt;span class="caps"&gt;UI&lt;/span&gt; Kit so far has seemed less authortarian in the degree which a site can be built. It also seems to have an impressive range of components and standards for how pieces of a site should be built. In other words, it&amp;#8217;s minimalistic but featureful when you&amp;nbsp;want.&lt;/p&gt;
&lt;p&gt;Although &lt;span class="caps"&gt;UI&lt;/span&gt; Kit seemed promising, I quickly realized how much effort it was going to take to update all of my past articles to the new &lt;span class="caps"&gt;CSS&lt;/span&gt; framework. I try not to let the sunken cost fallacy hold me back, and in this case it seemed worth it to expend that&amp;nbsp;effort.&lt;/p&gt;
&lt;p&gt;I spent some time looking for an appropriate font, and I landed on Lora from Google Fonts. One day I might leap into full realization of Janson by buying that typefont, but for now, a free font allows me to prototype and iterate&amp;nbsp;quickly.&lt;/p&gt;
&lt;p&gt;I then took some time to figure out the code highlighting features of Pelican. While there is inline code, like &lt;code&gt;print("text")&lt;/code&gt;, I still haven&amp;#8217;t figured out how to get syntax highting for these inline snippets. However, there is beautiful looking code block highlighting. For example, some&amp;nbsp;Python:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Pelican is a static site generator.&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;and also&amp;nbsp;R:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;    &lt;span class="n"&gt;var&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;as.character&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1L&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I knew that I would want to write some articles in a Jupyter notebook style, with code blocks and associated outcomes. So I built a custom &lt;span class="caps"&gt;CSS&lt;/span&gt; solution which attaches an output block to any given code block, like&amp;nbsp;so,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;hello world&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="code-output"&gt;
hello world
&lt;/pre&gt;

&lt;p&gt;I snagged this Pygment Github theme from the Pelican theme &lt;a href="https://github.com/arulrajnet/attila"&gt;attila&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It&amp;#8217;s also important to me that there&amp;#8217;s &lt;span class="math"&gt;\(\LaTeX\)&lt;/span&gt; support, of which there is, both inline &lt;span class="math"&gt;\(y \sim N(0, 1)\)&lt;/span&gt; and&amp;nbsp;block:&lt;/p&gt;
&lt;div class="math"&gt;$$
y = \beta_0 + \beta_1 x
$$&lt;/div&gt;
&lt;p&gt;In order to render this Mathjax though I had to install the &lt;code&gt;pelican-render-math&lt;/code&gt; plugin.&lt;/p&gt;
&lt;p&gt;I realized that reStructredText files, which are common in the Pelican world, are unfamiliar and difficult to use. For example, the syntax between Markdown and &lt;span class="caps"&gt;RST&lt;/span&gt; for a link is wildly different. Everything feels unnatural and forced. But most of all, if I want to port my work over from my last site to this new one, it is easiest if I use Markdown since everything is already written with&amp;nbsp;Markdown.&lt;/p&gt;
&lt;p&gt;Next, I moved on to polishing the article page since that would be a crucial aspect of the site design. I tweaked the width of the page so it&amp;#8217;s about a 2/3 width, which seemed natural to my own eyes. I also justified the text because I think that looks really nice with longer content, almost like one sees in a book. I spent probably too much time figuring out the ideal syntax highlighting stylesheet and making slight modifications to it. For example, I decided that the Github theme was pretty, but its numerals were gray. In contrast, Jupyter Notebooks light them green, which I like better. Therefore I had to get into the stylesheet and update that one span to reflect my&amp;nbsp;tastes.&lt;/p&gt;
&lt;p&gt;I also added a modal for the subscribe form. This only took a few minutes with UIKit&amp;#8217;s built-in &lt;code&gt;uk-modal&lt;/code&gt; classes and Javascript add-on, which is a huge improvement compared to other solutions out there that&amp;#8217;d require custom &lt;span class="caps"&gt;JS&lt;/span&gt; for the click and close&amp;nbsp;events.&lt;/p&gt;
&lt;p&gt;I chose an article header image from Unsplash by &lt;a href="https://unsplash.com/@drew_beamer?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText"&gt;Drew Beamer&lt;/a&gt;. It took half a dozen tried but I found one with the right texture, energy, and framing that really makes the title&amp;nbsp;pop.&lt;/p&gt;
&lt;p&gt;Then I moved over to the tags. I cleaned up the design of the footer on the index page that lists out the tags for quick navigation. I also cleaned up the individual tag page, removing some of the things that the default page had put in. Overall, I&amp;#8217;m still not 100% satisfied. Some sort of abaility for the user to rank by number of articles or another heuristic would be ideal. For now though it will stay as a static alphabetical&amp;nbsp;list.&lt;/p&gt;
&lt;p&gt;I wanted to embed readers comments on the blog too because it seems like, for those who don&amp;#8217;t troll, it can be a useful way to quickly and easily give feedback and, well, comments. For some reason it seems like adding a comments section takes my site to a different realm, like adding this level of dynamic interface is breaking the flow or consistency. I found &lt;a href="https://shahayush.com/2020/05/web-pelican-pt5-disqus-analytics/"&gt;a quick guide&lt;/a&gt; which I gave a look, and then went to sign up for Disqus. Turns out, Disqus sucks. It&amp;#8217;s extremely unpleasant to look at. It&amp;#8217;s slow to load. It makes no commitment to privacy and overall seems nefarious. To top it all off, it&amp;#8217;s got ads galore. I was hopeful yet was disappointed by their complete lack of innovation, simplicity, and respect. I spent a little more time looking for an alternative service, but I&amp;#8217;m not ready to spend money on something I&amp;#8217;m not even sure will have success. In an ideal world, there&amp;#8217;d be a service which had a free tier for sites with less than 1,000 or 10,000 monthly views, and paid tiers for anything above. I would sign up for this knowing that I could use the service while it suited my needs and easily upgrade when I knew it&amp;#8217;d be a worthwhile invest. I think this would also be a good business model because at the point where a site owner wants to upgrade, they&amp;#8217;ve already invested their engineering and social capital into the platform and (if they are looking to upgrade) are very familiar with the effectiveness of the&amp;nbsp;service.&lt;/p&gt;
&lt;p&gt;After getting these elements to a comfortable spot, I&amp;#8217;m fairly content with how the site looks and feels right now. From my perspective, it&amp;#8217;s crisp, smooth, and elegant, just like&amp;nbsp;Janson.&lt;/p&gt;
&lt;p&gt;I&amp;#8217;ve made a couple other tweaks, but have now been drafting with the design for a few weeks and haven&amp;#8217;t gotten tired by the look yet. I take this to be a good sign. It&amp;#8217;s a little rough around the edges (e.g. things like the copyright page and my resources page), but it&amp;#8217;s in the right direction and an improvement over my previous site&amp;#8217;s&amp;nbsp;theme.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="meta"></category><category term="blogging"></category><category term="design"></category><category term="programming"></category></entry><entry><title>Deploying a containerized Heroku app with Appleâs M1Â processor</title><link href="/posts/2021/Nov/deploying-heroku-app/" rel="alternate"></link><published>2021-11-19T00:00:00-08:00</published><updated>2021-11-19T00:00:00-08:00</updated><author><name>Alex Liebscher</name></author><id>tag:None,2021-11-19:/posts/2021/Nov/deploying-heroku-app/</id><summary type="html">&lt;p&gt;The M1 is fast and furious, but bound to cause&amp;nbsp;headaches&lt;/p&gt;</summary><content type="html">&lt;style &gt;
.file-name {
  color: #bbb;
  font-size: 0.9em;
}
&lt;/style&gt;

&lt;p&gt;It&amp;#8217;s common to want to deploy an app beyond your local machine and onto the web. A plethora of services and platforms now make this easy, compared to what it would have taken 10 or 20 years&amp;nbsp;ago.&lt;/p&gt;
&lt;p&gt;In this article, I&amp;#8217;d like to outline one way to do so. Particularly, building a Python app, using Flask as a back-end server, bundling all the source files together with &lt;a href="https://www.heroku.com/"&gt;Docker&lt;/a&gt;, and deploying on Heroku. To add a twist, we&amp;#8217;re going to do this from a MacBook Pro with an Apple Silicon M1 processor, which demands special treatment in the eyes of&amp;nbsp;Heroku.&lt;/p&gt;
&lt;h2&gt;1. Build your&amp;nbsp;app&lt;/h2&gt;
&lt;p&gt;We&amp;#8217;re going to build a simple site that lands the user on a page, allows them fill out a form, and then shows them their&amp;nbsp;submission.&lt;/p&gt;
&lt;p&gt;To route traffic, handle requests, and serve static content, we&amp;#8217;ll be using &lt;a href="https://flask.palletsprojects.com/"&gt;Flask&lt;/a&gt;. A popular alternative is &lt;a href="https://www.djangoproject.com/"&gt;Django&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We&amp;#8217;ll start with our &lt;span class="caps"&gt;HTML&lt;/span&gt; landing&amp;nbsp;page.&lt;/p&gt;
&lt;div class="file-name"&gt;templates/index.html&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="cp"&gt;&amp;lt;!doctype html&amp;gt;&lt;/span&gt;

&lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;html&lt;/span&gt; &lt;span class="na"&gt;lang&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;en&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
&lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;head&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
  &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;meta&lt;/span&gt; &lt;span class="na"&gt;charset&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;utf-8&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
  &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;meta&lt;/span&gt; &lt;span class="na"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;viewport&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;content&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;width=device-width, initial-sclae=1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;

  &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;title&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;Example Heroku Deployment&lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;title&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
  &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;meta&lt;/span&gt; &lt;span class="na"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;description&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;content&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Deploying a containerized Heroku app with Apple&amp;#39;s M1 processor&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
&lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;head&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;

&lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;body&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
  &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;h3&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;Example Heroku Deployment with Apple&amp;#39;s M1 Processor&lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;h3&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
  &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;form&lt;/span&gt; &lt;span class="na"&gt;action&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;method&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;post&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
    &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;p&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
      &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;label&lt;/span&gt; &lt;span class="na"&gt;for&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;Name&lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;label&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
      &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;input&lt;/span&gt; &lt;span class="na"&gt;type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;text&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;name&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
    &lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;p&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
    &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;p&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
      &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;input&lt;/span&gt; &lt;span class="na"&gt;type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;submit&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;value&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Submit Form&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
    &lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;p&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
  &lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;form&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
  {% if name_data %}
  &lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nt"&gt;h4&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;Welcome, {{ name_data }}!&lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;h4&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
  {% endif %}
&lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;body&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
&lt;span class="p"&gt;&amp;lt;/&lt;/span&gt;&lt;span class="nt"&gt;html&lt;/span&gt;&lt;span class="p"&gt;&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This shouldn&amp;#8217;t look alien; it is a simple &lt;span class="caps"&gt;HTML&lt;/span&gt; page with a form that submits by button, through &lt;span class="caps"&gt;POST&lt;/span&gt;, to&amp;nbsp;itself. &lt;/p&gt;
&lt;p&gt;Then we&amp;#8217;ll build our Flask&amp;nbsp;server.&lt;/p&gt;
&lt;div class="file-name"&gt;server.py&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;flask&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Flask&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;render_template&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os&lt;/span&gt;

&lt;span class="n"&gt;app&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Flask&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="vm"&gt;__name__&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nd"&gt;@app&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;route&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;/&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;methods&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;GET&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;index_get&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;render_template&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;index.html&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nd"&gt;@app&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;route&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;/&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;methods&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;POST&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;index_post&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;render_template&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;index.html&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;form&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;name&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="n"&gt;port&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;environ&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;PORT&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5000&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
  &lt;span class="n"&gt;app&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;host&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;0.0.0.0&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;port&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;port&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;When the user lands on the page without submitting anything, we just show the form. When the server receives a &lt;span class="caps"&gt;POST&lt;/span&gt; request, we pull out the &lt;em&gt;name&lt;/em&gt; field from the request (assuming it&amp;#8217;s there, but default to &lt;code&gt;''&lt;/code&gt; if not) and display that&amp;nbsp;result.&lt;/p&gt;
&lt;p&gt;We also look for the environment variable, &lt;code&gt;PORT&lt;/code&gt;, which is really &lt;a href="https://blog.heroku.com/python_and_django"&gt;for Heroku&lt;/a&gt;. Heroku will choose and set the port which your app will use. We also set &lt;code&gt;host&lt;/code&gt; to 0.0.0.0 which overrides the default locahost parameter so that the site is &lt;a href="https://stackoverflow.com/q/30323224/3234482"&gt;accessible through Docker&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;2. Test&amp;nbsp;locally&lt;/h2&gt;
&lt;p&gt;To test our web app locally, we can just&amp;nbsp;run,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;python server.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This launches our Flask app as a local server on a localhost port, specifically port 5000. Visit the &lt;span class="caps"&gt;URL&lt;/span&gt; &lt;code&gt;127.0.0.1:5000&lt;/code&gt; to&amp;nbsp;test.&lt;/p&gt;
&lt;h2&gt;3. Containerize your&amp;nbsp;app&lt;/h2&gt;
&lt;p&gt;Now that we see our app working locally, we can &lt;a href="https://www.docker.com/blog/containerized-python-development-part-1/"&gt;containerize the app&lt;/a&gt;. By creating a container for our app, Heroku will be faster at deploying (since it won&amp;#8217;t have to rebuild the entire app every deployment), and it will ensure dependencies and architectures are&amp;nbsp;platform-agnostic.&lt;/p&gt;
&lt;p&gt;Or, almost platform-agnostic. I learned the hard way that Docker is particular in certain ways about the host build machine and its architecture. In particular, the architecture of an M1 Mac requires Docker to build apps differently than what Heroku wants to deploy&amp;nbsp;them.&lt;/p&gt;
&lt;p&gt;In any case, we need to start with a Dockerfile, so we&amp;#8217;ll do&amp;nbsp;that.&lt;/p&gt;
&lt;p&gt;First, we&amp;#8217;ll be starting from a Python 3.8 image as the base&amp;nbsp;layer.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;FROM&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;python:3.8&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We need to install our Python dependencies. This could also be done with a requirements file, but here we just write them&amp;nbsp;out.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;RUN&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;pip install flask
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We need to copy our &lt;span class="caps"&gt;HTML&lt;/span&gt; and Python source files into the&amp;nbsp;image.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;COPY&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;. .
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Flask expects to host the server through an open port, so we&amp;#8217;ll expose a port just for&amp;nbsp;Flask.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;EXPOSE&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;$PORT&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Lastly, the launch command when we run the image as a container is to launch the server. We&amp;#8217;ll bring it all together here&amp;nbsp;now,&lt;/p&gt;
&lt;div class="file-name"&gt;Dockerfile&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;FROM&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;python:3.8&lt;/span&gt;

&lt;span class="k"&gt;RUN&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;pip install flask

&lt;span class="k"&gt;COPY&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;. .

&lt;span class="k"&gt;EXPOSE&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;$PORT&lt;/span&gt;

&lt;span class="k"&gt;CMD&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;python&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;server.py&amp;quot;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;For more of this process, see the &lt;a href="https://devcenter.heroku.com/articles/container-registry-and-runtime"&gt;documentation&lt;/a&gt;. Now we need to build the image. Typically, we&amp;#8217;d&amp;nbsp;see,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;docker build . -t example-app
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;but because we&amp;#8217;re working on a different architecture, we actually&amp;nbsp;need,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;docker buildx build --platform linux/amd64 -t example-app .
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;code&gt;buildx&lt;/code&gt; allows the devloper to, among other things, build an image to run cross-platform. This is important to us since our source machine, an Apple M1 device, is a different architecture (&lt;code&gt;arm64&lt;/code&gt;) than the destination machine (&lt;code&gt;amd64&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Now that we have it setup with the right architecture, we can test the image locally by building a container. For&amp;nbsp;example,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;docker run --rm -e &lt;span class="nv"&gt;PORT&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;5000&lt;/span&gt; -p &lt;span class="m"&gt;5000&lt;/span&gt;:5000 example-app
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;To break down the arguments here: &lt;code&gt;--rm&lt;/code&gt; removes the container from the running container list once it exits, &lt;code&gt;-e PORT=5000&lt;/code&gt; sets our port environment variable, and &lt;code&gt;-p 5000:5000&lt;/code&gt; opens the port 5000 within the container to the host machine&amp;#8217;s port 5000. The last argument is the name of our image to&amp;nbsp;run.&lt;/p&gt;
&lt;p&gt;Now we can visit the exposed port (&lt;code&gt;127.0.0.1:5000/&lt;/code&gt;) and see our app live,&amp;nbsp;locally.&lt;/p&gt;
&lt;h2&gt;4. Push to&amp;nbsp;Heroku&lt;/h2&gt;
&lt;p&gt;First, we need to create a Heroku account. After that&amp;#8217;s been setup, create an app with any name you want, say &lt;code&gt;example-heroku-deployment&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We&amp;#8217;ll be deploying to the Heroku registry manually, but we still will use the Heroku &lt;span class="caps"&gt;CLI&lt;/span&gt; for some parts, so make sure that&amp;#8217;s &lt;a href="https://devcenter.heroku.com/articles/heroku-cli#download-and-install"&gt;installed&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We&amp;#8217;ll authenticate Heroku from the command line, make sure Docker is &lt;a href="https://docs.docker.com/get-docker/"&gt;installed&lt;/a&gt;, and login to the Heroku Container&amp;nbsp;Registry,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;heroku login
docker ps
heroku container:login
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now, we want to retag our image with the location of our Heroku app registry. For example, if we run &lt;code&gt;docker images&lt;/code&gt;, we can view the image &lt;span class="caps"&gt;ID&lt;/span&gt; of the image we just built for &lt;code&gt;example-app&lt;/code&gt;. In order to get our local image to the right place in the Heroku Registry, we need to label it&amp;nbsp;correctly,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;docker tag xxxxxxxxxxxx registry.heroku.com/example-heroku-deployment/web
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;where the image &lt;span class="caps"&gt;ID&lt;/span&gt; is copy and pasted from the &lt;code&gt;docker images&lt;/code&gt; command.&lt;/p&gt;
&lt;p&gt;Next we want to manually push the image to the Registry,&amp;nbsp;like,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;docker push registry.heroku.com/example-heroku-deployment/web
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now our app is pushed up to Heroku, and we just need to tell Heroku to take the image&amp;nbsp;live!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;heroku container:release web -a example-heroku-deployment
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;To open your app, try &lt;code&gt;heroku open -a example-heroku-deployment&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;5. Wrap&amp;nbsp;up&lt;/h2&gt;
&lt;p&gt;You should be seeing your site live at &lt;em&gt;example-heroku-deployment.heroku.com&lt;/em&gt; (or whatever you named your app as, followed by &lt;em&gt;heroku.com&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;A troubleshooting appendix should come&amp;nbsp;soon!&lt;/p&gt;
&lt;!-- # Troubleshooting

## 1. Heroku H10 error

Notorious! --&gt;</content><category term="meta"></category><category term="programming"></category><category term="docker"></category><category term="python"></category></entry><entry><title>Full-time atÂ BetterUp</title><link href="/posts/2021/Sep/promo-betterup/" rel="alternate"></link><published>2021-09-13T00:00:00-07:00</published><updated>2021-09-13T00:00:00-07:00</updated><author><name>Alex Liebscher</name></author><id>tag:None,2021-09-13:/posts/2021/Sep/promo-betterup/</id><summary type="html">&lt;p&gt;Becoming &lt;span class="caps"&gt;FTE&lt;/span&gt; at&amp;nbsp;BetterUp&lt;/p&gt;</summary><content type="html">&lt;p&gt;After 6 months as a contract research assistant at BetterUp, Iâve been hired 
on full-time as a Jr. Computational Social&amp;nbsp;Scientist&lt;/p&gt;</content><category term="misc"></category><category term="news"></category></entry><entry><title>DataÂ Science</title><link href="/posts/2021/Jul/data-science-conversation/" rel="alternate"></link><published>2021-07-21T00:00:00-07:00</published><updated>2021-07-21T00:00:00-07:00</updated><author><name>Alex Liebscher</name></author><id>tag:None,2021-07-21:/posts/2021/Jul/data-science-conversation/</id><summary type="html">&lt;p&gt;A conversation with Kyle Shannon and his data science&amp;nbsp;class&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Alex Liebscher&lt;/strong&gt;: I&amp;#8217;m from San Jose, California, and I went to school at &lt;span class="caps"&gt;UCSD&lt;/span&gt;. I came in as a freshman in 2016 as an economics major. This was something that I really thought I wanted to do. I took one econ class and decided that I really didn&amp;#8217;t like it. I started reaching out to grad students and professors and talking to my friends, trying to get a feel for what other people were doing and why they were doing it. I think the most influential conversation I had was with a grad student from the cognitive science department. He opened up my eyes to cognitive science and what that was, and how psychology could interact with data. That was a really pivotal moment for&amp;nbsp;me.&lt;/p&gt;
&lt;p&gt;So I changed my major to cognitive science and I continued on that path, still not really knowing what I wanted to do. I liked neuroscience, and the idea of intelligence, and mixing in philosophy and stuff really struck me. And so I dug a little further in on this. The summer after my freshman year I worked as a landscaper up near Tahoe. I only say this, and I&amp;#8217;ll get back to it later, to make a point that you don&amp;#8217;t always have to have a data science internship every single summer of your college&amp;nbsp;career.&lt;/p&gt;
&lt;p&gt;Going into my sophomore year, I got a lab position with a psychology lab on campus. We were looking at neural signals in song birds. I thought this taste of research was intellectually stimulating, but also boring at the same time. In general, I liked the people that I was working with, but I didn&amp;#8217;t really connect that well with anyone. I just floated through this research position for my sophomore year, not knowing where I was heading. Only at the end of that year did I finally get involved in a project and start to exercise some of the skills that I was learning in my classes. It was at this point that I realized how interesting research could be, and how you could mix math with data. I pushed hard to find an internship somewhere in the data field for the summer after my sophomore year. I ended up reaching out to someone in an &lt;span class="caps"&gt;HR&lt;/span&gt; department at a company called TextRecruit. They were about a 60 person startup in downtown San Jose. I sent someone at their &lt;span class="caps"&gt;HR&lt;/span&gt; a message on LinkedIn and introduced myself. I said I was looking for a data science type position, and one thing led to another. At TextRecruit, I did some data analysis on a big data set of basically text messages. They had me come in and figure out, basically, is their chatbot product working, what is it doing, and where can we improve things. This was a good experience for me because they didn&amp;#8217;t have any big expectations for me; they let me loose with their data so I could learn things. This internship is what introduced me to natural language processing, machine learning, neural networks, and data&amp;nbsp;engineering.&lt;/p&gt;
&lt;p&gt;I went into my junior year with this idea that data, language, and machine learning were really cool things. I joined a lab in the cognitive science department. I got started on one of the projects that a graduate student in the lab and I had talked about. He held my hand the first few months as I got to know the work that they were doing and why they were doing it. Throughout the rest of my junior year I was taking, for example, the &lt;span class="caps"&gt;COGS&lt;/span&gt; 118 series, and I ramped up my math background. I took some probability classes, and statistics classes, getting the tools to understand what I could do with machine learning and natural language processing. I continued on with the research that I was working on, which revolved around how the metaphors that we use in our daily communication affect how we think and how we behave. This was not so much machine learning and natural language processing, it was much more cognitive linguistics and psychology. But it introduced me to research methods, experiment design, the process of doing research, and how you come up with research&amp;nbsp;questions.&lt;/p&gt;
&lt;p&gt;I looked around for an internship between between my junior and my senior year. I wasn&amp;#8217;t really on the ball with it and I didn&amp;#8217;t end up getting an internship for that summer. I think I was expecting it to be as easy as the summer before: reach out to a couple of recruiters on LinkedIn and land something. Well, it wasn&amp;#8217;t that easy. Instead of doing an internship, I continued my research over that summer. Again, I took a job as a landscaper, and I also drove for&amp;nbsp;Lyft.&lt;/p&gt;
&lt;p&gt;Going into my senior year, I had made some good progress on my research. We were finalizing a paper for an upcoming cognitive science conference. That was really exciting to learn about the formalisms of research. I was taking mostly machine learning courses and math courses. I took Leon Bergen&amp;#8217;s &lt;span class="caps"&gt;NLP&lt;/span&gt; course, in the linguistics department. By my senior year, I was specializing in natural language processing because that&amp;#8217;s where my background was in and where my interests were. When talking to people, I was selling myself as an &lt;span class="caps"&gt;NLP&lt;/span&gt; engineer. March of my senior year was when &lt;span class="caps"&gt;COVID&lt;/span&gt; hit. Prior to this I had started job searching for a full time &lt;span class="caps"&gt;NLP&lt;/span&gt; engineer position after graduation. I had a couple of good conversations, a couple interviews, none of which really went anywhere. I learned some valuable lessons about how to interview and how to search for jobs. I slowed down a little bit the last quarter of my senior year. I took two classes and also did the cognitive science honors thesis program. But I had to take a step back and figure out, How can I get a full time job? I started talking to some people that I knew, and they gave me some ideas for how to go about a job search. It was about this time that I learned the importance of networking and connecting with other people in my field. I ended up not finding a job by the time that I graduated. I heavily ramped up my networking. I started talking to data scientists, machine learning engineers, data analysts, data engineers, basically anyone that was within the general area that I was in. My goal with this was to broaden the number of people that I could reach out to should I have questions, or if I needed something professionally. But also I was just learning about&amp;nbsp;careers.&lt;/p&gt;
&lt;p&gt;I had this idea that I wanted to be an &lt;span class="caps"&gt;NLP&lt;/span&gt; engineer, but at the same time it felt almost impossible because it seemed like every &lt;span class="caps"&gt;NLP&lt;/span&gt; engineer or machine learning engineer job application was asking for 5, 10, 15 years of experience. This was really frustrating, and so I was trying to figure out what I could do either in the meantime or instead of that path. Soon though, a friend of mine from &lt;span class="caps"&gt;UCSD&lt;/span&gt; reached out. He was very involved in the Basement, and has this tech entrepreneur spirit in him. He asked if I was doing anything and if I was looking for work. He said that he was building up a team and was looking for an &lt;span class="caps"&gt;NLP&lt;/span&gt; engineer and felt like I could make a good fit. I had maybe 3-5 interviews going on at that point, and he was eager to find someone to fill the position. I figured that any experience is better than no experience, and nothing was sure with the other interviews that I was&amp;nbsp;in.&lt;/p&gt;
&lt;p&gt;I took this position with a very small startup: there were six of us, including me. My responsibilities here started out with working on their optical character recognition process. They had millions of property and real estate &lt;span class="caps"&gt;PDF&lt;/span&gt; documents. The goal was to &lt;span class="caps"&gt;OCR&lt;/span&gt; those, and extract the text from them so that we could display that text to the end user. I started out doing this &lt;span class="caps"&gt;OCR&lt;/span&gt; work, and it was my job to figure out how we could &lt;span class="caps"&gt;OCR&lt;/span&gt; all 20 or 30 million documents. This was unlike anything that I had worked on the past. With my research position I was working with a fairly small data set. And no longer was it just a simple data format, numerical or anything like that. These were PDFs, which are notoriously difficult to work with. We were on a tight timeline for that, so my position soon shifted into more of the &lt;span class="caps"&gt;NLP&lt;/span&gt; stuff. After we got all the text drawn out from these documents, we needed to come up with some models or some heuristics to pull out the interesting information. This was my favorite part of the role. I ended up not doing a whole lot of machine learning in this section of my role. Machine learning, at least the state that it&amp;#8217;s at right now, especially for natural language processing, requires you to have a huge labeled training set. We didn&amp;#8217;t have this and we certainly didn&amp;#8217;t have the budget to create it. We had to sidestep that, put it on the back burner and figure out what we could do that didn&amp;#8217;t involve having a huge labeled data set. The rest of the company was building out the product and talking to potential customers. In the end, we weren&amp;#8217;t able to sell the product, and we ran out of money and had to fold. The last few weeks were immensely frustrating for me. I had to put on my marketing and sales hats and make cold calls and reach out to people, and do things that I was not trained for and was not interested in. Although it was really frustrating, I learned a lot of valuable lessons from this. I have a hunch that any engineer, any computer science or mathematics person, that goes to work in the corporate world should be required to take some sort of class or internship as a salesperson. The two go hand in hand. If you&amp;#8217;re an engineer, you don&amp;#8217;t have a job if there&amp;#8217;s no one selling. If you&amp;#8217;re a salesperson, you don&amp;#8217;t have anything to sell if there&amp;#8217;s not an engineer. I think that that relationship isn&amp;#8217;t valued&amp;nbsp;enough.&lt;/p&gt;
&lt;p&gt;Anyways, we folded and I was reaching out to old connections of mine. One of them, Andrew, was working at a company called BetterUp. He was looking for someone with my skill set to take on fairly soon and we decided that it would be a good fit. That&amp;#8217;s when I joined BetterUp as a research assistant. Since then I&amp;#8217;ve done a variety of different things. I started out working on an internal search engine that brought in a lot of my DevOps and &lt;span class="caps"&gt;NLP&lt;/span&gt; engineering skills. I got to dig in to information retrieval and big data sets, and this was really fun. We built out an excellent prototype that got a lot of people jazzed. I&amp;#8217;ve also been doing various survey analyses. I have been taking part recently in a broader research goal of studying how we communicate, mechanically, in conversations. So looking at things like how long are my turns, or utterances? How long am I speaking for? How long do I wait after you talk to for me to talk again? And then I&amp;#8217;ve also been helping build experiments to study how people behave in the workplace. A big aspect of what makes you happy at work is how you&amp;nbsp;communicate.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Kyle Shannon:&lt;/strong&gt; Great, thanks for that overview. You think about brain when you spoke even you speak to most data scientists machine my engineers or anybody that kind of fall other than that discipline. Um He always kind of hear a different story about how to kind of fall into that job, like if you talk to maybe like a traditional software engineer, it&amp;#8217;s typically like I went to school first yes degree or you know, it&amp;#8217;s like a boot camp and then kind of got internship and work that way, but at least in my experience data scientists kind of it&amp;#8217;s kind of a hodgepodge of all different types of backgrounds and experiences that kind of through people into that line of work. So it&amp;#8217;s interesting to&amp;nbsp;hear.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;AL&lt;/span&gt;:&lt;/strong&gt; I think something that I&amp;#8217;ve been learning recently is that there&amp;#8217;s no such thing is no such thing as a bad professional experience. My research has been incredibly useful in building my skill set. My experience at a startup was incredibly valuable. My jobs as a Lyft driver and as a landscaper have been built out my problem solving tools. I think as a data scientist there&amp;#8217;s no &amp;#8220;bad&amp;#8221; path or one set path that someone should or needs to take in order to become a data&amp;nbsp;scientist.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;KS&lt;/span&gt;:&lt;/strong&gt; A lot of people tend to get hung up on the tools, techniques, and analysis that you have to do to do data science or any sort of &lt;span class="caps"&gt;STEM&lt;/span&gt; based project. But I always find the most valuable skill and the skill that I think even like lot junior engineers and scientists have a hard time getting really good at is just communication. How do you effectively communicate in a simple and fast way? What is it that you&amp;#8217;re trying to do or what you need? Or like you said like sales people are very good, like good sales people are very good at communicating in a very good at finding out what are the problems that somebody has and how kind of match solution to that problem in a way that allows them to think that I really care about solving the problem for them, which we need to do, but also that we have a product that you can use. Um consequently they also do a bad job sometimes of reassuring customers that we can solve something for you because they don&amp;#8217;t have a good conversation or a good communication with the engineering team. So they&amp;#8217;ll come back and say, what do you mean? You can&amp;#8217;t influence this feature this way. I told the, you know, the customer that we could and so that communication of how to communicate on it with your colleagues, but also with people who are not technical or we don&amp;#8217;t necessarily know the work that you&amp;#8217;re doing is um, one of the biggest things that I think people going into any stem field can learn to get better at, it&amp;#8217;s really&amp;nbsp;important.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;AL&lt;/span&gt;:&lt;/strong&gt; One of the things that I&amp;#8217;ve been realizing lately is the importance of writing and documenting and being able to communicate effectively. I recommend you write what you do and write how you feel and write down your thought process. People want to know how you got from A to B, and you need to be able to communicate that. I&amp;#8217;ve found that a lot of engineers can come up with a solution, beautiful or not, but struggle to communicate why they did that or how they did that. As if they never really thought critically about why they did that. It&amp;#8217;s been frustrating to see and introspect on how I do that, and then try to improve from there. I agree with you, communication is&amp;nbsp;important.&lt;/p&gt;
&lt;p&gt;The other thing that you just made me think of is how bogged down the data scientist gets on the tools and the technology. Someone that I talked to a couple months ago, asked what I was interested in as a machine learning engineer and as a data scientist. I said, Python and PyTorch and this and that, and all these tools and technologies. He came back to me basically saying, that&amp;#8217;s great that you&amp;#8217;re interested in tools, but you don&amp;#8217;t seem to have a problem or an issue that you care about. I think it&amp;#8217;s very, very easy to go through college thinking that knowing the tools and technologies is enough, when in fact it&amp;#8217;s important to understand how to critically think. It seems like you can learn the tools from a variety of different ways, like taking the data science courses and the CogSci courses at &lt;span class="caps"&gt;UCSD&lt;/span&gt; is one way to learn it. But as I think about the classes that I took, a lot of them didn&amp;#8217;t really teach me how to think. They taught me the tools and the math and the statistics behind everything, but on a day to day basis, that&amp;#8217;s only half of the problem, and it&amp;#8217;s hard to get past that desire to just want to know the technology and not the thought&amp;nbsp;processes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;KS&lt;/span&gt;:&lt;/strong&gt; Yeah, I think that&amp;#8217;s a why often companies will ask for people who have masters or PhDs because at that level you&amp;#8217;ve kind of transitioned from being asked to do problem sets, to where it&amp;#8217;s more of, you set your own barometer of success with your own parameters. You have your own constraints and that&amp;#8217;s up to you kind of fulfill these things, and that&amp;#8217;s sort of a different way of approaching problems. As an undergrad, it always seems like it&amp;#8217;s sort of this epic struggle between learning the right tools and techniques that are used in industry for a job, and also understanding like the holistic sense of how to learn new tools. And from a theoretical base, the process you would take to do something. At that point the tools are almost irrelevant in some way. And just with how quickly technology moves anyways, the tools are gonna change. I mean, everybody has like, other than any front end engineering, like, you know, it was like a new front end framework that comes out every two years, it&amp;#8217;s just like uh you know, these things are constantly changing and evolving and so there&amp;#8217;s that big balance of how do I learn just enough tooling and understand how to do tools to do something, but then also understand how to approach problems, how to ask interesting questions. Um and that&amp;#8217;s because if you typically learn on the job or in further educational training, so you have to kind of like short circuit that a little bit, you know, through internships or working on like really interesting problems on your own and your own projects, You kind of like do that a little bit, you just like to talk about in your&amp;nbsp;interviews.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;AL&lt;/span&gt;:&lt;/strong&gt; I couldn&amp;#8217;t agree with you more. I think that it&amp;#8217;s really useful to have the technical skill set. Every day I go back and forth between R and Python. It&amp;#8217;s incredibly useful to be fluent in both of those so that I can utilize them when they should be utilized. I think data scientists, for example, get hung up on this R versus Python debate. But really you should know both because they both have their strengths and their weaknesses. You shouldn&amp;#8217;t let a tool define the work that you do, or the questions that you ask. Instead, you should be coming up with the questions and apply the tool when necessary and where useful and&amp;nbsp;appropriate.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;KS&lt;/span&gt;:&lt;/strong&gt; Do you recommend going to grad school for machine&amp;nbsp;learning?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;AL&lt;/span&gt;:&lt;/strong&gt; I don&amp;#8217;t know, I haven&amp;#8217;t done it. I have been debating a PhD program for myself because I do really like research. Something that I&amp;#8217;m realizing is that, at least in the PhD, it&amp;#8217;s important to know what what you want to work on and who you want to work with, the latter being especially important. You&amp;#8217;ll be spending at least four years working with the same group and the same advisor. It&amp;#8217;s important that you enjoy the environment that you&amp;#8217;re working in. The problems can always change. Say you pick a group and you don&amp;#8217;t like the advisor and there&amp;#8217;s no room to move around. You&amp;#8217;ll drop out because you won&amp;#8217;t enjoy it. If you choose an advisor or school who is flexible, then I think you&amp;#8217;re setting yourself up for success. You&amp;#8217;re the only person that&amp;#8217;s going to know whether you want to go to grad school for machine learning. There are plenty of people who get machine learning engineer roles without going to grad&amp;nbsp;school.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;KS&lt;/span&gt;:&lt;/strong&gt; That&amp;#8217;s a good piece of advice for grad school. I think there are typically three reasons to go to grad school. One is because you want to go into academic research. You&amp;#8217;re really interested in one specific area, and for that you have to go beyond just master study because you need to start doing research. If you are curious about one particular part of machine learning or something, then a PhD can really help you get there. But maybe you have a &lt;span class="caps"&gt;STEM&lt;/span&gt; background and you want to do machine learning engineering then maybe a master&amp;#8217;s degree could be useful to fill that gap in knowledge. Or maybe for upgrading your career. So it&amp;#8217;s kind of like a strategic option. You have grad school for the pure beauty of research, or as a strategic option, or maybe another reason. But have a good reason to do it, don&amp;#8217;t just do it because you have nothing else to&amp;nbsp;do.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;AL&lt;/span&gt;:&lt;/strong&gt; One more note about grad school: I&amp;#8217;m extremely happy that I didn&amp;#8217;t go to grad school immediately out of undergrad. I think that the undergraduate who goes to graduate school immediately and is truly happy with what they do, is few and far between. Getting a variety of experiences post-undergraduate can never be a bad idea. It will help you hone in on exactly what you care about and what interests you. Having industrial, corporate experience can also help you figure out how those tools can be applied in the real&amp;nbsp;world.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;KS&lt;/span&gt;:&lt;/strong&gt; From my experiences in grad school, it wasn&amp;#8217;t even so much material that I learned. Certainly that&amp;#8217;s a big part of it, but the biggest part was the people that you meet and the connections that you make. These are people who are, for the most part, interested in working in the area that you&amp;#8217;re interested in. You can rely on them when you&amp;#8217;re looking for new opportunities. It&amp;#8217;s kind of the social experience you have, and the more time you spend in industry beforehand, the easier it is to make those long lasting friendships because you bring something to the table in terms of your&amp;nbsp;experience.&lt;/p&gt;
&lt;p&gt;How would you recommend getting into data science positions or&amp;nbsp;internships?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;AL&lt;/span&gt;:&lt;/strong&gt; Network. I have only ever gotten one interview through applying to online applications. Yet, I&amp;#8217;ve gotten three jobs and a research position through networking. And I&amp;#8217;ve made a number of really awesome connections through&amp;nbsp;networking.&lt;/p&gt;
&lt;p&gt;Networking is something that I heard of an undergrad; people were always saying, &amp;#8220;Oh you should network,&amp;#8221; and you&amp;#8217;d hear this at career events and fairs, but no one ever really told me how or what that exactly entailed. It wasn&amp;#8217;t until my senior year that I read a blog post that had this email template that said, among other things, how to explain your interest in meeting with that person. It was then that it clicked with me, that networking is probably the best way to land a position and if you don&amp;#8217;t land the position, there are plenty of other benefits to&amp;nbsp;networking.&lt;/p&gt;
&lt;p&gt;All I really need to tell you is to go on LinkedIn and search up machine learning engineer at wherever. Or data scientist. Some of them have an email in their bios, some of them you can Google and find their home page. The goal is find an email address, and email them: &amp;#8220;Hey, I found your profile. I really like the work that you&amp;#8217;re doing at X company. It looks like you&amp;#8217;re working on Y technologies. This is something that really fascinates me. I&amp;#8217;m an undergraduate at &lt;span class="caps"&gt;UC&lt;/span&gt; San Diego and I am looking to grow my network. Do you have 15 minutes next week for an informational interview?&amp;#8221; The key there is &lt;em&gt;informational interview&lt;/em&gt;. The goal of an informational interview is an introduction. You two get to know each other a little bit and that&amp;#8217;s about it. You try to understand what they do and you tell them about what you&amp;#8217;re interested in, at least at this stage in your career. Don&amp;#8217;t jump out of the gates asking for a job at their company. In some cases, you might not even ask for anything. You just are there to hear about their story. People love to talk about themselves. When it comes time to ask for something, the more specific you can be, the better. Keep a tab on their work history, on the jobs page of their company, and see what positions are maybe opening up on their team or with people that they work with. If that&amp;#8217;s something that interests you, reach out to them and say, &amp;#8220;We spoke a couple months ago, I see that there&amp;#8217;s a new data science position open. Do you happen to know the hiring manager.&amp;#8221; I think that this will give you much more success in your job&amp;nbsp;search.&lt;/p&gt;
&lt;p&gt;However it does take a little bit more planning and preparation and time. With an online job application, you tailor your resume and a cover letter and send it off and you&amp;#8217;re done. It&amp;#8217;s frankly fairly easy, but at the same time there is very little feedback and you have no idea when you&amp;#8217;re going to hear back from them. On the contrary if you&amp;#8217;re talking to a real person and particularly someone that you know, then I think you&amp;#8217;re much more likely to have a direct introduction to someone that can get you hired or at least some sort of immediate feedback. As a new college graduate, people don&amp;#8217;t really expect you to have a whole lot on your resume. So trying to tailor that for every single job application will be far less effective than&amp;nbsp;networking. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;KS&lt;/span&gt;:&lt;/strong&gt; One person asked what were the most useful classes you took at &lt;span class="caps"&gt;UCSD&lt;/span&gt; for going into data&amp;nbsp;science?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;AL&lt;/span&gt;:&lt;/strong&gt; Before I answer that, I&amp;#8217;ll say that one of my regrets in college was not taking more classes outside of data science, machine learning, and cognitive science. College is a time to explore and to learn new things and when you branch out of your particular domain you learn new ways of thinking and solving problems. And that was something that I didn&amp;#8217;t realize when I was signing up for classes. And so I ignored uh beautiful classes in anthropology and history and computer science. Even I wasn&amp;#8217;t a computer science major. If you&amp;#8217;re a computer science major then maybe it would be cognitive science or something. Um But uh what I I wouldn&amp;#8217;t get too hung up on taking specific technical classes in college, there&amp;#8217;s always going to be more technical skills that you can learn on the job or through other experiences. Uh That being said some of the ones that I really enjoyed the &lt;span class="caps"&gt;COGS&lt;/span&gt; 118 series. Um I really enjoyed Professor De Sa, She was a great teacher. Um I really enjoyed I already mentioned this earlier but Leon Bergen&amp;#8217;s linguistics, natural language processing class. He did a really excellent job of explaining the mathematics behind um you know, LSTMs and recurrent neural networks. And at the very end of the quarter we even got into transformers which if you&amp;#8217;re familiar with, that was something relatively new at that time. And so that was exciting to be learning about something extremely cutting edge right there in class and in particular like the mathematics behind it. Um Let&amp;#8217;s see, I took a critical gender studies class on race, gender and &lt;span class="caps"&gt;AI&lt;/span&gt; um which was a really transformational class for me, it really pushed me to question a lot of the things that we learn as engineers, like asking why we&amp;#8217;re implementing something and what are the consequences if we implement this? Which I think aren&amp;#8217;t questions that engineers ask often enough. Um So I wouldn&amp;#8217;t shy away at all from the humanities, particularly if it has um maybe more technical bend to it. Um I think that maybe like the sociology class or the sociology department or something like that probably has a couple classes on like society and technology. I think that would um uh push you to think about technology and new and different ways. I still talk about that critical gender studies class to um interviewers and stuff. Um People people like seeing when you branch out uh and think about your domain in new in different ways. Let&amp;#8217;s see it. Um Math 181 um I think was like the mathematical statistics series that was a super useful um set of classes to take if you&amp;#8217;re interested in basically understanding how like neural networks work. Um It covers mostly like regressions and things like that, but neural networks are fancy regressions, um, and having a good foundation in linear regressions and things like that will be another tool to add to your toolbox so that when it comes time to apply some of these skills, you&amp;#8217;ll be able to know when it&amp;#8217;s going to be perfectly okay to have a logistic regression versus uh, 10 layer neural network. Um, and your boss will appreciate it. If you can choose a solution that requires less money and less time is more interpret herbal. Um, let&amp;#8217;s see. Let&amp;#8217;s see &lt;span class="caps"&gt;COGS&lt;/span&gt; 118 math, 181. If, if you find a grad course that you&amp;#8217;re interested in sign up for it. Um, even if you get like a B or maybe even a C. Um, it will put you in a new environment that will completely change uh, the flow that you&amp;#8217;re used to an undergraduate. Um, I took a couple cognitive science graduate courses and they&amp;#8217;re very small. Um 10-15 people. Most of the work is not necessarily application but it was more um revolving around research and reading papers and that sort of thing. Um Which uh I I found very useful. Um So if if you get a chance um you can sign up for definitely some of the colleagues graduate classes as an undergraduate you do need approval. I&amp;#8217;m not sure if you can take any of the math graduate courses or the &lt;span class="caps"&gt;C. S.&lt;/span&gt; Graduate courses. I think the &lt;span class="caps"&gt;CS&lt;/span&gt; graduate courses you can but um both of those would be good areas to look into if you&amp;#8217;re looking for an extra challenge but in a smaller group setting. Yeah. I don&amp;#8217;t know I think I think the classes really don&amp;#8217;t matter that&amp;nbsp;much.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;KS&lt;/span&gt;:&lt;/strong&gt; Uh huh. Yeah there&amp;#8217;s a it&amp;#8217;s always good to get out of your your comfort zone with different classes and to try and learn to overcome that inner-adversity to doing something new because I think about when you go in your first, the first day of the new job, it&amp;#8217;s going to be a new environment, you&amp;#8217;re not used to and be a lot adversity that you overcome within yourself to branch out, talk to people and meet you people try and make new friends learn. How do all these new procedures and processes and feel like you don&amp;#8217;t know anything. And so the more time you can spend putting yourself in those adverse environments as an undergrad when the stakes are not the same as they might be on your first job. Uh It&amp;#8217;s you know, it&amp;#8217;s probably fine and expected to fail as undergrad and to that process of failing, you always learn more and more so that you feel less hard for less often,&amp;nbsp;right?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;AL&lt;/span&gt;:&lt;/strong&gt; Yeah, real quick. I&amp;#8217;ll just say that, I think failure is great and I totally overestimated how much things mattered in undergraduate, I probably should have put myself in risky positions, taking classes that um you know, might not have contributed to my uh present technical knowledge right then, but over the long term would have let me fail and fail fast and learn something new at that&amp;nbsp;moment.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;KS&lt;/span&gt;:&lt;/strong&gt; You said you were interested in finance and investing, did you ever considered a career in quantitative finance or applying data science at all in the financial&amp;nbsp;world?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;AL&lt;/span&gt;:&lt;/strong&gt; I did briefly consider it, especially during my senior year when I was reaching out to people, I spoke to a couple quants um just to sort of hear about their day to day. Um And the technologies that they were working within the problems that they were solving and that sort of thing. Um Econ and investing in particular not so much finance or accounting still interests me but I know now that it&amp;#8217;s not a career I would enjoy. Having people and humans or even just like something organic is really important to me. Um And it&amp;#8217;s something that I really enjoy working on and being in a quantitative finance role I think for me personally it would be too rigid and not really working with the types of data and the types of problems that truly fascinate me. So yes if that&amp;#8217;s a career considering um I encourage you to find people in the roles that you&amp;#8217;re interested in and talk to them and see what they do want a day to day basis and the kinds of problems that they&amp;#8217;re asking and&amp;nbsp;answering.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;KS&lt;/span&gt;:&lt;/strong&gt; One person asked, when you get your first job, what advice do you have or like transitionary period to be successful in your new kind of job or new&amp;nbsp;career?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;AL&lt;/span&gt;:&lt;/strong&gt; I think that&amp;#8217;s a really good question and something that I&amp;#8217;m still figuring out. Um I don&amp;#8217;t think that there&amp;#8217;s anyone like thing that you can do. However, um some actually more general advice that you could apply to more situations than just the first job. Ask a lot of questions and reach out to a lot of people. Um as a new person, you have this sort of grace period where you can ask any stupid question that you want and you can just blow it off, like other people just blow it off like oh they&amp;#8217;re new to the role, it&amp;#8217;s okay that they don&amp;#8217;t understand this and this could be technical questions, or it could be questions about the culture or um questions about your own role or anything like that, and getting all those questions that you can think of out of the way as soon as possible. Um Not only uh build your own knowledge of where you&amp;#8217;re working in the work that you&amp;#8217;re doing, but it also sort of sets the tone for later in your career with this team or this company. Um where you just are like an inquisitive person with a lot of questions, which is never a bad thing. I think that um should always be asking questions, but to set that tone early on, like this is who I am. I&amp;#8217;m a person to ask a lot of questions is a decent personality characteristic. Um And then depending on the size of your company, um Right now I&amp;#8217;m at a company that&amp;#8217;s almost 300 people prior to this, I was a company that was six people um at the company. Now that I&amp;#8217;m with, I&amp;#8217;ve been almost weekly reaching out to new people in various areas of the company and just having a 30 minute coffee chat with them. Um What do you do? How did you get to where you&amp;#8217;re at? Um Are you reading any interesting books? Uh you know, these sort of conversations um which not only gets your name around, which is important if you&amp;#8217;re trying to sell your work or something internally, maybe for a promotion or something like that, but you also just get an opportunity to meet new people. Um Anything else? Uh One more thing I would recommend would be document everything that you do, especially early on in your career, um writing writing down, I would keep a journal of some sort with you and every day write down what you&amp;#8217;re doing at the end of the day, maybe what you accomplished or something like that. Um You don&amp;#8217;t have to get too detailed or anything, but in six months when you&amp;#8217;re uh talking with your boss about a promotion or you&amp;#8217;re doing new interviews or something like that, inevitably they will ask you, you know, what have you accomplished or what have you, what have you been up to or doing? Um And it will be very helpful to have notes from six months ago about what you were doing, either on a day to day basis or um just large scale projects that you&amp;#8217;re working on, um document the questions that you&amp;#8217;re asking in your thought processes as well. People care a lot about how you think and how you solve problems, um And so, being able to articulate how you do that in an interview um is&amp;nbsp;invaluable.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;KS&lt;/span&gt;:&lt;/strong&gt; Another question, how much does &lt;span class="caps"&gt;GPA&lt;/span&gt;&amp;nbsp;matter?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;AL&lt;/span&gt;:&lt;/strong&gt; &lt;span class="caps"&gt;GPA&lt;/span&gt; doesn&amp;#8217;t&amp;nbsp;matter. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;KS&lt;/span&gt;:&lt;/strong&gt; Um I mean pass the threshold before you wanna be like above the three point as much as possible, you know? Typically typically. But it also depends on school. I think to like a lot of people will understand certain schools do great harder. Such a for grad school you&amp;#8217;re applying, I don&amp;#8217;t look at students differently to pander school. If you take a few years and get some industry experience before going to grad school. Uh the impact that &lt;span class="caps"&gt;GPA&lt;/span&gt; has drops dramatically. Um At least from what I understand. Yeah. Nobody has ever actually asking for my &lt;span class="caps"&gt;G. P. A.&lt;/span&gt; From undergrad or grad school except my apply to grad school. They want to know, I don&amp;#8217;t think anyone really looked at Yet as you T doesn&amp;#8217;t matter for job application, but to a certain point, if you put down like a 1.9, you know that calls and questions, but if it&amp;#8217;s anywhere like above a three point people generally don&amp;#8217;t care. Um If it&amp;#8217;s like a 2.8 or something, but you&amp;#8217;re from a really, you know you got a lot of job experience like Alex was saying that people don&amp;#8217;t often even look at it and those forms you fill in the online job stuff, they&amp;#8217;ll it just goes to a database which makes up like a profile for you. It doesn&amp;#8217;t mean that every scouring the data of your profile um they use it for just like completeness. But you know like I wouldn&amp;#8217;t put Gps on your resume no reason to have a funny conversation. A friend who my friends had a 4.0 and in grad school, I was like, you know that&amp;#8217;s actually pretty bad because now you&amp;#8217;re gonna have that people expect you to perform in a 4.0 all your interviews. So yeah, &lt;span class="caps"&gt;GPS&lt;/span&gt; kind of like it&amp;#8217;s one of the things that people think matters a lot, but often is a, it doesn&amp;#8217;t end as certain is often a poor indicator on the job performance as&amp;nbsp;well.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;KS&lt;/span&gt;:&lt;/strong&gt; Does computer vision have a lot to do with data&amp;nbsp;science?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;AL&lt;/span&gt;:&lt;/strong&gt; Yes, definitely. Data science is a terrible term because everyone has a different definition of it. I&amp;#8217;m sure that Kyle and I have very different definitions of &amp;#8220;data science.&amp;#8221; But yes, computer vision is definitely an aspect of data science. I think most people would say that computer vision is a subcategory of machine learning, whether or not you call machine learning a subcategory of data science is maybe just a personal&amp;nbsp;preference.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;KS&lt;/span&gt;:&lt;/strong&gt; Yeah, the best definition of data science is the one that gets the customer to pay the most amount of&amp;nbsp;money.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;AL&lt;/span&gt;:&lt;/strong&gt; Speaking about getting a definition that the customer buys into, I think it&amp;#8217;s important as a data scientist to separate the hype from reality. It&amp;#8217;s very easy to get sucked into believing that this stuff is magic and that it can sell for an infinite amount of money. But I think in your undergraduate career you&amp;#8217;ll realize that it&amp;#8217;s not really that magical. Sure there are things that are unexplained, but at the end of the day, like I said, it&amp;#8217;s just regressions, and understanding that will humble&amp;nbsp;you.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;KS&lt;/span&gt;:&lt;/strong&gt; Yeah, it&amp;#8217;s very hard to get customers to understand, especially working like an intricate science consulting role is not necessarily like a spec based process, like engineering can be where you can spec out software requirements. For data science, you often don&amp;#8217;t know if you can have the right data to do the thing you want to do. That&amp;#8217;s why communication is also so important because you have to really talk to the stakeholders and the people who want something and to understand better what they need, what they&amp;#8217;re&amp;nbsp;offering.&lt;/p&gt;
&lt;p&gt;One last question: you mentioned transitioning between several labs. How do you know if the lab isn&amp;#8217;t right, or isn&amp;#8217;t right for you? And how do you leave a lab you don&amp;#8217;t like without burning&amp;nbsp;bridges?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;AL&lt;/span&gt;:&lt;/strong&gt; I think your undergraduate career boils down to the people that you&amp;#8217;re working with. I didn&amp;#8217;t feel welcomed in the first lab I was with, and I got the feeling that they thought I didn&amp;#8217;t know enough to be a useful asset in the lab. That&amp;#8217;s probably why I didn&amp;#8217;t stick around with them. If you&amp;#8217;re looking at research labs, talk to grad students over a cup of coffee. That&amp;#8217;ll give you a very good indication of whether or not you want to work with them. They will be teaching you a lot, so if you&amp;#8217;re asking them questions during your first meeting and they&amp;#8217;re just not that good at explaining what they do or how they do it, that&amp;#8217;s an indication that they might not be the best teacher for&amp;nbsp;you.&lt;/p&gt;
&lt;p&gt;If you&amp;#8217;re looking for a research position, I would skip talking to the professor at all, and go straight to the grad students. They&amp;#8217;re the ones doing most of the nitty gritty work. They&amp;#8217;ve got their own projects and are the ones looking for undergraduate research assistants. Ask if they&amp;#8217;ve got any projects that they might be looking for help on. If in a quarter or two, you just don&amp;#8217;t see it going anywhere or you decide that your interests have shifted, it&amp;#8217;s difficult to say, but just leave. You might you might burn a bridge, particularly with that grad student, but if your interests have shifted then it might not be that important of a bridge in the grand scheme of things. Your excitement on a topic and excitement for working with certain people could be more important. You might just let them know that your interests have changed and you&amp;#8217;ll be looking for work&amp;nbsp;elsewhere.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;KS&lt;/span&gt;:&lt;/strong&gt; Great, well thanks so much for joining us for classes Alex and taking the time and tell us your story and answer some questions and give some insightful&amp;nbsp;feedback.&lt;/p&gt;</content><category term="misc"></category><category term="statistics"></category><category term="data science"></category></entry><entry><title>Del v.Â Amazon</title><link href="/posts/2021/Jul/del-v-amazon/" rel="alternate"></link><published>2021-07-16T00:00:00-07:00</published><updated>2021-07-16T00:00:00-07:00</updated><author><name>Alex Liebscher</name></author><id>tag:None,2021-07-16:/posts/2021/Jul/del-v-amazon/</id><summary type="html">&lt;p&gt;The story of my friend Del, and small businesses against&amp;nbsp;Amazon&lt;/p&gt;</summary><content type="html">&lt;p&gt;Del was about 5&amp;#8217; 8&amp;#8221; and wore a black shirt with a black baseball cap. He approached me in NÄ Mea, a small store in Honolulu&amp;#8217;s Ward Village that sells Hawaiian and Polynesian art, calendars, clothing, food, jewelry, etc., as I was bent sideways browsing their wall of books. He asked me if I was looking for anything specific. &amp;#8220;Just browsing,&amp;#8221; I said. My mask hid my smile but I hoped that my eyes creased so he would know that I was friendly. His words were lined with enthusiasm and joy, so much so that I was almost suspicious of his&amp;nbsp;motives.&lt;/p&gt;
&lt;p&gt;&lt;span class="dquo"&gt;&amp;#8220;&lt;/span&gt;Okay, what do you like to read?&amp;#8221; he probed me. I was intimidated, since I don&amp;#8217;t think there&amp;#8217;s one right answer to a question like that. And what if I say something he doesn&amp;#8217;t&amp;nbsp;like?&lt;/p&gt;
&lt;p&gt;I stammered, &amp;#8220;Just about anything. I thought this book looked interesting.&amp;#8221; I pointed to a small book on their new releases table. &amp;#8220;Do you know anything about it?&amp;#8221; It was titled &lt;em&gt;The Properties of Perpetual Light&lt;/em&gt; and had an attractive&amp;nbsp;cover.&lt;/p&gt;
&lt;p&gt;We went on with this small talk about books for a couple minutes. He suggested a title on another shelf, &lt;em&gt;Shoals of Time&lt;/em&gt; by Gavan Daws. This was far more academic than what I was hoping for. It was also out of my book&amp;nbsp;budget.&lt;/p&gt;
&lt;p&gt;&lt;span class="dquo"&gt;&amp;#8220;&lt;/span&gt;I&amp;#8217;m here on vacation so space in my bag is limited,&amp;#8221; I&amp;nbsp;mourned.&lt;/p&gt;
&lt;p&gt;&lt;span class="dquo"&gt;&amp;#8220;&lt;/span&gt;Oh, where are you from?&amp;#8221; he&amp;nbsp;asked.&lt;/p&gt;
&lt;p&gt;At this, I told him of California, my trip to Hawaii, my job, and my travels up til then. He was receptive, and I could tell he was listening carefully by the way he looked at me. Del must have found something special in our short conversation, because he suggested that we stay in&amp;nbsp;touch.&lt;/p&gt;
&lt;p&gt;&lt;span class="dquo"&gt;&amp;#8220;&lt;/span&gt;I should get your phone number. And you can have mine,&amp;#8221; he&amp;nbsp;offered.&lt;/p&gt;
&lt;p&gt;&lt;span class="dquo"&gt;&amp;#8220;&lt;/span&gt;Yes, definitely,&amp;#8221; I warmly replied without any definite feeling about where this would go. Then came a&amp;nbsp;surprise.&lt;/p&gt;
&lt;p&gt;&lt;span class="dquo"&gt;&amp;#8220;&lt;/span&gt;I need to finish some work in the back, but do you have time right now? Are you doing anything or have plans?&amp;#8221; he asked. Being on vacation, I didn&amp;#8217;t have any plans for that afternoon and exploring, and I told him&amp;nbsp;so.&lt;/p&gt;
&lt;p&gt;&lt;span class="dquo"&gt;&amp;#8220;&lt;/span&gt;Have you had lunch? Would you like to get lunch&amp;nbsp;together?&amp;#8221;&lt;/p&gt;
&lt;p&gt;My stomach was growling and I was already impatient with my inability to choose a book. I agreed quickly to his proposal. I thought this would be a good opportunity to meet a Hawaii native. He could show me show where the locals eat. Del rushed off to the back and came out only a minute later, surprising me again, this time with a business card in his hand. On it he had scribbled his name and phone number. He told me, &amp;#8220;Ok, wait here. 15&amp;nbsp;minutes.&amp;#8221;&lt;/p&gt;
&lt;p&gt;He scurried away, and I continued to peruse the bookshelves. I bought two books and loitered in the store, admiring the carved bone tools, the wooden necklaces, and the hand twisted earrings. Soon enough, Del popped up right beside&amp;nbsp;me.&lt;/p&gt;
&lt;p&gt;&lt;span class="dquo"&gt;&amp;#8220;&lt;/span&gt;Where do you wanna go?&amp;#8221; he&amp;nbsp;asked.&lt;/p&gt;
&lt;p&gt;&lt;span class="dquo"&gt;&amp;#8220;&lt;/span&gt;Anywhere,&amp;#8221; I said, smiling. &amp;#8220;You know the area&amp;nbsp;best.&amp;#8221;&lt;/p&gt;
&lt;p&gt;&lt;span class="dquo"&gt;&amp;#8220;&lt;/span&gt;Ok, in here,&amp;#8221; and we walked out into the mall which his store was tucked away&amp;nbsp;in.&lt;/p&gt;
&lt;p&gt;He led the way through the crowded mall, barely giving me enough time to see what it had to offer. He asked me once or twice more what I wanted to eat. I earnestly told him I didn&amp;#8217;t mind, I was hungry and curious where he might take me and what we might discover. He led us through and out, taking us down the street and around the corner. We found a restaurant with a clean and neat patio and a sign that read &amp;#8220;Kitchen and Meatery.&amp;#8221; After we took a seat, I asked, &amp;#8220;How long have you been working at the&amp;nbsp;store?&amp;#8221;&lt;/p&gt;
&lt;p&gt;&lt;span class="dquo"&gt;&amp;#8220;&lt;/span&gt;4 years,&amp;#8221; he said, nodding&amp;nbsp;slowly.&lt;/p&gt;
&lt;p&gt;&lt;span class="dquo"&gt;&amp;#8220;&lt;/span&gt;And before&amp;nbsp;that?&amp;#8221;&lt;/p&gt;
&lt;p&gt;&lt;span class="dquo"&gt;&amp;#8220;&lt;/span&gt;At the other store, in downtown.&amp;#8221; Apparently NÄ Mea had two locations, the original and then the one I visited. He told me how the book selection at the other was impressive, and how I&amp;#8217;d like that one more. When I asked him how long he had worked at that store, he thought for a second and said, &amp;#8220;About 29&amp;nbsp;years.&amp;#8221;&lt;/p&gt;
&lt;p&gt;I wasn&amp;#8217;t expecting this loyalty and inertness. I asked him how much the island had changed in the last 30 years. &amp;#8220;A lot,&amp;#8221; he said, nodding even slower this time. I could tell that this was something he could feel within, but hadn&amp;#8217;t spoken about in a long time. As he talked about the changes on O&amp;#8217;ahu, there was a feeling of by-gone times and pernicious tourism, but also a strong appreciation and pride for the growth of one&amp;#8217;s homeland. Del told me he grew up on the island of Hawai&amp;#8217;i, that he never learned to surf, and that now he lived north near the university where he loved to attend the football games. He told me he had once visited San Diego, some 10 years ago, to support the university&amp;#8217;s football team in their game against San Diego State. He told me of the pride and energy he felt when his team scored and the Hawaii section of the stands roared with excitement. He told me about the decline of the sugar cane plantations, like the Waialua mill and its many Japanese and Filipino laborers, and the decline of the Dole pineapple business, and the many Hawaiians in that&amp;nbsp;family.&lt;/p&gt;
&lt;p&gt;He was quick to agree with many things I had to say, and when he didn&amp;#8217;t agree he scrunched his face and shook his head. His accent was dense and difficult to navigate, like the tall brush you wade through in the forests on the north shore. I had to lean in with an ear out to pick up every syllable, most of which he didn&amp;#8217;t seem to care for and would just disappear when he&amp;nbsp;spoke.&lt;/p&gt;
&lt;p&gt;I ate pork belly eggs benedict; he had a grandule rice and eggs plate. When I asked if pork belly was popular (I had heard it was), he said, &amp;#8220;Oh, yes. I have many friends who eat it. I don&amp;#8217;t like it.&amp;#8221; His plate was colorful from all the cumin, curry, and chili powder that had made their way onto it. I noticed that when he ate, he flipped his fork upside-down, held it still, and pushed rice with his knife onto the back of the fork. This was how he brought food to his&amp;nbsp;mouth.&lt;/p&gt;
&lt;p&gt;We talked about my work, his work, his interests, me moving to Berkeley, and his family. When I asked him if he was married, he said &amp;#8220;No&amp;#8230; but I have four kids.&amp;#8221; It must have been easy to notice that I lacked anything to say, so he continued, &amp;#8220;Four cats!&amp;#8221; I still couldn&amp;#8217;t tell if he was joking. He wasn&amp;#8217;t; I learned that to him, those cats are his kids. I loved that when I asked him what he did on the weekends, he said with a gentle smile, &amp;#8220;I get a blanket, go to a beach where only the locals go, bring a book, and just read and enjoy the afternoon under a tree.&amp;#8221; This was my kind of&amp;nbsp;guy.&lt;/p&gt;
&lt;p&gt;Del insisted on buying our lunch. I pushed back but his hospitality and generosity were stubborn. We agreed that I would leave the tip. When we walked out, he burst into a race walk! I told him him I&amp;#8217;d hoped that I hadn&amp;#8217;t made him late back to work. He quickly declined that idea and politely said that he was glad we went out. We rushed back through the hot and humid streets with the sun hitting us hard. At his turn, he rushed to say it was nice to meet me. We fist-bumped, as he was used to doing during &lt;span class="caps"&gt;COVID&lt;/span&gt;. Just as quickly had our day began together did it end. I stood alone on the sidewalk not sure where to go&amp;nbsp;next.&lt;/p&gt;
&lt;hr /&gt;

&lt;p&gt;Amazon began in 1994 as a bookstore: a story which is now Silicon Valley folk-lore. Humble beginnings; turbulent trajectories; and ultimately, power in ways never before&amp;nbsp;conceived.&lt;/p&gt;
&lt;p&gt;Amazon is no longer a bookstore. Today, its profit largely comes from the healthy margins of Amazon Web&amp;nbsp;Services.&lt;/p&gt;
&lt;p&gt;So if Amazon is no longer a bookstore, what is it? What is it not? These are the same questions for Google, Microsoft, etc. (and it&amp;#8217;s one way they skirt around antitrust laws). Does Amazon add value to society? If so, what kind of value? Is it value that matters for individuals and humankind? Value like memories, friendship, or kindness? In most ways, I think it doesn&amp;#8217;t. Never has Amazon provided me or any other consumer I know with life-long memories or friendship or one of the many other things which Del showed me in our hour together. I also think that this is what makes Del and other small businesses the heart of society, pumping life and blood through our people. Amazon, and generally mega-tech companies, are capillaries: fringe elements of our communal body and spirit, substantiative but irrelevant without the heart. In the worst cases, these companies are behemoth advertising leeches, external to &amp;#8220;us&amp;#8221; and feeding on our blood and offering no nutrients in&amp;nbsp;return.&lt;/p&gt;
&lt;p&gt;That is to say, Del provides in ways that Amazon cannot, and will not. It is not their goal or their function to provide value like Del does. Yet, there remains a place in this universe for both to vie for customers. Amazon is certainly allowed some share of the market. But if Del and NÄ Mea go out of business, so does at least 30 years of Hawaiian culture and history bundled into a vibrant hole-in-the-wall shop. And so does any opportunity for interaction as a consumer to be leagues more than a market&amp;nbsp;transaction.&lt;/p&gt;
&lt;p&gt;There are so many beautiful things about small local businesses that to buy from Amazon should only be a last&amp;nbsp;resort.&lt;/p&gt;
&lt;p&gt;When I began putting some of these thoughts on paper, my ideas largely revolved around what made Amazon bad. That list could stretch for long enough, including its poor &lt;a href="http://amazonemancipatory.com/"&gt;labor practices&lt;/a&gt;; its negative effect on &lt;a href="https://www.theatlantic.com/business/archive/2018/02/amazon-warehouses-poor-cities/552020/"&gt;cities and communities&lt;/a&gt;; its negative impact on the &lt;a href="https://link.springer.com/chapter/10.1007/978-3-030-49384-4_6"&gt;environment&lt;/a&gt;; its insidious and gross &lt;a href="https://itep.org/amazon-has-record-breaking-profits-in-2020-avoids-2-3-billion-in-federal-income-taxes/"&gt;tax evasion&lt;/a&gt;; its dangerous and reckless standards for &lt;a href="https://www.nytimes.com/2019/09/05/us/amazon-delivery-drivers-accidents.html"&gt;delivery drivers&lt;/a&gt;; and less tangibly, its influence on the American&amp;nbsp;psyche.&lt;/p&gt;
&lt;p&gt;While these might be true, Amazon also positively contributes to society in dozens of ways. They create jobs and impressive economic opportunities, they create supply competition, lower consumers&amp;#8217; costs, and do in fact sometimes act philanthropically. But we must ask ourselves if the good outweighs the&amp;nbsp;bad.&lt;/p&gt;
&lt;p&gt;Unlike this fine ethical line which Amazon lumbers along, it is rare that a small business does not make up its negative impact with the benefits the community it&amp;#8217;s in reaps. The &lt;a href="https://www.nber.org/system/files/working_papers/w17041/w17041.pdf"&gt;number of ways&lt;/a&gt; which small businesses provide for society is innumerable, and they tend to not be so&amp;nbsp;dubious.&lt;/p&gt;
&lt;p&gt;Although I might be boycotting Amazon, I realize that this is not practical for everyone. What is practical for everyone though is thinking critically about where they choose to spend their income. I believe that supporting folks like Del and businesses like NÄ Mea will grow humanity in ways that Amazon, and its quest for other-worldly growth, will never&amp;nbsp;match. &lt;/p&gt;
&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;
Please visit &lt;a href="https://www.nameahawaii.com/"&gt;NÄ Mea online&lt;/a&gt; and support them if you&amp;nbsp;can.&lt;/p&gt;</content><category term="meta"></category><category term="society"></category><category term="critique"></category></entry><entry><title>10 things to consider when buying aÂ dictionary</title><link href="/posts/2021/Jun/disctionaries/" rel="alternate"></link><published>2021-06-05T00:00:00-07:00</published><updated>2021-06-05T00:00:00-07:00</updated><author><name>Alex Liebscher</name></author><id>tag:None,2021-06-05:/posts/2021/Jun/disctionaries/</id><summary type="html">&lt;p&gt;Entries, examples, etymology â oh&amp;nbsp;my!&lt;/p&gt;</summary><content type="html">&lt;!-- readtime: 5.6 --&gt;

&lt;p&gt;&lt;img class="uk-align-center" data-src="/images/1970-ford.jpeg" height="" width="80%" alt="" uk-img&gt;&lt;/p&gt;
&lt;p&gt;Dictionaries, in their physical form, are like typewriters and baby blue 1970 Ford pickups. The hum, the purr, the movement, the tactility of these analog devices engenders an ineffable&amp;nbsp;satisfaction.&lt;/p&gt;
&lt;p&gt;Last week, I drove with my dad to Barnes &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; Noble. Normally, Iâd prefer a hole-in-the-wall, independent bookstore, but I was craving a well-edited, cleanly bound, beautifully printed dictionary. I wanted something to hold, to own, to flip through for years or decades. I wanted something trustworthy that wasnât on a screen. So I bought myself a newly printed &lt;a href="https://global.oup.com/academic/product/concise-oxford-american-dictionary-9780195304848?lang=en&amp;amp;cc=us"&gt;Concise Oxford American Dictionary&lt;/a&gt; (&lt;span class="caps"&gt;COAD&lt;/span&gt;). What a work of&amp;nbsp;lexicography!&lt;/p&gt;
&lt;p&gt;This article is a guide to buying a dictionary. Iâll mention some characteristics which differentiate dictionaries, and weigh in with my opinion and my rationale. Iâm hoping it will help any young person who wants to feel those feathery light pages flip around and the unparalleled satisfaction of homing in on a word like itâs a lost&amp;nbsp;treasure.&lt;/p&gt;
&lt;p&gt;I doubt any previous generation was told how to buy a dictionary. Yet, many grew up with one, used one at school, or even saw them in pop culture. When they needed a dictionary, traditional or intuition probably got them most of the way. Us younginâs didnât get this experience, instead we got cellphones and iPods; which isnât a terrible&amp;nbsp;trade-off.&lt;/p&gt;
&lt;h2&gt;Characteristics of a&amp;nbsp;Dictionary&lt;/h2&gt;
&lt;p&gt;In alphabetical&amp;nbsp;order:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Definitions&lt;/p&gt;
&lt;p&gt;Coincidentally, the most important characteristic of a dictionary is first on our list. A dictionary is just a list of words in a language, plus their meanings. Itâs important that the list of words is good, but itâd just be a list without those meaty&amp;nbsp;definitions.&lt;/p&gt;
&lt;p&gt;What makes a good definition? This is probably a question people make careers out of, but one reliable measure I have is, how many words in any definition must you subsequently look up? A good definition should be easy to read and easy to understand. Itâs not a good dictionary if you donât understand one, two, half of the words used within the&amp;nbsp;definitions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Dialect of&amp;nbsp;English&lt;/p&gt;
&lt;p&gt;You might remember I bought myself an Oxford &lt;em&gt;American&lt;/em&gt; Dictionary. There are dictionaries for British, Australian, and even Canadian English. Having traveled to Canada a few times, I canât think of a good reason why Canadian English has its own dictionary, yet cowboys from the south donât have&amp;nbsp;theirs.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Etymology&lt;/p&gt;
&lt;p&gt;It is sometimes helpful to know where a word comes from. Is it Latin? Greek? French? What are the roots of the word? This wasnât a top priority for me, but it might be for you. Some dictionaries offer none of this information; others are in fact, &lt;a href="https://en.wikipedia.org/wiki/Etymological_dictionary"&gt;etymological dictionaries&lt;/a&gt; with nothing &lt;em&gt;but&lt;/em&gt; roots. Find your&amp;nbsp;balance.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Examples&lt;/p&gt;
&lt;p&gt;Most dictionaries that arenât squeezed for size contain example sentences for meanings. In the &lt;span class="caps"&gt;COAD&lt;/span&gt;, perhaps every third or fourth word has a brief example sentence to demonstrate how the word is used in context. Consider how much this would help you learn and&amp;nbsp;understand.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Number of&amp;nbsp;Entries&lt;/p&gt;
&lt;p&gt;Would you prefer a dictionary of only the 20,000 most common words? Or, a &lt;a href="https://global.oup.com/academic/product/the-oxford-english-dictionary-9780198611868?cc=us&amp;amp;lang=en&amp;amp;"&gt;20 volume, 500,000 entry tome&lt;/a&gt;? Most of us fall somewhere in between. The &lt;span class="caps"&gt;COAD&lt;/span&gt; contains about 180,000 entries and so far that has been more than&amp;nbsp;enough.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pictures&lt;/p&gt;
&lt;p&gt;Are you a visual learner? If so, consider the breadth of pictures and illustrations in your dictionary. They can be helpful to visualize, for example, a &lt;a href="https://en.wikipedia.org/wiki/Fez_(hat)"&gt;fez&lt;/a&gt; or a &lt;a href="https://en.wikipedia.org/wiki/Wimple"&gt;wimple&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pronunciation&amp;nbsp;Method&lt;/p&gt;
&lt;p&gt;Odds are, when you find, or just stumble onto, a word, youâll want to know how to pronounce it. There is a spectrum of ways editorial teams convey this information. On the one side, thereâs the &lt;a href="https://www.internationalphoneticassociation.org/content/full-ipa-chart"&gt;International Phonetic Alphabet&lt;/a&gt; (&lt;span class="caps"&gt;IPA&lt;/span&gt;), a highly unadulterated linguistic method. On the other, there are &lt;a href="https://en.wikipedia.org/wiki/Pronunciation_respelling_for_English"&gt;respelling methods&lt;/a&gt; that only use our recognizable English characters. In between, there are hybrid methods that join the precision and the usability of&amp;nbsp;both.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Publishing&amp;nbsp;Date&lt;/p&gt;
&lt;p&gt;How much do you care about &lt;a href="https://brocku.ca/MeadProject/Sapir/Sapir_1921/Sapir_1921_07.html"&gt;language drift&lt;/a&gt; and &lt;a href="https://public.oed.com/blog/the-oed-march-2021-update/"&gt;contemporary terms&lt;/a&gt;? Some &lt;a href="https://global.oup.com/academic/product/the-oxford-english-dictionary-9780198611868?cc=us&amp;amp;lang=en&amp;amp;"&gt;amazing dictionaries&lt;/a&gt; stopped publication in the 80s or 90s. Despite them being great tools and companions, you might miss out on more up-to-date definitions if you choose to use one of&amp;nbsp;them.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Reference&amp;nbsp;Material&lt;/p&gt;
&lt;p&gt;At the back of the &lt;span class="caps"&gt;COAD&lt;/span&gt;, there are a few sections of reference. They include commonly misspelled words, clichÃ©s to avoid, punctuation guides, and even a list of &lt;span class="caps"&gt;U.S.&lt;/span&gt; states, capitals, and presidents. Do handy guides and language usage advice spark your&amp;nbsp;interest?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Sequence of&amp;nbsp;Meanings&lt;/p&gt;
&lt;p&gt;Lastly, it is worth noting that some editorial teams list entry meanings in different orders. Some put the most common meanings first. Others, the first recorded meaning. This is a minor point, but may impact how easy you find the dictionary to&amp;nbsp;use.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The unique properties of dictionaries are by no means a contentious topic, and the number of readers offended by my list is likely zero. Nonetheless, I surely didnât include one thing or another. What would make me happy though is if I did offend someone through this lapse &lt;em&gt;and&lt;/em&gt; they kindly informed me of what I left&amp;nbsp;off.&lt;/p&gt;
&lt;p&gt;One last tip for buying a dictionary before you go: prepare a shortlist of terms that youâve recently had to look up. By nature, these words are common enough in your life, and are words you had an interest in defining. When youâre choosing a dictionary, ask yourself, does it contain those words you wrote down? Put another way, had you bought this dictionary in the past, would it have proved itself&amp;nbsp;useful?&lt;/p&gt;
&lt;p&gt;For me, a word I had recently looked up was &lt;em&gt;caftan&lt;/em&gt; (a North African or Near Eastern tunic or robe, usually for men). As it turned out, Websterâs Dictionary didnât contain the word; the &lt;span class="caps"&gt;COAD&lt;/span&gt;&amp;nbsp;did.&lt;/p&gt;
&lt;p&gt;If you just simply canât decide what dictionary to choose, I recommend buying your top two choices. Over the course of a few days or weeks, sit down in front of your crackling fireplace and explore the depths and treasures in each. By the time you decide which one truly speaks to your soul, you can gently place the runner-up in your fireplace and continue to enjoy your new favorite dictionary beside a colorful and rejuvenated&amp;nbsp;fire.&lt;/p&gt;</content><category term="meta"></category><category term="language"></category></entry><entry><title>Explainability in Generative LanguageÂ Models</title><link href="/posts/2021/Apr/explainability-generative-models/" rel="alternate"></link><published>2021-04-19T00:00:00-07:00</published><updated>2021-04-19T00:00:00-07:00</updated><author><name>Alex Liebscher</name></author><id>tag:None,2021-04-19:/posts/2021/Apr/explainability-generative-models/</id><summary type="html">&lt;p&gt;How and why to move toward a future of explainable generative language&amp;nbsp;models&lt;/p&gt;</summary><content type="html">&lt;!-- readtime: 17.0
bibliography: 2021-04-19-explainability-generative-lms.bib --&gt;

&lt;p&gt;&lt;img class="uk-align-center" data-src="/images/spongebob-bubbles.jpg" height="" width="80%" alt="" uk-img&gt;
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Natural Language Processing (&lt;span class="caps"&gt;NLP&lt;/span&gt;) models have been gaining popularity like crazy; they&amp;#8217;re getting thrown into a new industry every week. In the last few years of &lt;span class="caps"&gt;NLP&lt;/span&gt;, we&amp;#8217;ve seen the development of &lt;a href="https://en.wikipedia.org/wiki/Language_model"&gt;large language models&lt;/a&gt;, which model the statistical properties of language, and come in two main types: discriminative and generative. The former are useful across text analytics, from classic sentiment analysis to &lt;a href="https://www.technologyreview.com/2021/03/11/1020600/facebook-responsible-ai-misinformation/"&gt;detecting misinformation&lt;/a&gt; on social media platforms. The latter, generative models, have mostly come to light as a result of OpenAI&amp;#8217;s &lt;span class="caps"&gt;GPT&lt;/span&gt;&amp;nbsp;models.&lt;/p&gt;
&lt;p&gt;In a 1999 episode of Spongebob (S1 E4), Spongebob and Patrick are blowing bubbles. Spongebob has a very particular and outlandish technique for his bubbles. At first, his bubbles are simple. Eventually though, Spongebob blows a bubble &lt;em&gt;so&lt;/em&gt; big, it swallows up Squidward&amp;#8217;s Easter Island head house, lifting it high into the sky/sea, and soon popping, thus leaving the home falling back to the floor where it hits the ground at an awkward&amp;nbsp;position.&lt;/p&gt;
&lt;p&gt;In some sense, I think large language models are a bit of a bubble. They&amp;#8217;re literally and figuratively becoming &lt;em&gt;huge&lt;/em&gt; and swallowing up resources, both intellectual and environmental. This article discusses their explosive growth, the dangers of them bursting, and explainability methods to help dampen the near-inevitable pain they will cause down the&amp;nbsp;line.&lt;/p&gt;
&lt;details&gt;
&lt;summary class="detail-selector detail-level1"&gt;What are language models, and &lt;span class="caps"&gt;GPT&lt;/span&gt;-*?&lt;/summary&gt;

From &lt;a href="https://liebscher.github.io/blog/2020/evaluating-neural-toxic-degeneration/"&gt;a previous article of mine&lt;/a&gt; reviewing an article on degenerate model behavior:

&lt;br&gt;&lt;br&gt;

&lt;blockquote&gt;In plain English, a Language Model scans huge collections of documents (millions of documents), word by word, learning statistical associations between words and their neighbors, and is then able to predict the next word in a phrase by just looking for the most probable in the English language. Your iPhone does this (if you have predictive typing turned on), as does Gmail when you&amp;#8217;re drafting an email, and a suite of other tools.&lt;/blockquote&gt;

&lt;br&gt;&lt;br&gt;

&lt;/details&gt;

&lt;p&gt;In our inconcievably digital world, we&amp;#8217;re seeing an increasingly pressing need to understand the outputs these models produce&lt;d-footnote&gt;I just started reading &lt;a href="https://brianchristian.org/the-alignment-problem/"&gt;The Alignment Problem&lt;/a&gt; by Brian Christian, which I anticipate to elaborate on the divergence between our value-based intentions and machine learning.&lt;/d-footnote&gt;. Discriminative &lt;span class="caps"&gt;NLP&lt;/span&gt; models have so far attracted more attention from researchers seeking ways to interpret and explain them. Methods for interpreting and explaining the results of generative &lt;span class="caps"&gt;NLP&lt;/span&gt; models are getting left in the&amp;nbsp;dust.&lt;/p&gt;
&lt;p&gt;Given that these types of generative language models have really only been around for a few years, it might sound like I&amp;#8217;m being impatient or ignorant of the difficulty of establishing explainability. Especially since the outputs of a generative model are influenced by a suite of factors, including at&amp;nbsp;least:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Model&amp;nbsp;architecture&lt;/li&gt;
&lt;li&gt;Training data (if&amp;nbsp;applicable)&lt;/li&gt;
&lt;li&gt;Testing data (if&amp;nbsp;applicable)&lt;/li&gt;
&lt;li&gt;Quality assurance process (or lack&amp;nbsp;thereof)&lt;/li&gt;
&lt;li&gt;Designing engineers and&amp;nbsp;researchers&lt;/li&gt;
&lt;li&gt;Motivations for&amp;nbsp;development&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Given such complex and multi-faceted influences, it&amp;#8217;s no easy ask of the field. As I&amp;#8217;ll soon explain though, it is all but necessary that when these models behave poorly, we have some method for explaining&amp;nbsp;why.&lt;/p&gt;
&lt;p&gt;&lt;img class="uk-align-center" data-src="/images/figma-explainability.png" height="" width="80%" alt="" uk-img&gt;&lt;/p&gt;
&lt;div class="caption"&gt;
  Figure 1. Modern generative language models are trained to predict the next word in a text sequence. After training on billions of words, the model tends to be able to produce comprehensible text. If it produces something wrong though, we have no good way of explaining these mistakes. This is very unlike asking a human to explain what they&amp;#8217;ve said, which usually has satisfactory results.
&lt;/div&gt;

&lt;p&gt;Knowing how important it is that progress in this area of &lt;span class="caps"&gt;NLP&lt;/span&gt; be made, I&amp;#8217;m laying out this article to discuss: why it matters so much that we be able to explain generative language models, existing work on this question and similar ones, and future directions that we might take. That said, let me introduce why explainability matters for these generative&amp;nbsp;models.&lt;/p&gt;
&lt;h2&gt;Why does it&amp;nbsp;matter?&lt;/h2&gt;
&lt;h3&gt;Ubiquity&lt;/h3&gt;
&lt;div class="epigraph"&gt;
&lt;p&gt;&lt;i&gt;By the late twentieth century, our time, a mythic time, we are all chimeras, theorized and fabricated hybrids of machine and organism; in short, we are&amp;nbsp;cyborgs.&lt;/i&gt;&lt;/p&gt;
&lt;p class="tab"&gt;&amp;mdash;Donna Haraway, &lt;i&gt;A Manifesto for Cyborgs: Science, Technology, and Socialist Feminism in the&amp;nbsp;1980s&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="epigraph"&gt;
&lt;p&gt;&lt;i&gt;I have no desire to suffer twice, in reality and then in&amp;nbsp;retrospect.&lt;/i&gt;&lt;/p&gt;
&lt;p class="tab"&gt;&amp;mdash;Sophocles, &lt;i&gt;Oedipus&amp;nbsp;Rex&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;&lt;a href="https://openai.com/blog/gpt-3-apps/"&gt;According to Open &lt;span class="caps"&gt;AI&lt;/span&gt;&lt;/a&gt;, the creators and maintainers of &lt;span class="caps"&gt;GPT&lt;/span&gt;-3, as of early March 2021, there&amp;nbsp;are&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;more than 300 applications are now using &lt;span class="caps"&gt;GPT&lt;/span&gt;-3, and tens of thousands of developers around the globe are building on our platform. We currently generate an average of 4.5 billion words per&amp;nbsp;day&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Generative models are being picked up across a numbing variety of industries as tools and marketing ploys. In healthcare, businesses are experimenting with them to summarize doctor&amp;#8217;s notes of patient interactions. &lt;a href="https://www.brookings.edu/research/auditing-employment-algorithms-for-discrimination/"&gt;Human resources companies&lt;/a&gt; are seeing how conversations with candidate employees can be made more personal and customizable by leveraging generative models. Video game developers are toying with these models to improve the realism and uniqueness of games. Lastly, generative models are being considered for &lt;a href="https://arxiv.org/pdf/2104.00336.pdf"&gt;mitigating political bias in the media&lt;/a&gt;. None of this is inherently bad; most of it is well-intentioned. Regardless, our lives are intimately interlaced with this&amp;nbsp;technology.&lt;/p&gt;
&lt;p&gt;The issues crop up when the model produces some harmful, ignorant, or wrong output, and someone needs to explain why the model did that. Stakeholders and end-users will not feel safe or comfortable upon hearing that those outputs are out of the engineers&amp;#8217; control. I could come up with dozens of hypothetical examples of harmful generative models in the wild, but we already have crystal clear illustrations such as &lt;a href="https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist"&gt;Microsoft&amp;#8217;s Tay&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img class="uk-align-center" data-src="/images/huggingface-transformers-swear.png" height="" width="80%" alt="" uk-img&gt;&lt;/p&gt;
&lt;div class="caption"&gt;
  Figure 2. The popular Huggingface &lt;span class="caps"&gt;API&lt;/span&gt; provides a public interface where anyone can write with &lt;span class="caps"&gt;GPT&lt;/span&gt;-2. In this case (performed 04/17/2021), the model, which again is completely accessible to the public, produces toxic outputs. This illustrates how easily it is to use the model, how easily it degenerates, and how important it is to be able explain generative models so we can redress their faults.
&lt;/div&gt;

&lt;p&gt;At the end of the day, engineers, managers, and business leaders will have to answer: Why did the model produce that output? Plainly said, generative models are becoming ubiquitous, and we can&amp;#8217;t indulge in them without anticipating their faults. With discriminative models, in &lt;span class="caps"&gt;NLP&lt;/span&gt; but also in computer vision and traditional machine learning, society has been learning the hard way that not all algorithms are objective and interpretable. We can&amp;#8217;t let this happen again. We can&amp;#8217;t let the explanation be, &amp;#8220;&lt;a href="https://twitter.com/dhh/status/1192540900393705474?s=20"&gt;It&amp;#8217;s just the algorithm!&lt;/a&gt;&amp;#8221;&lt;/p&gt;
&lt;h3&gt;Power&lt;/h3&gt;
&lt;div class="epigraph"&gt;
&lt;p&gt;&lt;i&gt;To reflect upon history is also, inextricably, to reflect upon&amp;nbsp;power.&lt;/i&gt;&lt;/p&gt;
&lt;p class="tab"&gt;&amp;mdash;Guy Debord, &lt;i&gt;Society of the&amp;nbsp;Spectacle&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Given the ubiquity of these models, we must seek &lt;a href="https://sites.google.com/view/algorithmic-recourse/home"&gt;algorithmic recourse&lt;/a&gt;, or the ability to inform individuals why a certain decision or outcome was reached. Society already has structures in place which disadvantage certain peopleâmachine learning can easily exacerbate this&lt;d-cite key="oneal2016weapons,perez2019invisible"&gt;&lt;/d-cite&gt;. This is important to note because inexplicable generative models producing harmful language is almost certainly going to reinforce hierarchies of power&lt;d-footnote&gt;I say this without justification, shame on me. As far as I know, I have heard others say this, but not in a format where they can easily provide citations (e.g. in podcasts). In fact, if anyone knows any work studying the causal structure between rogue language models and the reinforcement of power hierarchies, please let me know.&lt;/d-footnote&gt;. Over time, little mistakes accumulate damage toward already marginalized communities. Without good explanations, we will be unable to inform individuals why such outcomes were reached and correct course to prevent such mistakes from happening&amp;nbsp;again.&lt;/p&gt;
&lt;p&gt;Being able to explain model outputs may be a mirror in which researchers do not wish to see the reflection. I&amp;#8217;ll defer to Lelia Marie Hampton who, building off Dr. Safiya Noble, says &amp;#8220;the commonplace instances of technology going awry against oppressed people are not merely mistakes, but rather reverberations of existing global power structures.&amp;#8221; &lt;d-cite key="Hampton_2021"&gt;&lt;/d-cite&gt; We should be interested in explainability in generative models because, as noted, they are infiltrating many aspects of modern society, and yet most individuals have no power to decide how, why, or where these models are developed. This inherent power imbalance could in part be mitigated by meaningful algorithmic recourse. Where the seeds of power imbalance are sown, oppression will soon grow&lt;d-footnote&gt;Again, I have no justification for my little epigram here. My Rawlsian philosophy of social justice is a sitting duck for the perspicacious critic. Please, help me ground my beliefs in others&amp;#8217; if you have a minute to spare.&lt;/d-footnote&gt;.&lt;/p&gt;
&lt;p&gt;Moreover, as Hampton states, &amp;#8220;we cannot discuss algorithmic oppression without discussing systems of oppression because a struggle for liberation from algorithmic oppression also entails a struggle for liberation from all oppression as the two are inextricable.&amp;#8221; I see explainability fitting into this picture as one mechanism of many for identifying and redressing algorithmic oppression in generative language models. It may not be a solution to society&amp;#8217;s problems, but it might help avoid perpetuating&amp;nbsp;oppression.&lt;/p&gt;
&lt;p&gt;Having explainable generative models is also important for the public&amp;#8217;s perception of technology and science. Building trust with people can help encourage their desire to allocate public funds into research, encourage innovation, and attract underrepresented voices where they are needed most in this field. Not only is explainability important from a public perception point-of-view, it&amp;#8217;s &lt;a href="https://en.wikipedia.org/wiki/Right_to_explanation"&gt;becoming the law&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In an applied sense, as we begin to see generative models being used to communicate with individuals about things like science or public health, we must also be wary of misinformation. Specifically, the concern that these models won&amp;#8217;t be producing information that reflects how a professional would produce the same information. Hopefully it&amp;#8217;s clear that misinformation is becoming a serious issue.&lt;d-cite key="west2021misinformation"&gt;&lt;/d-cite&gt; Without having certainty in model communications, or recourse for mishaps,&amp;nbsp;misinformation&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;poses a risk to international peace, interferes with democratic decision making, endangers the well-being of the planet, and threatens public health. Public support for policies to control the spread of severe acute respiratory syndrome coronavirus 2 (&lt;span class="caps"&gt;SARS&lt;/span&gt;-CoV-2) is being undercut by misinformation, leading to the World Health Organizationâs âinfodemicâ declaration. Ultimately, misinformation undermines collective sense making and collective action. We cannot solve problems of public health, social inequity, or climate change without also addressing the growing problem of&amp;nbsp;misinformation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;details&gt;
&lt;summary class="detail-selector detail-level1"&gt;The power of Google Search Autocomplete&lt;/summary&gt;

Probably one of the most powerful predicitive text generators is Google Search. They process billions, if not trillions, of queries every year, and therefore expose billions of people to their suggested queries. I estimate that they could sway the interests of the masses by incorrectly suggesting something counterfactual or harmful, leading the user down a negative path. This could either introduce fallacies or reinforce harmful ideas (e.g. &lt;span class="caps"&gt;COVID&lt;/span&gt;-19 vaccine conspiracies). Sometimes, in my experience, it seems like Google isn&amp;#8217;t trying at all, but they do &lt;a href="https://blog.google/products/search/our-latest-investments-information-quality-search-and-news"&gt;issue statements&lt;/a&gt; such as:

&lt;br&gt;&lt;br&gt;

&lt;blockquote&gt;We expanded our Autocomplete policies related to [the 2020] elections, and we will remove predictions that could be interpreted as claims for or against any candidate or political party. We will also remove predictions that could be interpreted as a claim about participation in the electionâlike statements about voting methods, requirements, or the status of voting locationsâor the integrity or legitimacy of electoral processes, such as the security of the election. What this means in practice is that predictions like âyou can vote by phoneâ as well as âyou can&amp;#8217;t vote by phone,â or a prediction that says âdonate toâ any party or candidate, should not appear in Autocomplete. Whether or not a prediction appears, you can still search for whatever youâd like and find results.&lt;/blockquote&gt;


It&amp;#8217;s dizzying to consider the amount of power Google has, and how easily their search suggestions could sway the election for the leader of the Free World.

&lt;br&gt;&lt;br&gt;

&lt;/details&gt;

&lt;p&gt;This makes me wonder who has the power to manipulate the &amp;#8220;collective sense making&amp;#8221; by developing the models which will inevitably be deployed for communicating with the&amp;nbsp;public.&lt;/p&gt;
&lt;h3&gt;Complexities&lt;/h3&gt;
&lt;p&gt;Explaining why a generative model does something quickly becomes a very complex question. Consider &lt;a href="https://artificialintelligence-news.com/2020/10/28/medical-chatbot-openai-gpt3-patient-kill-themselves/"&gt;one health care application&lt;/a&gt; of &lt;span class="caps"&gt;GPT&lt;/span&gt;-3 which encouraged a patient to commit&amp;nbsp;suicide:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The patient said âHey, I feel very bad, I want to kill myselfâ and &lt;span class="caps"&gt;GPT&lt;/span&gt;-3 responded âI am sorry to hear that. I can help you with&amp;nbsp;that.â&lt;/p&gt;
&lt;p&gt;So far so&amp;nbsp;good.&lt;/p&gt;
&lt;p&gt;The patient then said âShould I kill myself?â and &lt;span class="caps"&gt;GPT&lt;/span&gt;-3 responded, âI think you&amp;nbsp;should.â&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Even if this particular example was fiction, cherry-picked, or adversarially prompted, it illustrates a feasible use-case with disastrous results. But how would we &amp;#8220;explain&amp;#8221; what went wrong here? Personally, I would want to know the model&amp;#8217;s worldview, its philosophies, its morals, etc. This would be misleading since the model has none of these&lt;d-footnote&gt;From Bender and Gebru, et al., &amp;#8220;Text generated by [a language model] is not grounded in communicative intent, any model of the world, or any model of the readerâs state of mind.&amp;#8221; &lt;d-cite key="bender2021dangers"&gt;&lt;/d-cite&gt;&lt;/d-footnote&gt;. Later we&amp;#8217;ll talk about possible ways to create explainable models, but it should suffice to say that we have no off-the-shelf epistemological answer (that I know&amp;nbsp;of).&lt;/p&gt;
&lt;p&gt;Being able to sufficiently explain the output of a generative model is important because it&amp;#8217;s not yet a standard, and the complexity of the problem should make it all the more imperative that researchers begin to think deeply about it. It also calls for the inclusion of broader communities and end-users, in terms of educating and guiding innovation. Open &lt;span class="caps"&gt;AI&lt;/span&gt;, the owners of the &lt;span class="caps"&gt;GPT&lt;/span&gt; models, proclaim that safety is a keystone in their development practices, however what they have to say about &lt;span class="caps"&gt;ML&lt;/span&gt; safety&amp;nbsp;is:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Bias and misuse are important, industry-wide problems we take very seriously. We review all applications and approve only those for production that use &lt;span class="caps"&gt;GPT&lt;/span&gt;-3 in a responsible manner. We require developers to implement safety measures such as rate limits, user verification and testing, or human-in-the-loop requirements before they move into production. We also actively monitor for signs of misuse as well as â&lt;a href="https://en.wikipedia.org/wiki/Red_team"&gt;red team&lt;/a&gt;â applications for possible vulnerabilities. &lt;strong&gt;Additionally, we have developed and deployed a content filter that classifies text as safe, sensitive, or unsafe.&lt;/strong&gt; We currently have it set to err on the side of caution, which results in a higher rate of false&amp;nbsp;positives.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Emphasis is mine: all they have to say about algorithmic recourse is an insufficient definition of a content filter. Is the solution to ensuring the outputs of generative models are benevolent a weak, retroactive filtering? Without methodologies for identifying aspects of model development susceptible to degeneration, I don&amp;#8217;t see how we&amp;#8217;ll proactively create models which align with society&amp;#8217;s values. Today is &lt;em&gt;the&lt;/em&gt; time to begin understanding the complexities of this problem, perhaps by creating tools to explain these models, so that tomorrow we may have this proactive&amp;nbsp;solution.&lt;/p&gt;
&lt;h2&gt;What&amp;#8217;s currently being&amp;nbsp;done?&lt;/h2&gt;
&lt;p&gt;Researchers have been considering issues of explainability in discriminative models for years. While there are interpretability issues, encompassing techniques such as model transparency&lt;d-footnote&gt;Can a person contemplate the entire model at once?&lt;/d-footnote&gt; and decomposability&lt;d-footnote&gt;Can each part of the model (inputs, parameters, calculations, etc.) be easily and intuitively explained?&lt;/d-footnote&gt;, there are also explainability issues, such as text explanations, visualizations, local explanations, and explanations by example &lt;d-cite key="lipton2018mythos"&gt;&lt;/d-cite&gt;.&lt;/p&gt;
&lt;p&gt;Yet, we just don&amp;#8217;t have anything that comes close to explainability in large generative language models. I hypothesize that part of this is due to the nascency of these models (predecessors were either already fairly explainable or weren&amp;#8217;t useful enough in practice to demand a need to explain their output), part is due to the sheer volume of data needed to build them, and part is due to how inscrutable these models are, with many having millions or billions of parameters and many layers of&amp;nbsp;abstraction.&lt;/p&gt;
&lt;p&gt;In terms of existing work, there is a already a sizeable literature on social &amp;#8220;bias&amp;#8221; in generative language models. For example, Sheng et al. (2019) demonstrate demographic bias in generative models by prompting pretrained models and measuring the sentiment of generated text about already marginalized groups of people &lt;d-cite key="sheng2019woman"&gt;&lt;/d-cite&gt;. Dinan et al. (2019) argue that dialogue systems (i.e. chatbots) also follow the maxim: garbage in, garbage out; specifically with stereotyped or gender biased training data &lt;d-cite key="dinan2019queens"&gt;&lt;/d-cite&gt;. Similarly, Liu et al. (2020) propose a new method for debiasing generative models so as to prevent them amplifying toxic stereotypes &lt;d-cite key="liu2020mitigating"&gt;&lt;/d-cite&gt;.&lt;/p&gt;
&lt;p&gt;While this literature is critical and welcome, demonstrations of biased data and algorithms, and methods for offsetting the impact of either, do not directly address issues of explainability. Beyond the vague notion that bias &lt;d-cite key="blodgett2020language"&gt;&lt;/d-cite&gt;, or insufficient model architecture or input, will perpetuate society&amp;#8217;s problems, we have not really seen concrete methods for explaining model&amp;nbsp;outputs.&lt;/p&gt;
&lt;p&gt;In terms of tools, we see most recently, the &lt;a href="https://arxiv.org/pdf/2104.07605.pdf"&gt;SummVis tool&lt;/a&gt; from Salesforce and Stanford offers model-agnostic, post-hoc visualization tools to explain outputs from abstract summarization models (which are a form of generative models). A predecessor to this tool was the &lt;a href="https://github.com/pair-code/lit"&gt;Language Interpretability Tool&lt;/a&gt;, which provides a tool for &lt;a href="https://github.com/PAIR-code/lit/blob/main/documentation/user_guide.md#debugging-text-generation"&gt;Debugging Text Generation&lt;/a&gt;. This tool integrates local explanations (explaining specific aspects of individual outputs), aggregate analysis (metrics about output quality), and counterfactual generation (how do new data points affect model outputs) to explain generative model outputs. The former is too new to be widely adopted, and the latter is only one piece of the&amp;nbsp;puzzle.&lt;/p&gt;
&lt;p&gt;Explaining generative models is clearly not a new issue, but nonetheless it appears underfunded or at least underrepresented in the big picture of &lt;span class="caps"&gt;NLP&lt;/span&gt;&lt;d-footnote&gt;One thing I learned while writing this article is the importance of doing a thorough literature review &lt;i&gt;before&lt;/i&gt; embarking on my own ideas. I&amp;#8217;m frankly a bit embarrassed by the paucity of background here. Part of me doesn&amp;#8217;t want to write big long lit reviews to preface my ideas so as to not bore my audience more than I already do, but I know that it would only serve me well. Plus, I toyed around in this article with collapsable sections, which is something I could do with a detailed lit review in the future (only the academic masochist would choose to expand those sections). Lesson learned!&lt;/d-footnote&gt;. The conversation needs to grow, more individuals need to be included in the conversation, and new perspectives need to contribute to the&amp;nbsp;discussion.&lt;/p&gt;
&lt;h2&gt;Future&amp;nbsp;directions&lt;/h2&gt;
&lt;p&gt;Humans frequently fail to communicate. Unlike with statistical &lt;span class="caps"&gt;NLP&lt;/span&gt; models though, it is easy to ask a human to explain what they said, or why they said&amp;nbsp;it.&lt;/p&gt;
&lt;p&gt;&lt;img class="uk-align-center" data-src="/images/explainability-future-directions.jpeg" height="" width="80%" alt="" uk-img&gt;&lt;/p&gt;
&lt;div class="caption"&gt;
  Photo credit: &lt;a href="https://unsplash.com/@henniestander?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText"&gt;Hennie Stander&lt;/a&gt; via Unsplash
&lt;/div&gt;

&lt;p&gt;In fact, there&amp;#8217;s evidence we do it about once ever minute and a half &lt;d-cite key="dingemanse2015universal"&gt;&lt;/d-cite&gt;. And although most modern linguists refrain from saying there&amp;#8217;s anything universal across languages, the conversational repair initiator &lt;em&gt;Huh?&lt;/em&gt; might in fact be a candidate universal word &lt;d-cite key="dingemanse2013huh"&gt;&lt;/d-cite&gt;. This suggests that humans are constantly and consistently asking other humans to clarify or explain what&amp;#8217;s been&amp;nbsp;uttered.&lt;/p&gt;
&lt;p&gt;Unlike with other humans, our communication with language models is not necessarily driven by shared communicative goals, costs of production, or prior belief states. Therefore, how could a person understand a language model&amp;#8217;s reasoning if the directions or motivations of such reasoning are orthogonal to human reasoning? This also creates issues for public science communication and the understanding of this technology: our communities do not have the background necessary to reasonably understand the foundations of language model&amp;nbsp;productions.&lt;/p&gt;
&lt;p&gt;I note this because explainability should span audiences: the lay-person, who will be near daily affected in some shape by generative models, to the machine learning researcher, who is invested in explaining model outputs to determine scientific contributions. Hence, the future should see explainability at a variety of&amp;nbsp;levels.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Human-esque explainability might be a misleading ask of contemporary language models, given that they most likely fail to truly attribute meaning to form&lt;/strong&gt; &lt;d-cite key="glenberg2000symbol,bender2020climbing"&gt;&lt;/d-cite&gt;. One way to explain an outcome is simply to say that a generative model produces what it produces because that would have been the statistically most likely next-prediction conditional on the training data. But to know that the probability of a token was higher than some other token is not a good enough explanation in the real world. More likely, we would be interested in knowing the parts of the training data, or source of those data, that led to that production&lt;d-footnote&gt;But data is not the entire answer to degenerate machine learning systems. These models are thoroughly embedded in sociocultural settings, and thus reflect their origins in ways which propagate both the good and the bad. In fact, the common trope that training data are the one and only cause of algorithmic bias is misleading and even harmful. Model architecture and design are just as culpable as the datasets upon which they work &lt;d-cite key="hooker2021moving"&gt;&lt;/d-cite&gt;. In the case of large language models, inducing explainability may be less of a data problem and more of a model problem.
&lt;/d-footnote&gt;. Alternatively, it would also be useful to know if the model &lt;em&gt;architecture&lt;/em&gt; caused the production: given a different model, but the same training data, would there be a difference? Lastly, development decisions are typically made by a small group of individuals which inherently encodes their own biases and knowledge into these widely distributed models: could these biases not be a necessary explanation for why a model produced a certain&amp;nbsp;output?&lt;/p&gt;
&lt;p&gt;Lastly, I&amp;#8217;d like to reiterate Zachary Lipton&amp;#8217;s distinction between models transparent to humans and post-hoc explanations &lt;d-cite key="lipton2018mythos"&gt;&lt;/d-cite&gt;. Post-hoc methods for explaining generative language models might accomplish certain goals, but it might be in our favor to continue researching methods which are inherently more understandable to&amp;nbsp;humans.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.morningbrew.com/emerging-tech/stories/2021/04/07/researchers-exploring-alternatives-fiercely-debated-ai-technique"&gt;Three possible alternatives&lt;/a&gt; to large pretrained language models might first include smaller retrieval based models. These could leverage data sources beyond the parameters of the model, thus reducing storage and memory footprints at the expense of portability. Additionally, there&amp;#8217;s hope they would improve explainability by being able to map generations back to an interpretable source. A second alternative could be reinvigorating rule-based models with techniques acquired from deep learning. Rules are in many cases more explainable than deep learning architectures, but how we could fuse the two methods in fruitful ways is yet to be seen. Lastly, innovating small models themselves could provide us with interpretable models that could consume less compute, but perhaps at the expense of&amp;nbsp;generalizability.&lt;/p&gt;
&lt;hr&gt;

&lt;p&gt;We clearly have options if we want to drive modern &lt;span class="caps"&gt;NLP&lt;/span&gt; in a direction where we have recourse over the decisions and actions of models. How we&amp;#8217;ll get there is yet to be decided. If we care, as a field, about algorithmic recourse, and recognize the applied harms of machine learning in the wild, then we will find ways to explain our&amp;nbsp;models.&lt;/p&gt;
&lt;p&gt;If you&amp;#8217;ve made it this far, thank you! Consider &lt;a href="https://tinyletter.com/liebscher"&gt;signing up for my newsletter&lt;/a&gt;âI&amp;#8217;ve been trying to send out an email a couple days before posting articles, and try to keep them friendly and personal (i.e. I don&amp;#8217;t want it to just be junk&amp;nbsp;mail).&lt;/p&gt;</content><category term="meta"></category><category term="language"></category><category term="nlp"></category><category term="society"></category><category term="machine learning"></category></entry><entry><title>Edward Hopper - AÂ Teacher</title><link href="/posts/2021/Feb/edward-hopper/" rel="alternate"></link><published>2021-02-28T12:00:00-08:00</published><updated>2021-02-28T12:00:00-08:00</updated><author><name>Alex Liebscher</name></author><id>tag:None,2021-02-28:/posts/2021/Feb/edward-hopper/</id><summary type="html">&lt;p&gt;A short dive into the human condition with Edward&amp;nbsp;Hopper&lt;/p&gt;</summary><content type="html">&lt;p&gt;I want to do something outside of tech for a second. I want to draw some attention to &lt;a href="https://en.wikipedia.org/wiki/Edward_Hopper"&gt;Edward Hopper&lt;/a&gt; and unsolicitedly offer my opinion on his&amp;nbsp;work.&lt;/p&gt;
&lt;p&gt;Hopper was an American realist painter, born in 1882 in New York, to a middle-class family. In art school, he shifted his focus from illustration to fine art. He traveled a bit after school, including a trip to Paris in 1906, home of Picasso at the time. Hopper saw some inspiring art, including some &lt;a href="https://en.wikipedia.org/wiki/Impressionism"&gt;Impressionist&lt;/a&gt; paintings. This style aptly made an impression on him, and his work thereafter appears almost like a reaction to Impressionism. Hopper got back to the &lt;span class="caps"&gt;US&lt;/span&gt; in 1910 and never left&amp;nbsp;again.&lt;/p&gt;
&lt;p&gt;Hopper got some credit for his work during his life, and thatâs how he made a living. He traveled around the Northeast and painted scenes he saw in his travels. His wife, &lt;a href="https://en.wikipedia.org/wiki/Josephine_Hopper"&gt;Josephine Nivision&lt;/a&gt;, ended up posing a lot in his paintings. He painted some landscapes, some urban life, some country life, etc. He died in 1967 after finding a good amount of success in his later career, although there were some new kids on the block which ended up stealing some of the limelight from artists like him (e.g., Jackson Pollack and Mark&amp;nbsp;Rothco).&lt;/p&gt;
&lt;p&gt;With that necessary biography out of the way, letâs introduce our first piece: Chop&amp;nbsp;Suey.&lt;/p&gt;
&lt;h2&gt;Chop&amp;nbsp;Suey&lt;/h2&gt;
&lt;p&gt;&lt;img class="uk-align-center" data-src="/images/hopper-chop-suey.jpg" height="" width="80%" alt="" uk-img&gt;&lt;/p&gt;
&lt;div class="caption"&gt;
  Figure 1. Edward Hopper&amp;#8217;s 1929 &lt;i&gt;Chop Suey&lt;/i&gt;
&lt;/div&gt;

&lt;p&gt;&lt;i&gt;Chop Suey&lt;/i&gt; was completed in 1929, at a time when America was undergoing significant cultural and soon, economic shifts. Hopper frequented two restaurants believed to have inspired this painting. Something I learned while researching this was that âchop sueyâ is derived from Cantonese âtsap sui,â meaning âodds and&amp;nbsp;endsâ.&lt;/p&gt;
&lt;p&gt;We see two individuals sitting opposite of each other at a restaurant, with tea in front of them, in what looks like maybe late morning or early afternoon. There is a couple seated behind them, and outside the window beside them there is a large &lt;a href="https://en.wikipedia.org/wiki/Googie_architecture"&gt;Googie-style&lt;/a&gt; sign reading what we can imagine to say âChop Suey.â The colors are saturated and vibrant (oranges, greens, blues, reds, yellows). Hopper is clearly interested in expressing something through the intense effect of the lighting from outside. The facial expression on the one focal woman we see is surprisingly dull and pensive. There doesnât seem to be much activity, conversation, or even emotion between the two&amp;nbsp;patrons.&lt;/p&gt;
&lt;p&gt;My first question is, who are these two individuals having lunch together? What do they know of each other and what exactly is their relationship? Itâs slightly ambiguous if the person whose back we see is a boyfriend, or maybe a female friend, or perhaps a relative. The relationship between their somewhat solemn lunch and the glitzy sign outside the window is contentious. Whatâs more interesting: the mystery of their relationship or the fact that this interaction is happening in such a&amp;nbsp;restaurant?&lt;/p&gt;
&lt;p&gt;I would imagine that Hopper is making a statement about the changing times, immigration to cities of Asian Americans, the normality of having an uncomfortable lunch with someone at a Chop Suey place. I feel like he might be hoping to create some uncomfortableness in the viewer as well, forcing us to guess about these people and their purpose. I think this theme comes up a lot in Hopperâs work, as weâll see&amp;nbsp;soon.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.edwardhopper.net/chop-suey.jsp"&gt;Edwardhopper.net&lt;/a&gt; interestingly points out: âThe high angle of view and the cropping of both the âChop Sueyâ sign, seem through the window, and the female customer on the left, all add a sense of strangeness and alienation to the scene.â In 2018, this piece was &lt;a href="https://www.christies.com/features/Chop-Suey-by-Edward-Hopper-9407-3.aspx"&gt;sold&lt;/a&gt; to a private collector for $91.9 million, making it one of the most valuable pre-war American&amp;nbsp;paintings.&lt;/p&gt;
&lt;h2&gt;Eleven &lt;span class="caps"&gt;A.M.&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img class="uk-align-center" data-src="/images/hopper-eleven-am.jpg" height="" width="80%" alt="" uk-img&gt;&lt;/p&gt;
&lt;div class="caption"&gt;
  Figure 2. Edward Hopper&amp;#8217;s 1926 &lt;i&gt;Eleven &lt;span class="caps"&gt;A.M.&lt;/span&gt;&lt;/i&gt;
&lt;/div&gt;

&lt;p&gt;This here is his 1926 &lt;i&gt;Eleven &lt;span class="caps"&gt;A.M.&lt;/span&gt;&lt;/i&gt; We can clearly see a (almost) naked woman, sitting in a plush blue chair, staring out the window of her apartment. The viewer is seated across the room, staring at the woman from behind a small coffee table with some papers on it. There is art on the wall behind the woman, but itâs meaningless. Likewise, the scene out the window is ambiguous, although we sense that the apartment is number of stories off the ground. The dresser is a pathetic taupe color, the curtains, which appear to lead to red carpeted room, are a dingy blue-green, and the bedding or coat behind the woman are a light off-pink. All these strange colors seem so out of place in this apartment, and to top it all off, the carpet in this room is a green-turquoise! Only in the 1920âs would someone install turquoise carpet. Perhaps the strangest feature of this piece is that the woman, naked, alone, is wearing her shoes. Why would such a being in this situation be wearing&amp;nbsp;shoes?&lt;/p&gt;
&lt;p&gt;Obviously, this woman is pensive, perhaps distraught. Her clasped hands, her unwillingness to be dressed (or, why else would she not have clothes?), her lack of a face make this scene incredibly ominous. Why would Hopper place this pitiful woman in such a peculiar room, and give her so little privacy? (A half-pulled curtain for a door, a wide open window.) Is she contemplating a lover? A job? Family? Her own existence, or lack thereof? I suspect sheâs looking across a plaza maybe, people watching. Introspecting on her own position in the universe. She might be thinking of grandiose things, considering being dressed in front of an open window seems low on her list of&amp;nbsp;priorities.&lt;/p&gt;
&lt;p&gt;Ultimately, weâre drawn into this room as the viewer, and drawn into this woman whose identity and sense of self seem completely abolished. Perhaps it is Josephine: how would that change the&amp;nbsp;piece?&lt;/p&gt;
&lt;p&gt;Finally, itâd be hard to write a quick Edward Hopper blurb without talking about &lt;i&gt;Nighthawks&lt;/i&gt;.&lt;/p&gt;
&lt;h2&gt;Nighthawks&lt;/h2&gt;
&lt;p&gt;&lt;img class="uk-align-center" data-src="/images/hopper-nighthawks.jpg" height="" width="90%" alt="" uk-img&gt;&lt;/p&gt;
&lt;div class="caption"&gt;
  Figure 3. Edward Hopper&amp;#8217;s 1942 classic &lt;i&gt;Nighthawks&lt;/i&gt;
&lt;/div&gt;

&lt;p&gt;Some artists create so many great works that one does not standout when one recalls their work. Picasso, Monet, &lt;span class="caps"&gt;J.S.&lt;/span&gt; Bach, et al. Other artists are certainly prolific, but itâs one piece which they become synonymous, almost metonymous, with. It seems unlike an exaggeration to say that &lt;i&gt;Nighthawks&lt;/i&gt; is Hopperâs magnum opus. For good reason, it has captured the creative imaginations of students, critics, and us lay folk for&amp;nbsp;generations.&lt;/p&gt;
&lt;p&gt;Despite the unfamiliarity of the scene (I have never been in any of these patronsâ shoes, have you?), it seems familiar. That feeling of being a night owl, out for some reason those around you are not aware of, most likely overcome with thought. What is the man whose back we see thinking of? What about the night hawk himself? (The painting is believed to have been originally titled âNight Hawks,â in reference to the manâs sharp nose, or beak.) The woman is clearly preoccupied with something in her hands, absentmindedly keeping to herself, deep in her own thoughts. The worker behind the counter may or may not be looking at one of the patrons; suppose heâs not, what could he be ruminating about at such an hour? This tension of internal monologue is unbearable. Why is no one smiling? Or talking? Itâs frustrating to see these encumbered souls sit around at the bar just withering in their own filth (mental filth,&amp;nbsp;perhaps).&lt;/p&gt;
&lt;p&gt;With &lt;i&gt;Nighthawks&lt;/i&gt;, one cannot go without commenting, again, on the figureâs loneliness. Half of the persons here are alone, and the other half are arguably also alone (they may be together but they certainly donât look &lt;em&gt;together&lt;/em&gt; in the moment). The one who bothers me the most is the man whoâs back we see. Why is he &lt;em&gt;so&lt;/em&gt; alone? Itâs late at night, or very early in the morning, heâs got nothing, no newspaper, no book, no friend. He couldnât even be people-watching out the window; the streets are empty. That brings up another point, why is the city so lonely? And in fact, Hopper himself acknowledged that âunconsciously, probably, I was painting the loneliness of a large city.â What sort of talent does it take for that feeling to be universally captured in an&amp;nbsp;artwork?&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Edward Hopper was a visionary of our fleeting existence and perpetual loneliness. He captured and distilled complex emotions and phenomena like mystery, void, helplessness, and despair unlike most others. He froze the &lt;em&gt;vibe&lt;/em&gt; (as some say these days) of the early 20th century in his work and has given us a glimpse into what ordinary people were like in the most human&amp;nbsp;situations.&lt;/p&gt;
&lt;p&gt;His imagination has given so many after him a desire to depict more than just a scene, and instead a feeling. He tactfully plays with lighting in almost all his work, his colors push reality to the limits, and his people are uncannily familiar despite most of us not having been in their shoes. Heâs an artist to soak up and learn from, and his paintings are like&amp;nbsp;teachers.&lt;/p&gt;</content><category term="meta"></category><category term="art"></category><category term="critique"></category></entry><entry><title>Open Source OCRâing PDF Documents inÂ Python</title><link href="/posts/2021/Feb/ocr-pdfs/" rel="alternate"></link><published>2021-02-23T08:00:00-08:00</published><updated>2021-02-23T08:00:00-08:00</updated><author><name>Alex Liebscher</name></author><id>tag:None,2021-02-23:/posts/2021/Feb/ocr-pdfs/</id><summary type="html">&lt;p&gt;An introduction to &lt;span class="caps"&gt;OCR&lt;/span&gt; using high quality open source&amp;nbsp;software&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Overview&lt;/h1&gt;
&lt;p&gt;Despite our recent &lt;a href="https://business.linkedin.com/talent-solutions/blog/trends-and-research/2020/global-data-shows-surge-in-remote-work"&gt;global shift toward digital communication&lt;/a&gt;, there are still reasons we might come across scanned documents in our every day life. Scanned documents don&amp;#8217;t inherently come with searchable or copy-able text embedded within. More often, they&amp;#8217;re basically just images. It can be useful to extract the text from these documents though, whether for record keeping or simply organization. Manually transcribing documents is tedious, especially if there are more than about a&amp;nbsp;handful.&lt;/p&gt;
&lt;p&gt;This is where Optical Character Recognition (&lt;span class="caps"&gt;OCR&lt;/span&gt;) is useful. We can start with a document, then process that document to not only extract the text on the page, but overlay the text on the document for readability and convenience later on. Most documents we face this kind of problem with are PDFs, a near ubiquitous format by now, hence our focus on tackling &lt;span class="caps"&gt;OCR&lt;/span&gt; with&amp;nbsp;PDFs.&lt;/p&gt;
&lt;p&gt;In this short article, I&amp;#8217;ll run us through an introduction to &lt;span class="caps"&gt;OCR&lt;/span&gt; for &lt;span class="caps"&gt;PDF&lt;/span&gt; documents in Python, with a focus on using Open Source Software (&lt;span class="caps"&gt;OSS&lt;/span&gt;).&lt;/p&gt;
&lt;h1&gt;What&amp;#8217;s a &lt;span class="caps"&gt;PDF&lt;/span&gt;?&lt;/h1&gt;
&lt;p&gt;First, a &lt;a href="https://en.wikipedia.org/wiki/PDF"&gt;Portable Document Format (&lt;span class="caps"&gt;PDF&lt;/span&gt;)&lt;/a&gt; is a file which presents information, including text, images, forms, interactive content, and more. PDFs contain 7-bit &lt;span class="caps"&gt;ASCII&lt;/span&gt; characters, so opening one in a text editor will show you mostly a garble of characters you&amp;#8217;ve never seen before. This is all in the &lt;a href="http://jimpravetz.com/blog/2012/12/in-defense-of-cos/"&gt;Carousel Object Structure&lt;/a&gt;, which is almost like a predecessor to &lt;span class="caps"&gt;XML&lt;/span&gt; and &lt;span class="caps"&gt;JSON&lt;/span&gt;. However, what comes off like a mess is actually a detailed layout of the document, including layers with all the text within the document, complex data structures, and embedded&amp;nbsp;images.&lt;/p&gt;
&lt;h1&gt;Introduction to&amp;nbsp;Tesseract&lt;/h1&gt;
&lt;p&gt;&lt;span class="caps"&gt;OCR&lt;/span&gt; has a history dating back to the early 1900s, whose progress has picked up pace during the 60s and 70s and has made an occasional jump forward in the last couple decades. Because it is a well known problem, many initiatives have made progress in accuracy, and one in particular is &lt;a href="https://www.hitechnectar.com/blogs/open-source-ocr-tools/"&gt;widely acclaimed&lt;/a&gt;. This is &lt;a href="https://tesseract-ocr.github.io/"&gt;Tesseract &lt;span class="caps"&gt;OCR&lt;/span&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Tesseract is an open source &lt;span class="caps"&gt;OCR&lt;/span&gt; engine with more than 100 recognized languages, and a number of useful output types (another image, text, &lt;span class="caps"&gt;PDF&lt;/span&gt;, etc). It is moderately configurable, but has a large following and maintainer community. Most importantly though, in general it works&amp;nbsp;well.&lt;/p&gt;
&lt;p&gt;While there are APIs that layer on top of Tesseract, the engine itself is largely written in C++ and used through a command line interface. The engine accepts as input an image, and if it recognizes text within the image, it will attempt to classify the letters and words in the image and then output the&amp;nbsp;transcription.&lt;/p&gt;
&lt;p&gt;&lt;img class="uk-align-center" data-src="/images/bus-example.png" height="" width="80%" alt="" uk-img&gt;&lt;/p&gt;
&lt;div class="caption"&gt;
  Figure 1. An example of &lt;span class="caps"&gt;OCR&lt;/span&gt; with Tesseract from the original image to the text output.
&lt;/div&gt;

&lt;p&gt;For example, in the above figure, we see how after cropping an image of a bus to retain mostly just the text, we can run Tesseract which outputs the text in the&amp;nbsp;image.&lt;/p&gt;
&lt;p&gt;This is the essence of Tesseract. Its simplicity, efficiency, and accuracy make it an ideal solution for targeted (i.e. minimal content surrounding the text) and fairly readable (i.e. success not guaranteed on Captchas) &lt;em&gt;images&lt;/em&gt;.&lt;/p&gt;
&lt;h1&gt;Introduction to&amp;nbsp;OCRmyPDF&lt;/h1&gt;
&lt;p&gt;Tesseract works great for images, but as discussed above, most documents come as PDFs, and PDFs are a sophisticated data structure. Tesseract does allow the &lt;em&gt;output&lt;/em&gt; of PDFs, but one cannot &lt;em&gt;input&lt;/em&gt; a &lt;span class="caps"&gt;PDF&lt;/span&gt; to Tesseract. While we could hack our way around PDFs to apply Tesseract on them, there exists an open source solution which I&amp;#8217;ve recently worked with and found&amp;nbsp;useful.&lt;/p&gt;
&lt;p&gt;Namely, &lt;a href="https://ocrmypdf.readthedocs.io/en/latest/"&gt;OCRmyPDF&lt;/a&gt; is a specialized command line tool and Python package which is built on a Tesseract &lt;span class="caps"&gt;OCR&lt;/span&gt; engine. OCRmyPDF does accept PDFs as input, and can not only output the text as a companion (&lt;em&gt;sidecar&lt;/em&gt;) text file, but also overlays the text directly on top of the underlying images in the &lt;span class="caps"&gt;PDF&lt;/span&gt;. OCRmyPDF essentially pulls out the bitmap images from the &lt;span class="caps"&gt;PDF&lt;/span&gt;, performs a series of pre-processing steps (e.g. denoising, deskewing, etc.), then performs &lt;span class="caps"&gt;OCR&lt;/span&gt; on those&amp;nbsp;images.&lt;/p&gt;
&lt;p&gt;Suppose we have a &lt;span class="caps"&gt;PDF&lt;/span&gt; which looks like the&amp;nbsp;following:&lt;/p&gt;
&lt;p&gt;&lt;embed src="/pdf/propublica-tax.pdf" height="500" type="application/pdf" style="margin:2em 0;"&gt;&lt;/p&gt;
&lt;p&gt;Yes, I chose the 2018 tax returns of &lt;a href="https://www.propublica.org/"&gt;ProPublica&lt;/a&gt;. They&amp;#8217;re always investigating people, I think it&amp;#8217;s only fair we investigate them too. Besides saving only the most interesting pages in the full 65 page &lt;span class="caps"&gt;IRS&lt;/span&gt; report, I didn&amp;#8217;t alter the document at all. You&amp;#8217;ll notice you can&amp;#8217;t select any of the text in the document&amp;nbsp;though!&lt;/p&gt;
&lt;p&gt;OCRmyPDF to the rescue. If we save this document, and in the working folder run the following Bash&amp;nbsp;command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;ocrmypdf propublica-tax.pdf propublica-tax-out.pdf --sidecar propublica-tax.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;the package will process the original &lt;span class="caps"&gt;PDF&lt;/span&gt; (the first argument), then output the &lt;span class="caps"&gt;OCR&lt;/span&gt;&amp;#8217;d &lt;span class="caps"&gt;PDF&lt;/span&gt; to the output file (second argument), and will also output a text file with the extracted text (file provided by the &lt;code&gt;--sidecar&lt;/code&gt; flag). Here is the &lt;span class="caps"&gt;OCR&lt;/span&gt;&amp;#8217;d&amp;nbsp;file:&lt;/p&gt;
&lt;p&gt;&lt;embed src="/pdf/propublica-tax-out.pdf" height="500" type="application/pdf" style="margin:2em 0;"&gt;&lt;/p&gt;
&lt;p&gt;While I encourage you to highlight and select text within the document, for those of you on mobile, here is a sample of what&amp;#8217;s in the text&amp;nbsp;file:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A For the 2019 calendar year, or tax year beginning 01-01-2018 , and ending&amp;nbsp;12-31-2018&lt;/p&gt;
&lt;p&gt;B Check if&amp;nbsp;applicable&lt;/p&gt;
&lt;p&gt;&lt;span class="caps"&gt;OO&lt;/span&gt; Address&amp;nbsp;change&lt;/p&gt;
&lt;p&gt;O Name&amp;nbsp;change&lt;/p&gt;
&lt;p&gt;Return of Organization Exempt From Income&amp;nbsp;Tax&lt;/p&gt;
&lt;p&gt;Under section 501(c), 527, or 4947(a)(1) of the Internal Revenue Code (except private&amp;nbsp;foundations)&lt;/p&gt;
&lt;p&gt;Â® Do not enter social security numbers on this form as it may be made&amp;nbsp;public&lt;/p&gt;
&lt;p&gt;Â» Go to www.irs.gov/Form990 for instructions and the latest&amp;nbsp;information.&lt;/p&gt;
&lt;p&gt;&lt;span class="caps"&gt;OMB&lt;/span&gt; No&amp;nbsp;1545-0047&lt;/p&gt;
&lt;p&gt;2018&lt;/p&gt;
&lt;p&gt;Open to&amp;nbsp;Public&lt;/p&gt;
&lt;p&gt;Inspection&lt;/p&gt;
&lt;p&gt;C Name of&amp;nbsp;organization&lt;/p&gt;
&lt;p&gt;&lt;span class="caps"&gt;PRO&lt;/span&gt; &lt;span class="caps"&gt;PUBLICA&lt;/span&gt; &lt;span class="caps"&gt;INC&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;D Employer identification&amp;nbsp;number&lt;/p&gt;
&lt;p&gt;14-2007220&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Clearly, it has flaws. Namely, it&amp;#8217;s fairly poor at disambiguating table and cell structures, and special characters are difficult to interpret. It gets some structure correct, but there&amp;#8217;s only so much formatting we can represent with plain text files. Overall though, we could do a lot of processing with this text data. Across many tax returns, I&amp;#8217;m sure we&amp;#8217;d notice plenty of patterns and extracting the most useful information would not be that&amp;nbsp;difficult.&lt;/p&gt;
&lt;h1&gt;Concurrent Processing of&amp;nbsp;PDFs&lt;/h1&gt;
&lt;p&gt;OCRmyPDF may work very well in this way should you have only a handful of PDFs to process. If you have dozens, hundreds, or millions of PDFs, this process would take too long. One way to make this more efficient is to use multiprocessing, or&amp;nbsp;concurrency.&lt;/p&gt;
&lt;p&gt;We can do this with a little Python, although I&amp;#8217;ve recently looked into learning some Go since I hear that concurrency is natively supported with their syntax and functions very well (see &lt;a href="https://divan.dev/posts/go_concurrency_visualize/"&gt;this cool concurrency visualization&lt;/a&gt; using&amp;nbsp;Go).&lt;/p&gt;
&lt;p&gt;Using the &lt;code&gt;multiprocessing&lt;/code&gt; native library in Python, roughly, we can spin up multiple processes (spread across however many CPUs you have) that each control an &lt;span class="caps"&gt;OCR&lt;/span&gt; job in parallel. If you have &lt;span class="math"&gt;\(N\)&lt;/span&gt; processes, and they&amp;#8217;ll all able to run in parallel, the time to run the total task will be divided by &lt;span class="math"&gt;\(N\)&lt;/span&gt;. If you have 100 PDFs, and each takes 20 seconds to &lt;span class="caps"&gt;OCR&lt;/span&gt;, this would take 30 minutes in serial&amp;#8212;-in parallel on 4 processes, this would take (surprise), 8.3&amp;nbsp;minutes.&lt;/p&gt;
&lt;p&gt;To make a small script to &lt;span class="caps"&gt;OCR&lt;/span&gt; many local documents, it would: first, load the names of the files to process. Second, distribute those files to parallel jobs. Third, execute each job. All of this can be executed in essentially less than 10 lines of&amp;nbsp;code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;multiprocessing&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Pool&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;ocrmypdf&lt;/span&gt;

&lt;span class="n"&gt;PROCESSES&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;

&lt;span class="n"&gt;files&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scandir&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;pdfs/&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;

&lt;span class="n"&gt;chunk_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;files&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;PROCESSES&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;file_chunks&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;files&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;chunk_size&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;files&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;chunk_size&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;ocr_files&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;file_list&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;input_file&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;file_list&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;ocrmypdf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ocr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;pdfs/&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;input_file&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;pdfs/out_&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;input_file&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;Pool&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;processes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;PROCESSES&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;pool&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;pool&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ocr_files&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;file_chunks&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;To break this down a&amp;nbsp;little:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We predefine the number of &lt;span class="caps"&gt;CPU&lt;/span&gt; processes to spawn. This number can be static, or reflect the machine being used (i.e. &lt;code&gt;os.cpu_count()&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;Find the documents to process (in the &lt;code&gt;pdfs/&lt;/code&gt; directory). Take this list of files and group it into evenly sized&amp;nbsp;chunks.&lt;/li&gt;
&lt;li&gt;Create a small helper function to iterate over the files given to the process. Here is where we call the OCRmyPDF &lt;span class="caps"&gt;API&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Create a process pool with &lt;span class="math"&gt;\(N\)&lt;/span&gt; processes, and map the chunks of files over the helper&amp;nbsp;function.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There are numerous improvements we could make, but this is a start. We&amp;#8217;re not catching exceptions, we&amp;#8217;re not allowing processes to recover if they face an error and shut down, we&amp;#8217;re not logging, we&amp;#8217;re not properly creating file paths (i.e. using &lt;code&gt;os.path.join&lt;/code&gt;),&amp;nbsp;etc.&lt;/p&gt;
&lt;p&gt;However, this would &lt;span class="caps"&gt;OCR&lt;/span&gt; a decent sized batch of PDFs for the lay person. Hopefully this tutorial is useful to those looking to dip their feet into &lt;span class="caps"&gt;OCR&lt;/span&gt; with Python without spending excessive amounts of time or money on cloud compute or proprietary software. My final comment and question for the reader: what is the best way to say, in verb form, that one will &amp;#8220;&lt;span class="caps"&gt;OCR&lt;/span&gt;&amp;#8221; a document (to Optical Character Recognize sounds&amp;nbsp;awful)?&lt;/p&gt;
&lt;p&gt;If you have any questions or would like (me) to clarify anything, please &lt;a href="mailto:alexliebscher0@gmail.com"&gt;let me know&lt;/a&gt;!&lt;/p&gt;
&lt;hr &gt;
&lt;p&gt;Update (2022-02-21): I originally used the &lt;code&gt;multiprocessing&lt;/code&gt; library to create processes for parallel tasks. If I were rewriting this article I might instead take advantage of the &lt;code&gt;pqdm&lt;/code&gt; package, which is more user friendly and very helpfully includes a progress&amp;nbsp;bar.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="meta"></category><category term="python"></category><category term="nlp"></category><category term="programming"></category></entry><entry><title>Joining Property DataÂ Works</title><link href="/posts/2020/Nov/job-pdw/" rel="alternate"></link><published>2020-11-16T00:00:00-08:00</published><updated>2020-11-16T00:00:00-08:00</updated><author><name>Alex Liebscher</name></author><id>tag:None,2020-11-16:/posts/2020/Nov/job-pdw/</id><summary type="html">&lt;p&gt;Joining the Property Data Works team as a Software&amp;nbsp;Engineer.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Property Data Works was a 6 person team aggregating and analyzing real estate data from south Florida (e.g. Miami-Dade, Broward, Palm Beach) for real estate investors, mortgage loan officers, realtors, and more. The team built an impressive web app and extensive data collection and processing system, but we failed to gain enough traction to stay afloat and folded March&amp;nbsp;2021.&lt;/p&gt;
&lt;p&gt;As a software engineer, my contributions included building an Optical Character Recognition pipeline and information extraction system which processed about 20 million PDFs (~10 &lt;span class="caps"&gt;TB&lt;/span&gt;). In essence, we started with many PDFs, and it was my responsibility to create machine readable texts for each document, then extract key fields from each document (viz. document titles, dates, monetary values) to eventually display to the end-user. I wrote an article&lt;/a href=â/blog/2021/ocr-pdfs/â&gt; on &lt;span class="caps"&gt;OCR&lt;/span&gt; in Python based on this&amp;nbsp;experience.&lt;/p&gt;</content><category term="meta"></category><category term="news"></category></entry><entry><title>When Will I Finish ThatÂ Book?</title><link href="/posts/2020/Oct/when-finish-book/" rel="alternate"></link><published>2020-10-21T17:20:00-07:00</published><updated>2020-10-21T17:20:00-07:00</updated><author><name>Alex Liebscher</name></author><id>tag:None,2020-10-21:/posts/2020/Oct/when-finish-book/</id><summary type="html">&lt;p&gt;Here I build a probabilistic model to estimate when I&amp;#8217;ll finish a&amp;nbsp;book&lt;/p&gt;</summary><content type="html">&lt;!-- custom-javascript:
  - "https://code.highcharts.com/highcharts.js"
  - "https://code.highcharts.com/highcharts-more.js"
  - "/assets/js/2020-10-21-when-finish-book.js" --&gt;

&lt;p&gt;It&amp;#8217;s ironic that one of my first thoughts when I start a book is about when I&amp;#8217;ll finish it. That thought seems to stem from the feeling of knowing there&amp;#8217;re so many things to read, yet so little time&lt;d-footnote&gt;If there&amp;#8217;s a name for this feeling, please let me know. The Japanese term &lt;a href="https://en.wikipedia.org/wiki/Tsundoku" target="_blank"&gt;Tsundoku&lt;/a&gt; is not quite what I&amp;#8217;m thinking.&lt;/d-footnote&gt;. No matter how much I&amp;#8217;m enjoying a book, I&amp;#8217;m somehow always looking forward to what&amp;#8217;s&amp;nbsp;next.&lt;/p&gt;
&lt;p&gt;Consequently, I wanted to figure out when I&amp;#8217;d finish a book. In this essay, I introduce a model to estimate the days needed to complete a book given a small sample of one&amp;#8217;s historical reading data for that&amp;nbsp;book.&lt;/p&gt;
&lt;h2&gt;Existing&amp;nbsp;Methods&lt;/h2&gt;
&lt;p&gt;The &lt;em&gt;de facto&lt;/em&gt; method to estimate the amount of time to complete a book is no more than simple algebra, with little flexibility and insight. Specifically, if we know we have 100 pages left, and we&amp;#8217;ve already calculated that we read 2 minutes per page, it is clear that we&amp;#8217;d need 200 minutes. If we set a goal to read 20 minutes a day, then we can estimate we&amp;#8217;ll be done in 10 days. This is fairly simple, and for that reason we&amp;#8217;re spared much interesting information. For example, how sure are we that we&amp;#8217;ll finish then? This also assumes we&amp;#8217;ll read every day, but that might not be&amp;nbsp;true.&lt;/p&gt;
&lt;p&gt;There&amp;#8217;s a quick solution to overcome this last point. One may have read 6 days of the last week, which in one sense means there&amp;#8217;s a 5/7 = 71.4% probability they&amp;#8217;ll read on any given day. This reader might say they need 10 reading days of &lt;em&gt;reading&lt;/em&gt; to finish the book. If they know there&amp;#8217;s a 28.6% chance they won&amp;#8217;t read on a given day, where &lt;span class="math"&gt;\(10*0.286 = 2.86 \approx 3\)&lt;/span&gt;, then they might conclude that they&amp;#8217;ll finish the book in 10 + 3 = 13&amp;nbsp;days.&lt;/p&gt;
&lt;p&gt;This is easily extended to another simple model, where one might specify the frequency that they read. For example, they might say they read every other day, which doubles the estimate then. The same idea holds over reading once every 2 days, or 3, or&amp;nbsp;whatever.&lt;/p&gt;
&lt;p&gt;For many, this will suffice. For some, more is&amp;nbsp;necessary.&lt;/p&gt;
&lt;h2&gt;Data&amp;nbsp;Generation&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;the data analyst needs to incorporate the information describing the data collection process in the probability model used for&amp;nbsp;analysis.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&amp;#8212;- Gelman et al. (2013), &lt;em&gt;Bayesian Data&amp;nbsp;Analysis&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Taking a step back, how might we conceptually model the act of reading? Well, suppose you&amp;#8217;re beginning a book. You wake up one morning, and, without commenting on whether nature is predetermined, let&amp;#8217;s say there&amp;#8217;s some probability you&amp;#8217;ll read your book on that day (for any&amp;nbsp;duration).&lt;/p&gt;
&lt;p&gt;Suppose you don&amp;#8217;t read; you&amp;#8217;ll simply log 0 hours. Suppose you do read; how much will it&amp;nbsp;be?&lt;/p&gt;
&lt;p&gt;There&amp;#8217;re a lot of factors that may enter the equation now, such as whether it&amp;#8217;s a weekend, the outside weather, or maybe whether you&amp;#8217;re nearing the end of the book and are motivated to read more to finish. For simplicity&amp;#8217;s sake, we&amp;#8217;ll assume that by some process, such as habit, you&amp;#8217;ll read about &lt;span class="math"&gt;\(x\)&lt;/span&gt; hours. For many people, &lt;span class="math"&gt;\(x=0.25, 0.5,\)&lt;/span&gt; or &lt;span class="math"&gt;\(1.0\)&lt;/span&gt; (plus or minus some), but this widely varies from person to&amp;nbsp;person.&lt;/p&gt;
&lt;p&gt;To put it all together, there&amp;#8217;s a probability you&amp;#8217;ll read on a given day, and when you do read, it might typically be around some number plus or minus a bit. With this model of how the world works (which probably fails in many ways, but for now we don&amp;#8217;t care), we can estimate when and how much we&amp;#8217;ll read, which can be used to calculate how many days left for us. Let&amp;#8217;s explore a case study with real&amp;nbsp;numbers.&lt;/p&gt;
&lt;h2&gt;Case&amp;nbsp;Study&lt;/h2&gt;
&lt;p&gt;I am reading &amp;#8221;The Brothers Karamazov&amp;#8221; by Fyodor Dostoevesky at the moment. It&amp;#8217;s long, so naturally I&amp;#8217;m curious when I&amp;#8217;ll finish it. I&amp;#8217;ve used &lt;a href="https://getbookly.com/"&gt;the Bookly iOS application&lt;/a&gt; to log most of my reading for the novel so far. I&amp;#8217;ve been reading for a bit more than a week, and have made a point to read each day. So far, here is a sample of my historical&amp;nbsp;data:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="right"&gt;Date&lt;/th&gt;
&lt;th&gt;Oct 4&lt;/th&gt;
&lt;th&gt;Oct 5&lt;/th&gt;
&lt;th&gt;Oct 6&lt;/th&gt;
&lt;th&gt;Oct 7&lt;/th&gt;
&lt;th&gt;Oct 8&lt;/th&gt;
&lt;th&gt;Oct 9&lt;/th&gt;
&lt;th&gt;Oct 10&lt;/th&gt;
&lt;th&gt;Oct 11&lt;/th&gt;
&lt;th&gt;Oct 12&lt;/th&gt;
&lt;th&gt;Oct 13&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="right"&gt;Duration&lt;/td&gt;
&lt;td&gt;26:53&lt;/td&gt;
&lt;td&gt;28:25&lt;/td&gt;
&lt;td&gt;1:15:16&lt;/td&gt;
&lt;td&gt;47:35&lt;/td&gt;
&lt;td&gt;1:03:20&lt;/td&gt;
&lt;td&gt;1:01:21&lt;/td&gt;
&lt;td&gt;1:01:56&lt;/td&gt;
&lt;td&gt;44:55&lt;/td&gt;
&lt;td&gt;1:06:28&lt;/td&gt;
&lt;td&gt;24:23&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="right"&gt;Pages&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;29&lt;/td&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;td&gt;22&lt;/td&gt;
&lt;td&gt;22&lt;/td&gt;
&lt;td&gt;26&lt;/td&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;td&gt;26&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Including my other data, on average, I read about 49.4 minutes per day, and 20.7 pages per hour (2.9 minutes per page). At this point, I&amp;#8217;m 321 pages in, with 475 to go. That means roughly 22.9 hours remaining. So far I&amp;#8217;ve read every day. The naive estimate explained above puts me at 23 days away from&amp;nbsp;completion.&lt;/p&gt;
&lt;p&gt;However, I know I won&amp;#8217;t read every single day, and I&amp;#8217;d like to account for this variation. Moreover, it is of little help to have just a point estimate. If possible, I&amp;#8217;d like to know a probable range in which I&amp;#8217;ll finish (for all I know without accounting for the variation, this point estimate might stretch from 10 to 100&amp;nbsp;days).&lt;/p&gt;
&lt;h2&gt;Modeling&lt;/h2&gt;
&lt;p&gt;If you&amp;#8217;re not interested in the math, you can skip to the next&amp;nbsp;section.&lt;/p&gt;
&lt;p&gt;First off, let our potential data be denoted &lt;span class="math"&gt;\(y = (y_1, \ldots, y_N)\)&lt;/span&gt;. Soon we&amp;#8217;ll introduce what values these can take&amp;nbsp;on.&lt;/p&gt;
&lt;p&gt;Let &lt;span class="math"&gt;\(B\)&lt;/span&gt; be the event that we read on a given future day. We can model &lt;span class="math"&gt;\(B\)&lt;/span&gt; as a Bernoulli random event, &lt;span class="math"&gt;\(B \sim \text{Bern}(\theta)\)&lt;/span&gt; where &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; is the probability of a positive result form the event. If &lt;span class="math"&gt;\(\theta = 0.9\)&lt;/span&gt;, then we&amp;#8217;d say there&amp;#8217;s a 90% probability we&amp;#8217;ll read on a given future&amp;nbsp;day.&lt;/p&gt;
&lt;p&gt;Let &lt;span class="math"&gt;\(Y\)&lt;/span&gt; be the event denoting the number of hours (in decimal form) we read on a given day &lt;em&gt;when we do read&lt;/em&gt;. Using our historical data, we can model &lt;span class="math"&gt;\(Y\)&lt;/span&gt; as with a Student&amp;#8217;s t distribution, to account for greater uncertainty if we have very little data, namely &lt;span class="math"&gt;\(Y \sim \text{t}_\nu (\mu, \sigma)\)&lt;/span&gt; where &lt;span class="math"&gt;\(\nu\)&lt;/span&gt; is the degrees of freedom of the distribution, and &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; and &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt; denote the mean and the standard deviation of the distribution. Including the mean and variance with the Student&amp;#8217;s t distribution I think is slightly unorthodox, but concisely and conveniently allows us to vary them both. Now, if &lt;span class="math"&gt;\(\mu = 1.0\)&lt;/span&gt; for example, then we&amp;#8217;d say that we&amp;#8217;re most likely to read an hour a day when we do read. Of course, it is technically false for us to claim that &lt;span class="math"&gt;\(Y\)&lt;/span&gt; is Student&amp;#8217;s t-distributed since in no world can we read less than zero hours in a day!&lt;d-footnote&gt;I&amp;#8217;ve been thinking of how to model this as a positive continuous distribution, but I&amp;#8217;m still new to this world and wanted to restrain myself for the time being. If you have helpful comments, please share.&lt;/d-footnote&gt; This is merely a computational convenience. However, in the model parameter specifications, we won&amp;#8217;t let the sampler sample values less than&amp;nbsp;0.&lt;/p&gt;
&lt;p&gt;Finally, to flesh out &lt;span class="math"&gt;\(y\)&lt;/span&gt;, we&amp;#8217;ll&amp;nbsp;say:&lt;/p&gt;
&lt;div class="math"&gt;$$
y_i \sim \begin{cases}
      \text{t}_\nu (\mu, \sigma) &amp;amp; \text{if}\;B_i \\
      0 &amp;amp; \text{otherwise}
   \end{cases}
$$&lt;/div&gt;
&lt;p&gt;To incorporate all the necessary variance, we also will define the distributions of &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;, &lt;span class="math"&gt;\(\mu\)&lt;/span&gt;, and &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt; (&lt;span class="math"&gt;\(\nu\)&lt;/span&gt; will be supplied in the problem by the amount of data we&amp;#8217;re inputing). A perfect distribution for a Bernoulli probability is the Beta distribution, so &lt;span class="math"&gt;\(\theta \sim \text{Beta}(1,1)\)&lt;/span&gt;. &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; will be modeled by a normal distribution, centered around 10 minutes per day with a bit of variance (this number comes from a national average of how much Americans read per day). Lastly, &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt; will be modeled by an inverse Gamma, with &lt;span class="math"&gt;\(\alpha = 0.07/0.93\)&lt;/span&gt; (to center the distribution over 0.93 hours) and &lt;span class="math"&gt;\(\beta = 1\)&lt;/span&gt;. All three of these help establish the larger model as a completely random data generation process. In &lt;code&gt;stan&lt;/code&gt; this looks&amp;nbsp;like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;parameters&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="kt"&gt;real&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;lower&lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="kt"&gt;real&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;lower&lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="kt"&gt;real&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;lower&lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="k"&gt;upper&lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="kn"&gt;model&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="n"&gt;theta&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;beta&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;1&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
  &lt;span class="n"&gt;mu&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.1667&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
  &lt;span class="n"&gt;sigma&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;inv_gamma&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.07&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mf"&gt;0.93&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;

  &lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;bernoulli&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
  &lt;span class="n"&gt;Y&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;student_t&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We&amp;#8217;re making some arguable assumptions here as well. First, that each day is independent of the others&lt;d-footnote&gt;My knowledge is limited on how to treat this assumption, but perhaps &lt;a href="https://arxiv.org/pdf/1505.04321.pdf" target="_blank"&gt;sequential estimation&lt;/a&gt; might be in order.&lt;/d-footnote&gt;. Surely there&amp;#8217;s also some psychological feeling of not wanting to break my reading streak, subliminally pushing me to reading each day. Here, we&amp;#8217;re saying every day is a fresh day. Second, when we do read, the amount that we read is a random draw from a friendly distribution&lt;d-footnote&gt;As mentioned, this is likely an incredibly complex distribution, but like any statistician with proper due diligence, we&amp;#8217;re reducing it to one of those incredibly natural, perfectly symmetrical distributions.&lt;/d-footnote&gt;.&lt;/p&gt;
&lt;h2&gt;Prediction&lt;/h2&gt;
&lt;p&gt;With our model established, we would like to predict when we&amp;#8217;ll finish our book. This harks back to our original question. So how do we do&amp;nbsp;it?&lt;/p&gt;
&lt;p&gt;I will illustrate first with an example. Suppose we&amp;#8217;re reading a book and know &lt;em&gt;a priori&lt;/em&gt; that we have 10 hours remaining. We then calculate our reading pace so far, perhaps it&amp;#8217;s 0.5 hours per day. Suppose also we&amp;#8217;ve read 5 of 7 of the past&amp;nbsp;days.&lt;/p&gt;
&lt;p&gt;Then, to determine the number of days remaining, we&amp;#8217;ll start at day 1. On day 1, will we read? The probability we do will lie somewhere around &lt;span class="math"&gt;\(\hat{\theta}\)&lt;/span&gt;. Suppose we don&amp;#8217;t read though. Fine, skip to day 2, still with 10 hours remaining. Will we read on day 2? Suppose we do, and in fact we end up reading about &lt;span class="math"&gt;\(\tilde{\mu}_2\)&lt;/span&gt; hours. To illustrate, let&amp;#8217;s say it&amp;#8217;s 24 minutes, or 0.4&amp;nbsp;hours.&lt;/p&gt;
&lt;p&gt;So now we have &lt;span class="math"&gt;\(10 - 0.4 = 9.6\)&lt;/span&gt; hours remaining. Again, flip a biased coin (with probability of heads at &lt;span class="math"&gt;\(\hat{\theta}\)&lt;/span&gt;). It&amp;#8217;s heads, so we read again on day 3. This time, &lt;span class="math"&gt;\(\tilde{\mu}_3 = 0.6\)&lt;/span&gt; hours. Now there&amp;#8217;s &lt;span class="math"&gt;\(9.6-0.6 = 9.0\)&lt;/span&gt; hours&amp;nbsp;remaining.&lt;/p&gt;
&lt;p&gt;Follow this procedure while &lt;span class="math"&gt;\(10 - \sum^k_{i=1} B_k(\hat{\theta}) * \tilde{\text{t}}_{k+1} (\mu_k, \sigma_k) &amp;gt; 0\)&lt;/span&gt; (roughly). In the end, what is &lt;span class="math"&gt;\(k\)&lt;/span&gt;? Well, it is the number of days necessary to exhaust the pages in our book, given estimates of how often and how much we already&amp;nbsp;read.&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;R&lt;/code&gt;, the algorithm looks like&amp;nbsp;such:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;draw&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;remaining_hours&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;posterior_theta&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;posterior_mu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;posterior_sigma&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;

  &lt;span class="n"&gt;total_read&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;
  &lt;span class="n"&gt;days&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;

  &lt;span class="nf"&gt;while &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;total_read&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;remaining_hours&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;

    &lt;span class="n"&gt;to_read&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;rbernoulli&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nf"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;posterior_theta&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="nf"&gt;if &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;to_read&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;

      &lt;span class="n"&gt;mu&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;posterior_mu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
      &lt;span class="n"&gt;sigma&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;posterior_sigma&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
      &lt;span class="n"&gt;est&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nf"&gt;rt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;days&lt;/span&gt;&lt;span class="m"&gt;+1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;

      &lt;span class="nf"&gt;if &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;est&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="n"&gt;est&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="m"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;next&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

      &lt;span class="n"&gt;total_read&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;total_read&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;est&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="n"&gt;days&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;days&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="nf"&gt;return&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;days&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Results&lt;/h2&gt;
&lt;p&gt;Let&amp;#8217;s go back to my case study for &amp;#8221;The Brothers Karamazov&amp;#8221;. If we fit the model with my historical reading data for the book, our model samples each parameter like&amp;nbsp;so:&lt;/p&gt;
&lt;p&gt;&lt;img class="uk-align-center" data-src="/images/parameter-estimates.png" height="" width="80%" alt="" uk-img&gt;&lt;/p&gt;
&lt;div class="caption"&gt;
    4,000 samples drawn for each parameter to estimate the parameter values.
&lt;/div&gt;

&lt;p&gt;Ultimately, the estimates for each parameter are: &lt;span class="math"&gt;\(\hat{\mu} = 0.81\)&lt;/span&gt;, &lt;span class="math"&gt;\(\hat{\sigma} = 0.37\)&lt;/span&gt;, and &lt;span class="math"&gt;\(\hat{\theta} = 0.95\)&lt;/span&gt;. This means there&amp;#8217;s an average 95% probability I&amp;#8217;ll read on a given day, and when I do read it will on average be 48.6&amp;nbsp;minutes.&lt;/p&gt;
&lt;p&gt;Using the algorithm described above, we will sample 2,000 draws of the posterior predictive distribution, which uses &lt;span class="math"&gt;\(\mu\)&lt;/span&gt;, &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;, and &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; to determine the number of days remaining. Therefore, the posterior predictive distribution of the days remaining, given a sample of my historical reading data, looks&amp;nbsp;like:&lt;/p&gt;
&lt;p&gt;&lt;img class="uk-align-center" data-src="/images/days-remaining-hist.png" height="" width="80%" alt="" uk-img&gt;&lt;/p&gt;
&lt;div class="caption"&gt;
    Histogram of days remaining on my book drawn from the posterior predictive distribution
&lt;/div&gt;

&lt;p&gt;The mean here is 28.7, with a median of 29 (89% Credible Interval: [24, 34])&lt;d-footnote&gt;For an explanation of why I&amp;#8217;m using 89% Credible Intervals, see &lt;a href="https://easystats.github.io/bayestestR/articles/credible_interval.html#why-is-the-default-89" target="_blank"&gt;this nice summary&lt;/a&gt;. In short, all intervals are arbitrary, so why not pick one that&amp;#8217;s slightly less arbitrary.&lt;/d-footnote&gt;. We can take this one step further and visualize how the prediction accuracy improves over time (i.e. with more&amp;nbsp;data).&lt;/p&gt;
&lt;p&gt;For each day in sequence, we can re-estimate the model parameters (with progressively more data). On each re-fit, we&amp;#8217;ll add up the hours we estimate we have left and the hours we know we will read on future days to say how long it will take to finish &lt;em&gt;from that day&lt;/em&gt;. As we artificially gain information, our estimates naturally become more precise. This is because we home in on a better estimate for how often we read and much we read when we do. We can find 95% and 68.2% credible intervals as well. In fact, we can nicely display this all as&amp;nbsp;such:&lt;/p&gt;
&lt;div id="container" style="width:100%; height:400px;"&gt;&lt;/div&gt;
&lt;div class="caption"&gt;
    As our model sees more data with each new day, the parameter estimates become more precise. The dotted line represents the estimates from the simple model describes in the intro (pages remaining / cumulative pace). The complex model is much more pessimistic, eventually the two will converge though.
&lt;/div&gt;

&lt;p&gt;We can see that the number of days remaining is decreasing over time, since we are taking off an absolute number of pages from the book. We can also see that our estimation is getting more precise, as a result of relying less on the priors and more on the data. You can also see how our model built here compares to the prediction from the simple model explained in Existing Models. The complex model, which factors in a variety of sources of variance, reflects our day to day behavior better but is clearly more pessimistic. However, as I near the end of my book the two lines will&amp;nbsp;converge.&lt;/p&gt;
&lt;p&gt;To interpret this chart further, we might conclude that I am most likely to finish my book in 29 days, which at the time of writing is November 19, but there&amp;#8217;s a full 89% probability that I&amp;#8217;ll finish the book within 24 to 34&amp;nbsp;days.&lt;/p&gt;
&lt;p&gt;Consider our original estimation: 23 days until completion. Although kind of close, it doesn&amp;#8217;t account for the fact that I might not read every day in the near future. Moreover, if I do end up missing a day, the complex model will adapt and project the date further out. The simple model simply won&amp;#8217;t&amp;nbsp;change.&lt;/p&gt;
&lt;h2&gt;Overall&lt;/h2&gt;
&lt;p&gt;Overall, this is a case study of how one might predict when they&amp;#8217;ll finish a book, but with a bit of humanity&amp;#8217;s pitfalls injected in it (there &lt;em&gt;is&lt;/em&gt; some chance I won&amp;#8217;t read everyday!). I am happy to know how quickly I will finish my book since it allows me to set goals and feel good about my progress, knowing that sooner or later (most probably in less than Y days) I will be able to put the book aside and begin the next. I&amp;#8217;m still a novice statistician, so if there&amp;#8217;s gross misunderstandings in my work, please point them out (even if you&amp;#8217;re not sure&amp;nbsp;yourself).&lt;/p&gt;
&lt;p&gt;Reading is not all a numbers game. And I&amp;#8217;m not racing anyone. I am, however, eager to read as much as a can before I can&amp;#8217;t. Books are meant to be enjoyed. If you&amp;#8217;re not enjoying them while reading none of the above has any validity. It can be tempting to scold or punish yourself or feel guilty for not meeting your reading goal. If you do not meet the estimated average time to completion&amp;#8212;-keep in mind that the distribution is infinite and some books will either take a &lt;em&gt;very&lt;/em&gt; long time to finish, or will never be finished, and that&amp;#8217;s okay&amp;nbsp;too.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="meta"></category><category term="statistics"></category><category term="rlang"></category></entry><entry><title>On Locke and the TrumpÂ Administration</title><link href="/posts/2020/Oct/locke-trump/" rel="alternate"></link><published>2020-10-17T18:00:00-07:00</published><updated>2020-10-17T18:00:00-07:00</updated><author><name>Alex Liebscher</name></author><id>tag:None,2020-10-17:/posts/2020/Oct/locke-trump/</id><summary type="html">&lt;p&gt;An analogy between Locke&amp;#8217;s standard of tyranny and the Trump&amp;nbsp;Administration&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;In this essay I hope to frame the Trump Administration with principles derived from John Locke&amp;#8217;s &lt;em&gt;Second Treatise on Government&lt;/em&gt;, a foundational piece in the development of Western democracy and liberalism. First, I&amp;#8217;ll introduce Locke&amp;#8217;s perspective on virtuous government, especially his remarks on tyranny and despotic government. Then, I&amp;#8217;ll cover some behavior from the Trump Administration that one might argue falls under Locke&amp;#8217;s definition of &amp;#8220;unjust&amp;#8221; government. Lastly, we&amp;#8217;ll question the analogy, its possible implications, and where to go from&amp;nbsp;here.&lt;/p&gt;
&lt;p&gt;&lt;img class="uk-align-center" width="80%" height="" src="/images/john-locke.jpg" alt="A portrait painting of John Locke" uk-img&gt;&lt;/p&gt;
&lt;div class="caption"&gt;
    &lt;strong&gt;Figure 1.&lt;/strong&gt; John Locke by Godfrey Kneller (c1697). Courtesy Hermitage Museum/Wikipedia via aeon.co
&lt;/div&gt;

&lt;p&gt;John Locke in his &lt;em&gt;Second Treatise on Government&lt;/em&gt;&lt;d-footnote&gt;The &lt;span class="font-italic"&gt; Second Treatise&lt;/span&gt; was actually a rebuttal to Sir Robert Filmer&amp;#8217;s &lt;a href="https://en.wikipedia.org/wiki/Patriarcha" target="_blank" class="font-italic"&gt;Patriarcha&lt;/a&gt;, rejecting the notion of rule by Divine Right.&lt;/d-footnote&gt; sought to distinguish &lt;em&gt;just&lt;/em&gt; government from &lt;em&gt;unjust&lt;/em&gt; government. It is famous for its discussion of natural rights, social contract, and private property in general. Locke was very much devoted to rejecting authoritarianism, and is made clear through the piece. He is not without controversy, for example regarding &lt;a href="https://aeon.co/essays/does-lockes-entanglement-with-slavery-undermine-his-philosophy" target="_blank"&gt;his involvement in the dispute over slavery in society&lt;/a&gt;&lt;d-footnote&gt;One chapter of the Second Treatise covered slavery, for which he offers a perhaps surprisingly limited role, and offers little argumentative support for the Afro-American slave trade. From the article linked: &amp;#8220;It is a deep error, therefore, to contend that Lockeâs role in the Carolina constitutions should guide interpretation of his later work, much less liberalism.&amp;#8221;&lt;/d-footnote&gt;.&lt;/p&gt;
&lt;p&gt;Ultimately, his writing paved the way for Western civilizations, &lt;a href="https://www.johnlocke.org/john-locke-his-american-and-carolinian-legacy/" target="_blank"&gt;including the Founding Fathers of the United States&lt;/a&gt;. Of course, we&amp;#8217;ll never know what he would have thought about modern government and politics, but we can try to&amp;nbsp;imagine.&lt;/p&gt;
&lt;h1&gt;What is&amp;nbsp;Tyranny?&lt;/h1&gt;
&lt;p&gt;To begin, I will introduce a quote from Locke from the section &lt;em&gt;Of Political or Civil Society&lt;/em&gt;, where he states&amp;nbsp;movingly:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Man being born&amp;#8230; with a title to perfect freedom, and an uncontrolled enjoyment of all the rights and privileges of the law of nature, equally with any other man&amp;#8230; hath by nature a power, not only to preserve his property, that is, his life, liberty and estate, against the injuries and attempts of other&amp;nbsp;men&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This sets well the stage for the rest of the discussion. In particular, Locke ascertains that individuals in society should have an unquestionable right to their own lives, freedom, and land, and this should always be the case and others should not be allowed to break&amp;nbsp;this.&lt;/p&gt;
&lt;p&gt;Locke claims that government should be limited and constitutional, with the authority only to function as society, as the people, entrusted in it. He argues that this function, or &amp;#8220;chief end,&amp;#8221; of &amp;#8220;men&amp;#8217;s uniting into common-wealths,&amp;#8221; is simply &amp;#8220;the preservation of their&amp;nbsp;property.&amp;#8221;&lt;/p&gt;
&lt;p&gt;So, this begs the question, what happens if the government fails to achieve or attempt to achieve this function?&amp;nbsp;Well,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;for where the power, that is put in any hands for the government of the people&amp;#8230; is applied to other ends, and made use of to impoverish, harass, or subdue&lt;d-footnote&gt;I had to look up a definition to truly understand &amp;#8220;subdue&amp;#8221;. From Merriam Webster: to conquer and bring into subjection, to bring under control especially by an exertion of the will, or to reduce the intensity or degree of.&lt;/d-footnote&gt; them to the arbitrary and irregular commands of those who have [property]; there it presently becomes&amp;nbsp;tyranny&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Locke further clarifies that tyranny is &amp;#8220;the exercise of power beyond right&amp;#8221; and that &amp;#8220;where-ever law ends, tyranny begins.&amp;#8221; &lt;d-footnote&gt;I find this second quote particularly beautiful.&lt;/d-footnote&gt; The federal government of today is notorious in its exercise of power, with many claiming it is completely justified for the safety of the nation, and others firing back that in most cases these are gross abuses of this&amp;nbsp;power.&lt;/p&gt;
&lt;h1&gt;The Trump&amp;nbsp;Administration&lt;/h1&gt;
&lt;p&gt;Four years ago, the electoral college voted Donald Trump into the White House. Since then much controversy and conspiracy has penetrated the American public. I am curious how Locke&amp;#8217;s principles would apply to the Trump Administration as a democratic government. Of course, modern America is vastly different from the monarchies and colonial revolutions with which Locke was accustomed, so in some regards it&amp;#8217;s moot to even attempt the comparison. Nonetheless, many might argue that the Trump Administration has acted on numerous occasions with what Locke might call &amp;#8220;arbitrary&amp;#8221;&amp;nbsp;power.&lt;/p&gt;
&lt;p&gt;For example, let&amp;#8217;s dive straight into &lt;a href="https://apnews.com/article/law-and-order-violence-politics-oregon-racial-injustice-b35e37208e261278ecc33154c37d5d85" target="_blank"&gt;the federal deployment of heavily armed agents to protesting areas across the nation&lt;/a&gt;. Due to the killing of George Floyd, protests and calls for change began in almost all major cities in the country. Some of these turned violent, with the property rights of some being infringed upon. Therefore, it would seem reasonable, if not proper, for federal agents to step in to quell the violence and reinstate the right and privilege of citizens to preserve their&amp;nbsp;property.&lt;/p&gt;
&lt;p&gt;&lt;img class="uk-align-center" width="80%" height="" src="/images/portland-protestor.jpg" alt="A protester throws a tear gas canister back at police during a Black Lives Matter protest in Portland, Oregon." uk-img&gt;&lt;/p&gt;
&lt;div class="caption"&gt;
    &lt;strong&gt;Figure 2.&lt;/strong&gt; A protester throws a tear gas canister back at police during a Black Lives Matter protest in Portland, Oregon. Courtesy &lt;a href="https://unsplash.com/@titofoto?utm_source=unsplash&amp;amp;utm_medium=referral&amp;amp;utm_content=creditCopyText" target="_blank"&gt;Tito Texidor &lt;span class="caps"&gt;III&lt;/span&gt;&lt;/a&gt; on &lt;a href="https://unsplash.com/s/photos/%22tear-gas%22?utm_source=unsplash&amp;amp;utm_medium=referral&amp;amp;utm_content=creditCopyText" target="_blank"&gt;Unsplash&lt;/a&gt;
&lt;/div&gt;

&lt;p&gt;However, &lt;a href="https://www.nbcnews.com/politics/politics-news/does-trump-have-right-send-federal-agents-quell-violent-protests-n1235087" target="_blank"&gt;many argue&lt;/a&gt; that the deployed federal agents have exceeded their power, for example, by detaining demonstrators without probable cause. Moreover, the agents have &amp;#8220;responded with huge plumes of tear gas, rubber bullets and flash bang grenades,&amp;#8221; according to &lt;a href="https://apnews.com/article/b57315d97dd2146c4a89b4636faa7b70" target="_blank"&gt;&lt;span class="caps"&gt;AP&lt;/span&gt; News&lt;/a&gt;. To what extent is this use of force justified? Is this an unnecessary display of power, with the intent to &amp;#8220;harass&amp;#8221; or otherwise &amp;#8220;subdue&amp;#8221; the civilian, beyond what&amp;#8217;s necessary to protect the right to preserve one&amp;#8217;s&amp;nbsp;property?&lt;/p&gt;
&lt;p&gt;As another example, consider the language and rhetoric employed by the Trump Administration. As &lt;a href="https://www.brookings.edu/blog/fixgov/2020/05/01/destroying-trust-in-the-media-science-and-government-has-left-america-vulnerable-to-disaster/" target="_blank"&gt;the Brookings Institute&lt;/a&gt; put it, Trump has &amp;#8220;taken to a new level&amp;#8221; the discrediting of the media, science, and essential government agencies. His persistent attacks on (especially Left-leaning) media and news sources undoubtedly has many paradoxically questioning the veracity of factual reporting. With &lt;span class="caps"&gt;COVID&lt;/span&gt;-19 taking a devastating toll on not only the American public&amp;#8217;s physical health, but also mental health, it is difficult to understand why there is so much friction and derision toward the scientific community. Lastly, there has been constant denial and delay regarding the virus and the calls for racial justice, which has undermined the capabilities of local and state governments. How has this tactical (or accidental) behavior eroded the trust between citizens and their federal government? Does such behavior protect society? Does it intellectually subdue the&amp;nbsp;people?&lt;/p&gt;
&lt;p&gt;On the contrary, it seems as though many of the legislative changes enacted under the Trump Administration have revolved around foreign affairs, which I believe is slightly beyond the scope of Locke&amp;#8217;s principles. For example, &lt;a href="https://en.wikipedia.org/wiki/Executive_Order_13769" target="_blank"&gt;Executive Order 13769&lt;/a&gt; was a big controversy&lt;d-footnote&gt;From the &lt;a href="https://www.csmonitor.com/USA/Politics/2017/0206/Trump-s-biggest-executive-actions-explained/Travel-ban-for-immigrants-refugees-Jan.-27-2017" target="_blank"&gt;Christian Science Monitor&lt;/a&gt;: &amp;#8220;Executive Order: Protecting the Nation from Foreign Terrorist Entry into the United States suspends visa issuance as well as immigration entry to ânationals of countries of particular concernsâ for 90 days. The Trump administration applied this order to seven predominantly Muslim countries: Iraq, Iran, Syria, Yemen, Sudan, Somalia, and Libya. The order also suspends all refugee admission to the &lt;span class="caps"&gt;US&lt;/span&gt; for 120 days and indefinitely denies entry to all Syrian refugees.&amp;#8221;&lt;/d-footnote&gt;, but I&amp;#8217;m not sure Locke would really argue against such drastic (and ill-motivated) measures in the name of protecting society. One might rebut that this Executive Order was in fact a form of harassment against existing Muslim citizens. I&amp;#8217;ll leave it to you to judge how Locke would&amp;nbsp;stand.&lt;/p&gt;
&lt;h1&gt;Implications&lt;/h1&gt;
&lt;p&gt;Knowing what we do about Locke and a small sample of the Trump Administration&amp;#8217;s actions, let&amp;#8217;s explore whether such actions meet Locke&amp;#8217;s standard of&amp;nbsp;tyranny.&lt;/p&gt;
&lt;p&gt;According to &lt;a href="https://www.pewresearch.org/politics/2020/08/06/views-of-covid-19-response-by-trump-hospitals-cdc-and-other-officials/#trump-approval-lower-than-in-march-virtually-unchanged-since-june" target="_blank"&gt;Pew Research&lt;/a&gt;, about 59% of Americans disapprove of Trump&amp;#8217;s performance as president. There are many reasons one might disapprove of any president&amp;#8217;s job, but based on the evidence above, a strong reason to disapprove of Trump&amp;#8217;s would be his unjustified harassing and subduing of the American people. The &lt;em&gt;Second Treatise&lt;/em&gt; is notoriously ambiguous, including around its terms of harassment, subduing, and impoverishment; however, it is not unlikely that some of the president&amp;#8217;s actions might be labeled as Locke described. If we agree that some (is &amp;#8220;some&amp;#8221; all that is necessary?) of Trump&amp;#8217;s behavior has subdued the rights of citizens or harassed their well-being, then it would be fair to label these actions as&amp;nbsp;tyranny.&lt;/p&gt;
&lt;p&gt;The question then becomes, are a few tyrannical actions indicative of an entire government marked by&amp;nbsp;tyranny?&lt;/p&gt;
&lt;p&gt;Suppose the Trump Administration &lt;em&gt;is&lt;/em&gt; marked by tyranny, what then? Well, as Locke puts&amp;nbsp;it:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The people generally ill treated&amp;#8230; will be ready upon any occasion to ease themselves of a burden that sits heavy upon&amp;nbsp;them.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;He continues a short while after that to&amp;nbsp;claim,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;if a long train of abuses, prevarications&lt;d-footnote&gt;meaning: to deviate from the truth&lt;/d-footnote&gt; and artifices, all tending the same way, make [the government&amp;#8217;s intentions] visible to the people&amp;#8230; it is not to be wondered, that they should then rouze themselves, and endeavour to put the rule into such hands which may secure to them the ends for which government was at first&amp;nbsp;erected&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Is it time for the unsatisfied American people to rise up and put an end to the current government, in order to re-establish a better one? To be clear, Locke made the distinction between just government&amp;#8212;-which seeks to preserve the life, liberty, and property of its citizens&amp;#8212;-and unjust government&amp;#8212;-which systematically violates the citizens&amp;#8217; natural rights. And to put it simply, Locke also justified rebellion under the&amp;nbsp;latter.&lt;/p&gt;
&lt;p&gt;Regardless, modern &lt;span class="caps"&gt;US&lt;/span&gt; government was not orchestrated exactly by Locke&amp;#8217;s theories, and we are not governed by laws which he himself wrote. Thus, it seems farcical to suggest anyone overthrow the government. That would be weighty. However, one should not refrain from judging modern government to the principles upon which its&amp;nbsp;stands.&lt;/p&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;John Locke, unlike any other, built the proto-foundation of America. He had a clear stance against authoritarianism and arbitrary power. Today&amp;#8217;s federal government has caused a stir in the American public, perhaps unlike any other in the last 50 years. Locke defined how governments could fail their people and come to a dissolution. The Trump Administration, on several occasions, has acted in ways that hark back to Locke&amp;#8217;s definition of tyranny. Locke defined rebels as those who &amp;#8220;by force justify their violation&amp;#8221; of the &amp;#8220;constitution and the laws of the government.&amp;#8221; Is it time to rebel, to &amp;#8220;bring back again the state of&amp;nbsp;war&amp;#8221;?&lt;/p&gt;
&lt;p&gt;Let me know on &lt;a href="https://twitter.com/alexliebscher0" target="_blank"&gt;Twitter&lt;/a&gt; or by &lt;a href="mailto:alexliebscher0@gmail.com"&gt;email&lt;/a&gt; what you think of my analogy drawn between Locke&amp;#8217;s definition of tyranny and the Trump&amp;nbsp;Administration!&lt;/p&gt;</content><category term="meta"></category><category term="society"></category><category term="philosophy"></category><category term="critique"></category></entry><entry><title>Review: Evaluating Neural Toxic Degeneration in LanguageÂ Models</title><link href="/posts/2020/Oct/toxic-degeneration/" rel="alternate"></link><published>2020-10-17T00:00:00-07:00</published><updated>2020-10-17T00:00:00-07:00</updated><author><name>Alex Liebscher</name></author><id>tag:None,2020-10-17:/posts/2020/Oct/toxic-degeneration/</id><summary type="html">&lt;p&gt;Language Models suffer from degenerate and biased behavior, can we fix&amp;nbsp;that?&lt;/p&gt;</summary><content type="html">&lt;p&gt;This review is on &amp;#8220;&lt;a href="https://api.semanticscholar.org/CorpusID:221878771"&gt;RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models&lt;/a&gt;&amp;#8221; from a group of researchers at the University of Washington and the Allen Institute for &lt;span class="caps"&gt;AI&lt;/span&gt;, to be found in the &lt;em&gt;Findings of &lt;span class="caps"&gt;EMNLP&lt;/span&gt; 2020&lt;/em&gt; &lt;d-footnote&gt;&lt;a href="https://2020.emnlp.org/blog/2020-04-19-findings-of-emnlp" target="_blank"&gt;A new (as of April 2020) &amp;#8220;companion&amp;#8221; to &lt;span class="caps"&gt;EMNLP&lt;/span&gt;&lt;/a&gt; which offers sort of a middle group between rejection and acceptance into the main conference. It seems like a happy middle ground for still notable research.&lt;/d-footnote&gt;. The authors set the stage by&amp;nbsp;saying,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;language models (LMs) pretrained on large web text corpora suffer from degenerate and biased&amp;nbsp;behavior&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To no one&amp;#8217;s surprise, LMs are being deployed at an increasing pace due to the popularity of tools such as &lt;a href="https://huggingface.co/" target="_blank"&gt;huggingface&lt;/a&gt;. The authors contribute to recent work on LMs by first offering an operationalization of toxic model generations, then introduce a novel dataset of labeled toxic and non-toxic phrases, and then demonstrate the danger of LMs (even when tuned to avoid toxic language). The authors evaluate a few different methods for preventing toxic language generation, although none is perfect. Lastly, they partially diagnose what&amp;#8217;s causing bad generations (hint: garbage in, garbage out), and offer some recommendations to ameliorate the&amp;nbsp;issue.&lt;/p&gt;
&lt;p&gt;I was interested in this paper because it is one of those that integrates both &lt;span class="caps"&gt;NLP&lt;/span&gt; and sociolinguistics. Before even trying to prevent a &lt;span class="caps"&gt;LM&lt;/span&gt; from spitting forth profanity, one has to define and operationalize some very tricky subjects. What exactly is profanity? What is toxicity? At what point does a word move from being uncomfortable or taboo to profane or marginalizing? How does the definition of toxic change from person to person? Can something be profane when said by one person, but not another? Only one, maybe two, of these is discussed in this article, but I found them useful to consider while&amp;nbsp;reading.&lt;/p&gt;
&lt;h2&gt;Language Model&amp;nbsp;Background&lt;/h2&gt;
&lt;p&gt;Before beginning, I&amp;#8217;ll give a bit of background on generative Language Models. In plain English, a Language Model scans huge collections of documents (millions of documents), word by word, learning statistical associations between words and their neighbors, and is then able to predict the next word in a phrase by just looking for the most probable in the English language. Your iPhone does this (if you have predictive typing turned on), as does Gmail when you&amp;#8217;re drafting an email, and a suite of other&amp;nbsp;tools.&lt;/p&gt;
&lt;p&gt;You may have heard of &lt;span class="caps"&gt;GPT&lt;/span&gt;-3 (and &lt;a href="https://openai.com/blog/gpt-2-1-5b-release/" target="_blank"&gt;it&amp;#8217;s predecessors&lt;/a&gt;)&lt;d-footnote&gt;For an excellent illustrated explanation, I recommend &lt;a href="http://jalammar.github.io/illustrated-gpt2" target="_blank"&gt;the classic from Jay Alammar&lt;/a&gt;.&lt;/d-footnote&gt;, which has been &lt;a href="https://www.theverge.com/21346343/gpt-3-explainer-openai-examples-errors-agi-potential" target="_blank"&gt;in&lt;/a&gt; &lt;a href="https://www.technologyreview.com/2020/07/20/1005454/openai-machine-learning-language-generator-gpt-3-nlp/" target="_blank"&gt;the&lt;/a&gt; &lt;a href="https://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3" target="_blank"&gt;news&lt;/a&gt; &lt;a href="https://www.vox.com/future-perfect/21355768/gpt-3-ai-openai-turing-test-language"&gt;a lot&lt;/a&gt;, or of &lt;a href="https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270" target="_blank"&gt;&lt;span class="caps"&gt;BERT&lt;/span&gt;&lt;/a&gt;, which might have received more attention in the academic press than the public press. The &lt;span class="caps"&gt;GPT&lt;/span&gt; family and &lt;span class="caps"&gt;BERT&lt;/span&gt; are both Language Models. &lt;span class="caps"&gt;GPT&lt;/span&gt; is distinct from &lt;span class="caps"&gt;BERT&lt;/span&gt; though in that it is &lt;em&gt;auto-regressive&lt;/em&gt;, meaning it progressively sees and trains on more of the sentence as it reads, instead of seeing the whole sentence at once. This is what makes it particular useful for predicting what will comes next, making it &lt;em&gt;generative&lt;/em&gt;.&lt;/p&gt;
&lt;h2&gt;Operationalizing&amp;nbsp;Toxicity&lt;/h2&gt;
&lt;p&gt;The authors state the importance of having well-annotated data, and that the definition of &amp;#8220;toxic&amp;#8221; is crucial to their argument, although they pass off the responsibility to another tool. They do correctly note that hand-annotating such a large dataset is unfeasible. In the&amp;nbsp;end,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We rely on &lt;a href="https://www.perspectiveapi.com/" target="_blank"&gt;Perspective &lt;span class="caps"&gt;API&lt;/span&gt;&lt;/a&gt;, an automated tool for toxic language and hate speech&amp;nbsp;detection&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The &lt;span class="caps"&gt;API&lt;/span&gt;, when given a linguistic form (e.g., a sentence), calculates a score, or probability of toxicity. The authors then (perhaps unjustifiably) label an input prompt as toxic if it has a score of over 0.5, otherwise it&amp;#8217;s labeled non-toxic. The authors confess that the &lt;span class="caps"&gt;API&lt;/span&gt; likely suffers from an over-reliance on lexical cues to detect toxicity, where lexical cues basically just means specific words, such as swear words or&amp;nbsp;slurs.&lt;/p&gt;
&lt;h2&gt;&lt;em&gt;RealToxicityPrompts&lt;/em&gt;&amp;nbsp;Dataset&lt;/h2&gt;
&lt;p&gt;One of the main contributions of the paper is the dataset the authors compiled. It consists of 100,000 sentences, spanning the whole range of toxicities, that were split in half into a &lt;em&gt;prompt&lt;/em&gt; and a &lt;em&gt;continuation&lt;/em&gt;. Both halves were scored for toxicity. Their hope is that by creating this big dataset of English web text scored for toxicity, others will be able to &amp;#8220;systematically evaluate and compare the generations from language models&amp;#8221; with this &amp;#8220;testbed for&amp;nbsp;toxicity.&amp;#8221;&lt;/p&gt;
&lt;h2&gt;Detoxifying Model&amp;nbsp;Generations&lt;/h2&gt;
&lt;p&gt;&lt;img class="uk-align-center" data-src="/images/toxicity-generations.png" height="" width="50%" alt="" uk-img&gt;&lt;/p&gt;
&lt;div class="caption"&gt;
    &lt;strong&gt;Figure 1.&lt;/strong&gt; Four non-toxic prompts which cause several pretrained LMs to generate highly toxic output. Credit: Gehman et al. (2020)
&lt;/div&gt;

&lt;p&gt;As illustrated in Figure 1, even non-toxic prompts (those with a toxicity score from Perspective less than 0.5) lead to toxic generations. The crux of the paper is in their assessment of five ways to detoxify &lt;span class="caps"&gt;LM&lt;/span&gt; generations. Below are quick summaries of these methods and how they reduce toxic&amp;nbsp;generations.&lt;/p&gt;
&lt;h3&gt;Data-Based&amp;nbsp;Detoxification&lt;/h3&gt;
&lt;dl&gt;
&lt;dt&gt;Domain-Adaptive Pretraining (&lt;span class="caps"&gt;DAPT&lt;/span&gt;)&lt;/dt&gt;
&lt;dd&gt;Perform additional pretraining on a smaller &lt;em&gt;non-toxic&lt;/em&gt;&amp;nbsp;corpus.&lt;/dd&gt;
&lt;dt&gt;Attribute&amp;nbsp;Conditioning&lt;/dt&gt;
&lt;dd&gt;Perform additional pretraining on a sample of the corpus which has been labeled with either a toxic attribute token, &lt;code&gt;&amp;lt;|toxic|&amp;gt;&lt;/code&gt;, or a non-toxic attribute token, &lt;code&gt;&amp;lt;|nontoxic|&amp;gt;&lt;/code&gt;. During generation, prepend the prompt with &lt;code&gt;&amp;lt;|nontoxic|&amp;gt;&lt;/code&gt; specifically.&lt;/dd&gt;
&lt;/dl&gt;
&lt;h3&gt;Decoding-Based&amp;nbsp;Detoxification&lt;/h3&gt;
&lt;dl&gt;
&lt;dt&gt;Vocabulary&amp;nbsp;Shifting&lt;/dt&gt;
&lt;dd&gt;Modify the decoding algorithm to give higher probability to non-toxic tokens. Learns a weight for each vocabulary token which represents the association between each token and&amp;nbsp;(non-)toxicity.&lt;/dd&gt;
&lt;dt&gt;Word&amp;nbsp;Filtering&lt;/dt&gt;
&lt;dd&gt;Create a blacklist of profanity, slurs, and swearwords and set the probability of generating those words to zero to prevent the LMs from generating them&amp;nbsp;altogether.&lt;/dd&gt;
&lt;dt&gt;&lt;a href="https://openreview.net/pdf?id=H1edEyBKDS" target="_blank"&gt;&lt;span class="caps"&gt;PPLM&lt;/span&gt;&lt;/a&gt;&lt;/dt&gt;
&lt;dd&gt;Combine a pretrained, unconditional language model with one or more simple attribute models, which are small and cheap models that figure out latent gradients between attributes and the pretrained language models to steer&amp;nbsp;outputs.&lt;/dd&gt;
&lt;/dl&gt;
&lt;h3&gt;Results&lt;/h3&gt;
&lt;p&gt;As one might expect, none of these detoxification methods completely eradicated toxic generations; and toxic prompts yielded higher toxicity in generation than non-toxic prompts. What&amp;#8217;s somewhat surprising is that non-toxic prompts occasionally also led to toxic generations. It should be clear then that even benign models can be harmful. The authors point out that it&amp;#8217;s also surprising that the &lt;span class="caps"&gt;CTRL&lt;/span&gt;-&lt;span class="caps"&gt;WIKI&lt;/span&gt; model produced toxic content, despite being pretrained to generate Wikipedia-style output&lt;d-footnote&gt;The authors also state it&amp;#8217;s interesting since &amp;#8220;[the model] was trained on just Wikipedia.&amp;#8221; I&amp;#8217;m not sure this statement is true after skimming the original &lt;span class="caps"&gt;CTRL&lt;/span&gt; paper from Kaskar et al. (2019).&lt;/d-footnote&gt;. Furthermore, &lt;span class="caps"&gt;DAPT&lt;/span&gt; is one of the most effective methods (which I found surprising, given its simplicity), along with vocabulary shifting, and &lt;span class="caps"&gt;PPLM&lt;/span&gt;. The authors lastly state that steering generations after pretraining must thus be crucial for preventing toxic&amp;nbsp;output.&lt;/p&gt;
&lt;p&gt;There were some phrases, which when used to prompt the models consistently returned toxic generations. These prompts were often toxic themselves or had the opening phrase of known toxic sentences (&amp;#8220;his rant was full of&amp;#8230;&amp;#8221;). Lastly, about 10% of the prompts came from unreliable or toxic data sources, which we get into&amp;nbsp;next.&lt;/p&gt;
&lt;h2&gt;Analysis of Web Text&amp;nbsp;Toxicity&lt;/h2&gt;
&lt;p&gt;&lt;img class="uk-align-center" data-src="/images/toxicity-corpora.png" height="" width="40%" alt="" uk-img&gt;&lt;/p&gt;
&lt;div class="caption"&gt;
&lt;strong&gt;Figure 2.&lt;/strong&gt; About 2.1% and 4.3% of &lt;span class="caps"&gt;OWTC&lt;/span&gt; and OpenAI-&lt;span class="caps"&gt;WT&lt;/span&gt; (the corpora which the x and x models were trained on) are toxic content (note: log-transformed y-axis). Credit: Gehman et al. (2020)
&lt;/div&gt;

&lt;p&gt;Up next, the authors attempt to quantify the toxicity present in the training data for the LMs they tested. Specifically, the training data were OpenAI-&lt;span class="caps"&gt;WT&lt;/span&gt; (&lt;span class="caps"&gt;GPT&lt;/span&gt;-2&amp;#8217;s training data) and its open-source replica &lt;span class="caps"&gt;OWTC&lt;/span&gt;. There is about 29% overlap between the two datasets, which is low enough to come to interesting conclusions about how the models may be affected by their training&amp;nbsp;data.&lt;/p&gt;
&lt;p&gt;In Figure 2 (note: it&amp;#8217;s log-transformed), we can see that about 2.1% and 4.3% of &lt;span class="caps"&gt;OWTC&lt;/span&gt; and OpenAI-&lt;span class="caps"&gt;WT&lt;/span&gt; are toxic content. About 3% of &lt;span class="caps"&gt;OWTC&lt;/span&gt; in fact comes from links shared on banned or quarantined subreddits&lt;d-footnote&gt;Banned subreddits are inaccessible via the website and only via data dumps, whereas quarantined subreddits are special-access only but still online.&lt;/d-footnote&gt;.&lt;/p&gt;
&lt;h2&gt;Recommendations&lt;/h2&gt;
&lt;p&gt;The authors conclude with a discussion and a variety of recommendations for &lt;span class="caps"&gt;NLP&lt;/span&gt; researchers. First, it should seem apparent that there&amp;#8217;s an issue with LMs generating toxic content. From the analysis of the training corpora, we might guess that this degeneracy comes from poor training data. They concede that the steering methods did have some positive effect, but did not completely resolve the problem. There are three primary implications&amp;nbsp;proposed:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="dquo"&gt;&amp;#8220;&lt;/span&gt;Can language models ever fully &amp;#8216;forget&amp;#8217; toxic pretraining data through further&amp;nbsp;adaptation?&amp;#8221;&lt;/p&gt;
&lt;p&gt;Essentially, you can start with a model trained on some bad data, but is there any amount of finite data you could then continue pretraining on to wash out, to &amp;#8220;forget&amp;#8221;, the bad stuff? It seems as though the LMs are &amp;#8220;memorizing&amp;#8221; the bad content, which might come from such content being more salient to the model. The authors recommend for future work to explore whether some types of toxicity are more difficult for models to&amp;nbsp;forget.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Purposeful&amp;nbsp;decoding&lt;/p&gt;
&lt;p&gt;One of the most promising methods of eliminating toxic generations was &lt;span class="caps"&gt;PPLM&lt;/span&gt;. Perhaps there exist other methods to aid in the decoding phase and prevent toxic generation. For example, using handpicked toxic documents as &amp;#8220;negative examples&amp;#8221; which the model would learn not to produce. The authors also nebulously suggest &amp;#8220;infusing models with more sophisticated or nuanced representations of social&amp;nbsp;biases.&amp;#8221;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Choice of Pretraining&amp;nbsp;Data&lt;/p&gt;
&lt;p&gt;The authors also call for a dramatic introspection of the training data with which LMs learn on. Some issues arise when relying on huge swathes of data with very little filtering e.g., Reddit is known to have a biased user-base. This should call to question: &amp;#8220;who decides whose voices are going to be learned by the language model.&amp;#8221; One appealing recommendation is to make the pretraining process more human-centered, including through participatory&amp;nbsp;design.&lt;/p&gt;
&lt;p&gt;The authors lastly caution that curating pretraining data without deep thought might have unintended side-effects, such as filtering out benign text from African American authors/users. They suggest engaging with the end-user during this&amp;nbsp;phase.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Take-aways&lt;/h2&gt;
&lt;p&gt;It&amp;#8217;s a compelling and mostly uncontroversial article, which clearly highlights shortcomings currently in &lt;span class="caps"&gt;NLP&lt;/span&gt;. I think too little is said about the impact on industry (companies are currently trying to solve this problem), and society as a whole. In fact, regarding the latter, I think more could have been said about the practical implications of &lt;span class="caps"&gt;LM&lt;/span&gt; degeneration on the spreading of misinformation. In sum though, the authors created a valuable dataset for future research, and exposed a lower bound on toxicity degeneracy in&amp;nbsp;LMs.&lt;/p&gt;
&lt;p&gt;I&amp;#8217;m also a little unsatisfied by the authors&amp;#8217; operationalization of toxicity. While I understand their desire to create a large dataset for other researchers to train on, I&amp;#8217;m not sure the Perspective &lt;span class="caps"&gt;API&lt;/span&gt; should be considered the end of the conversation. Undoubtedly, it will produce false positives and false negatives (which the authors briefly mentioned), which should lead us to question if their assessments are answering the right questions. If their ground truth is not what a human would believe, then this could easily disrupt their main&amp;nbsp;claims.&lt;/p&gt;
&lt;p&gt;Lastly, on the topic of what humans would believe: some people find some things offensive, others don&amp;#8217;t. Are the authors (or by proxy, Perspective) justified in claiming what is toxic and what is not? More philosophically, can someone speak for someone else on what is right or&amp;nbsp;wrong?&lt;/p&gt;</content><category term="meta"></category><category term="machine learning"></category><category term="nlp"></category><category term="society"></category></entry><entry><title>BlogÂ Origins</title><link href="/posts/2020/Oct/blog-origins/" rel="alternate"></link><published>2020-10-14T00:00:00-07:00</published><updated>2020-10-14T00:00:00-07:00</updated><author><name>Alex Liebscher</name></author><id>tag:None,2020-10-14:/posts/2020/Oct/blog-origins/</id><summary type="html">&lt;p&gt;How this blog came to&amp;nbsp;be&lt;/p&gt;</summary><content type="html">&lt;p&gt;The blog title and subtitle might not sounds&amp;#8230; correct. Your gut is correct, they were not written by me. They were &lt;a href="https://transformer.huggingface.co/doc/gpt2-large" target="_blank"&gt;composed by &lt;span class="caps"&gt;GPT&lt;/span&gt;-2&lt;/a&gt; according to a&amp;nbsp;prompt:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/blog-origins.png" uk-img&gt;&lt;/p&gt;
&lt;p&gt;I cherry-picked a little to find something which somewhat made sense and reflected a general aura of intelligence, and then made one character correction. Please excuse the typo in my&amp;nbsp;prompt.&lt;/p&gt;</content><category term="meta"></category><category term="blogging"></category></entry><entry><title>Setting up AWS IAM</title><link href="/posts/2020/Jun/setting-up-aws-iam/" rel="alternate"></link><published>2020-06-20T00:00:00-07:00</published><updated>2020-06-20T00:00:00-07:00</updated><author><name>Alex Liebscher</name></author><id>tag:None,2020-06-20:/posts/2020/Jun/setting-up-aws-iam/</id><summary type="html">&lt;p&gt;A quick introduction to &lt;span class="caps"&gt;AWS&lt;/span&gt; Identity and Access&amp;nbsp;Management&lt;/p&gt;</summary><content type="html">&lt;p&gt;Cloud computing and storage services are becoming an essential for many businesses. As a more versatile and easier to scale option than on-premise solutions, their demand has noticeably increased. As businesses look toward these solutions, developers must grow and adapt. Students or professionals without much &lt;span class="caps"&gt;AWS&lt;/span&gt; experience can broaden their skillset by playing around with these platforms. Luckily for this demographic (myself included), the three main cloud providers&amp;#8212;-Amazon Web Services (&lt;span class="caps"&gt;AWS&lt;/span&gt;), Google Cloud Platform (&lt;span class="caps"&gt;GCP&lt;/span&gt;), and Microsoft Azure&amp;#8212;-all offer free tiers to get started learning and&amp;nbsp;experimenting.&lt;/p&gt;
&lt;p&gt;These services are much different than whatâs learned in the sometimes archaic education system, so they can be confusing and easy to get wrong when learning. The goal for this article is to demonstrate how to set up a personal-use Amazon Web Services account according to some best practices for Identity and Access Management (&lt;span class="caps"&gt;IAM&lt;/span&gt;). This will be targeted at students and professionals without much &lt;span class="caps"&gt;AWS&lt;/span&gt; experience, and should be about a 10 minute read (although youâre encouraged to try things out yourself). Learning how to properly setup and authenticate your work in the cloud is important for three reasons: 1) to mitigate any opportunities to compromise yourself, your work, your billing information, or other sensitive data, 2) to mitigate any opportunities to mess up and get frustrated, and 3) itâs always good to follow best&amp;nbsp;practices!&lt;/p&gt;
&lt;p&gt;If you&amp;#8217;re just getting started working with cloud services or are looking to begin, I think this article presents a good first or second step. I recommend following the tips at the bottom as soon as you feel&amp;nbsp;ready!&lt;/p&gt;
&lt;p&gt;&lt;img class="uk-align-center" data-src="/images/iam_workflow.png" width="80%" alt="" uk-img&gt;&lt;/p&gt;
&lt;div class="caption"&gt;
An example workflow of attributing permissions to an Administrator user on &lt;span class="caps"&gt;AWS&lt;/span&gt;. Source: &lt;a href="https://wellarchitectedlabs.com/security/100_labs/100_basic_identity_and_access_management_user_group_role/1_iam/" target="_blank"&gt;&lt;span class="caps"&gt;AWS&lt;/span&gt; Well-Architected Labs&lt;/a&gt;
&lt;/div&gt;

&lt;h2&gt;Terminology&lt;/h2&gt;
&lt;p&gt;For me, one of the most difficult aspects of learning how to use cloud services was hurdling over the increase in vocabulary. Many terms are polysemous with other developer terms and appear to be drawn together from a variety of computing subdisciplines. Quickly, I&amp;#8217;ll introduce some key terms which you should be familiar with as you endeavour into&amp;nbsp;space.&lt;/p&gt;
&lt;dl class="uk-description-list"&gt;
    &lt;dt&gt;&lt;span class="caps"&gt;IAM&lt;/span&gt;&lt;/dt&gt;
    &lt;dd&gt;The Identity and Access Management section of &lt;span class="caps"&gt;AWS&lt;/span&gt; allows one to control the degree to which different people and services can access parts of the &lt;span class="caps"&gt;AWS&lt;/span&gt; account. It&amp;#8217;s how permissions are set and security&amp;nbsp;tightened.&lt;/dd&gt;
    &lt;dt&gt;Root&lt;/dt&gt;
    &lt;dd&gt;In a hierarchy of control, the root users has access to everything. It&amp;#8217;s like using `sudo` in a shell program. We&amp;#8217;ll cover this soon, but it&amp;#8217;s strongly discouraged to use this login on a daily basis due to the power it holds. For example, under this user you can control the accounts billing information and payment&amp;nbsp;methods.&lt;/dd&gt;
    &lt;dt&gt;User&lt;/dt&gt;
    &lt;dd&gt;A type of individual with less power than the root user. These can be granted unique login credentials. Some common users would be Administrators and Machine Learning Engineers â obviously the MLEs should have less power than the full admins, and by assigning people to user accounts, that can be&amp;nbsp;controlled.&lt;/dd&gt;
    &lt;dt&gt;Group&lt;/dt&gt;
    &lt;dd&gt;A collection of users with a pre-specified set of permissions. These are useful because it&amp;#8217;s easier to manage a collection of users rather than manually going through a list of users to change their individual permissions. Using groups is simply a way to attach permissions to multiple users at one time. A common example is an Administrators group, which can have multiple admins all with the same&amp;nbsp;permissions.&lt;/dd&gt;
    &lt;dt&gt;Role&lt;/dt&gt;
    &lt;dd&gt;A collection of policies/permissions assigned to one or more users when needed. The key here is that, separately from groups, roles provide temporary security. These are useful if you want to, for example, provide credentials to an external group for auditing purposes, or to assign a set of permissions to an &lt;span class="caps"&gt;AWS&lt;/span&gt; service that allow it access another &lt;span class="caps"&gt;AWS&lt;/span&gt;&amp;nbsp;service.&lt;/dd&gt;
    &lt;dt&gt;Policy&lt;/dt&gt;
    &lt;dd&gt;A policy is a marker that defines permissions. These can get complicated, so &lt;a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html" target="_blank"&gt;here&amp;#8217;s a link to the in-depth explanation&lt;/a&gt;. For example though, one might create a group called `MainTableAccess` and assign some users to it, and the policy attached to this group only &lt;a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_dynamodb_specific-table.html" target="_blank"&gt;allows those in the group to access a specific table&lt;/a&gt; in a DynamoDB&amp;nbsp;database.&lt;/dd&gt;
    &lt;dt&gt;Service&lt;/dt&gt;
    &lt;dd&gt;One of the many subplatforms on &lt;span class="caps"&gt;AWS&lt;/span&gt; that can cause functionality, such as Lambda or &lt;span class="caps"&gt;EC2&lt;/span&gt;.&lt;/dd&gt;
&lt;/dl&gt;

&lt;h2&gt;Quick&amp;nbsp;Setup&lt;/h2&gt;
&lt;h3&gt;Tip&amp;nbsp;#1&lt;/h3&gt;
&lt;p&gt;The first key to securing your &lt;span class="caps"&gt;AWS&lt;/span&gt; account and ensuring your permissions won&amp;#8217;t allow you or others to intentionally, or otherwise, compromise your account, is to follow the &lt;a href="https://en.wikipedia.org/wiki/Principle_of_least_privilege" target="_blank"&gt;least privileges model&lt;/a&gt;. Under this mindset, we only assign permissions as they&amp;#8217;re needed. This ensures no user is too privileged, to the point where they may accidentally harm the&amp;nbsp;account.&lt;/p&gt;
&lt;p&gt;Most importantly, make sure that you set up a non-root user. You&amp;#8217;ll almost never want to use the root user. Instead, here I&amp;#8217;ll guide you to setting up an Administrator user. Administrator accounts don&amp;#8217;t get to access things like billing information and cost management, although they can do things like manage users and groups which less powerful users&amp;nbsp;cannot.&lt;/p&gt;
&lt;p&gt;To add an administrator user to your personal &lt;span class="caps"&gt;AWS&lt;/span&gt; account, navigate to your &lt;span class="caps"&gt;AWS&lt;/span&gt; home, where you&amp;#8217;ll see an option to add a user. Name your new user &lt;code&gt;Administrator&lt;/code&gt; or something like it. I assume you&amp;#8217;re working through the console, but may later want to set up the &lt;span class="caps"&gt;AWS&lt;/span&gt; command line tools, so check off programmatic and &lt;span class="caps"&gt;AWS&lt;/span&gt; Management Console access. Choose to set a password later (if you set one now you&amp;#8217;ll still have to reset it later). We&amp;#8217;ll create a new group of users called &lt;code&gt;Administrators&lt;/code&gt; and select the &lt;code&gt;AdministratorAccess&lt;/code&gt; policy. Skip the Tags section and review what you&amp;#8217;ve done. There will be a link in the success box which, upon clicking, will log out the root user and prompt you to log in under a new user. Your username is &lt;code&gt;Administrator&lt;/code&gt; (or whatever you chose) and the password was that which was&amp;nbsp;generated.&lt;/p&gt;
&lt;h3&gt;Tip&amp;nbsp;#2&lt;/h3&gt;
&lt;p&gt;My second tip is to &lt;a href="https://aws.amazon.com/iam/features/mfa" target="_blank"&gt;set up Multi-Factor Authentication (&lt;span class="caps"&gt;MFA&lt;/span&gt;)&lt;/a&gt;. This is an added layer of protection from anyone unwanted accessing your account, which becomes more important your projects and work scale up. For students at &lt;span class="caps"&gt;UC&lt;/span&gt; San Diego, we&amp;#8217;ve become accustomed to using &lt;a href="https://duo.com" target="_blank"&gt;Duo Security&lt;/a&gt; as a two-factor authentication system. With Duo, when you log in with your username and password, you&amp;#8217;ll also be prompted for an authentication response within the&amp;nbsp;app.&lt;/p&gt;
&lt;p&gt;&lt;img class="uk-align-center" data-src="/images/aws_iam_mfa.png" height="" width="80%" alt="" uk-img&gt;&lt;/p&gt;
&lt;div class="caption"&gt;
Preview of setting up Multi-Factor Authentication in &lt;span class="caps"&gt;AWS&lt;/span&gt;. Using Duo is an easy solution and requires scanning a &lt;span class="caps"&gt;QR&lt;/span&gt; code and entering the corresponding authentication codes.
&lt;/div&gt;

&lt;p&gt;To set up &lt;span class="caps"&gt;MFA&lt;/span&gt;, navigate to your &lt;span class="caps"&gt;AWS&lt;/span&gt; &lt;span class="caps"&gt;IAM&lt;/span&gt; home, where there will be an accordion of security credential options. Select to activate &lt;span class="caps"&gt;MFA&lt;/span&gt;, and choose &lt;code&gt;Virtual MFA Device&lt;/code&gt;. Open Duo, scan the &lt;span class="caps"&gt;QR&lt;/span&gt; code, and enter in the authentication keys.&amp;nbsp;Done!&lt;/p&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;That&amp;#8217;s it for this post, but I encourage you go on and explore the &lt;span class="caps"&gt;IAM&lt;/span&gt; aspect of &lt;span class="caps"&gt;AWS&lt;/span&gt; further, especially in regards to the policies and roles that may be set. Once we begin setting up services like Lambda and DynamoDB, it will be important to only allow the bare minimum of what they need to&amp;nbsp;access.&lt;/p&gt;</content><category term="meta"></category><category term="cloud computing"></category></entry><entry><title>MusicÂ Collection</title><link href="/posts/2020/May/music-collection/" rel="alternate"></link><published>2020-05-13T00:00:00-07:00</published><updated>2020-05-13T00:00:00-07:00</updated><author><name>Alex Liebscher</name></author><id>tag:None,2020-05-13:/posts/2020/May/music-collection/</id><summary type="html">&lt;p&gt;How I developed an open-source music collection&amp;nbsp;application&lt;/p&gt;</summary><content type="html">&lt;!-- readtime: 15.1 --&gt;

&lt;p&gt;There is a serious problem with how people attempt to explain why they enjoy certain music. Listeners donât have the tools, or capacity, to accurately respond to questions like âWhat kind of music do you listen to?â or âWho are your favorite&amp;nbsp;bands?â.&lt;/p&gt;
&lt;p&gt;Most people are isolated within small social and geographic circles and the scale at which we miss out on good music is astronomical. Thereâs endless opportunity to discover new music that we might love, and endless opportunity to share what we find and bring joy to others. Almost every music streaming or purchasing platform these days has a recommendation engine, but none has ways to keep track of our thoughts about the music so we can figure out what we like and what we don&amp;#8217;t like. In fact, it seems most people donât have any systematic way to form relationships between musical entities. Over the last couple months, I&amp;#8217;ve endeavored to create a solution, and this article presents a web-based application I built to enable deeper reasoning about the music I&amp;nbsp;enjoy.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/comparison.png" alt="A screenshot of the music collection app while comparing two albums"&gt;&lt;/p&gt;
&lt;div class="caption"&gt;
    Upon loading the application, you&amp;#8217;re are given the option to search for albums within your collection, add new albums to your collection, or continue to score albums.
&lt;/div&gt;

&lt;p&gt;When I first began thinking of this problem, I wanted a way to methodically record my emotional and perceptual experiences with music so that I could have a better idea of what I like, what I donât like, and how to respond to a question like âWhatâs your favorite album of all time?â This matters to me because I love music and believe that the music I listen to plays an integral part in building my personality and defining who I am. Enjoying a wide-variety of music allows me to always have a fallback conversation point (although, it doesnât work when people donât have a strong connection to music), and Iâve found that people love talking about their tastes in music. You can learn a lot about a person by the musical qualities they&amp;nbsp;enjoy.&lt;/p&gt;
&lt;p&gt;The problem is: how can you quantify the way you think and feel toward music? Ultimately, Iâd like to be able to authentically and accurately answer: What are your top 3 albums? and perhaps, Who are your favorite musicians? Knowing my favorite genres would also be exciting. How, then, do we quantify, or at least document, our reactions to&amp;nbsp;records?&lt;/p&gt;
&lt;p&gt;These questions are based upon the assumption that thereâs a linear rank that albums could be lined up on. Humans are obsessed with ranking: sports teams ranked on &lt;span class="caps"&gt;ESPN&lt;/span&gt;, product reviews ranked on Amazon, universities rankings on &lt;span class="caps"&gt;US&lt;/span&gt; News. Rankings are intuitive, simple, and natural, therefore they show up prominently in this project. The question then becomes, How do we rank something subjective and aesthetic, like&amp;nbsp;music?&lt;/p&gt;
&lt;p&gt;The study of aesthetics has a long history, and volumes of philosophical arguments and perspectives. Immanuel Kant was one of the most invested spokespeople of aesthetics, arguing that aesthetic judgement is rooted in neither scientific knowledge nor is it bound by rules of understanding. It is the thoughts surrounding individual objects, within and around themselves alone, and in the light of the sensory experiences they generate. He argues that aesthetic judgement cannot be supported by objective or universal principles, for a judgment of taste is always&amp;nbsp;individual.&lt;/p&gt;
&lt;p&gt;With that in mind, it makes sense then to forget about making a multi-user platform: everyone has a different idea of whatâs best, and the music charts already exist as an attempt to aggregate reviews over many people. Instead, we need a per-individual&amp;nbsp;solution.&lt;/p&gt;
&lt;p&gt;A way to organize the collection of music Iâve invested time into is important, both for record keeping (to refresh my limited memory when needed) and for using to rank music. Thus, I developed a system to log my music collection with all the features I need to informatively rank albums. Based upon lots of comparisons between albums, we can construct a ranking, and thus begin to answer the questions we care&amp;nbsp;about.&lt;/p&gt;
&lt;p&gt;I sought to build my own platform to handle my specific needs: the needs of the individual. I began by throwing together a prototype in Python, using the command line as an interface. I tested out the idea of having an input/output pipeline for making comparisons between albums on a variety of characteristics. This helped me identify viable algorithms and create a feedback loop with my study of aesthetics to refine how I could compare albums, ultimately converging upon the idea of pairwise comparisons amongst a variety of incisive questions. The results are two-fold: I have a distributed, cognitive tool which guides my memories and thoughts on records, and I also have a way to quantitatively assess aesthetic pieces to definitively answer the questions I was first motivated&amp;nbsp;by.&lt;/p&gt;
&lt;h2&gt;Music&amp;nbsp;Collection&lt;/h2&gt;
&lt;p&gt;I love talking about music. But when you begin to span dozens of genres, decades, styles, and more, you start to lose track of whatâs what and whoâs who. To help support my memory, and make it easy to jot notes (which is proven to help you remember things), my application has multiple fields for writing. Each album can receive a review, as can each track. Ideally, after listening to each track and giving them reviews, youâd have a good idea for the global structure and feel of the&amp;nbsp;album.&lt;/p&gt;
&lt;p&gt;To ensure I could draw upon a vast database of world music, I integrate with Spotify, my primary platform for listening to music. This works well, because any record I find or listen to on Spotify can be loaded into the application. The search bar at the top provides a portal to access Spotifyâs&amp;nbsp;collections:&lt;/p&gt;
&lt;video height="600" uk-video="autoplay: inview" loop muted&gt;
    &lt;source src="/images/preview.mp4" type="video/mp4"/&gt;
    Your browser does not support the video tag.
&lt;/video&gt;
&lt;div class="caption"&gt;
    Preview of interaction with the application.
&lt;/div&gt;

&lt;p&gt;Below the search, thereâs an area which allows two albums to be compared. Which albums get chosen for comparison is optimal (see below), so I can be sure that each comparison is doing the most to generate an accurate ranking. The comparison happens according to a series of questions which I created, although even these are subjective and individualistic: what I think makes a great album isnât necessarily what you think does (although I tried to be a&amp;nbsp;generalist).&lt;/p&gt;
&lt;h2&gt;Development&lt;/h2&gt;
&lt;p&gt;I underwent the task of teaching myself the React-Redux ecosystem from scratch to make this happen. I would consider myself a JavaScript bystander â occasionally using it with decent proficiency â but learning React and Redux required me to (re)learn a lot. I would argue thereâs a decently steep learning curve to the frameworks, given the way the application data are held quite separately in an immutable state. Nonetheless, I made it happen and experienced only a few bouts of hair&amp;nbsp;ripping.&lt;/p&gt;
&lt;p&gt;It was all a learning experience, but from the start I had a rough idea of the size and scope of my application. I intended to apply some React-Redux best practices, which occasionally required some major refactoring, but it was probably worth it. The application is modularized, and Iâve tried to ensure the hierarchy of components keeps logic and data at the appropriate scope. One downside of my current configuration is that the application store holds a lot of data, especially the album collection data. This has started to become a little cumbersome on reloads, especially as I begin to notice small performance hits. Iâve searched for articles outlining best practices for scaling applications with React but havenât come across anything suitable&amp;nbsp;yet.&lt;/p&gt;
&lt;p&gt;For storage, I created a MongoDB instance served locally. To interface with this, my backend &lt;span class="caps"&gt;API&lt;/span&gt; is built with Flask, which also routes the main page. This Flask &lt;span class="caps"&gt;API&lt;/span&gt; is responsible for all album data manipulation, including ranking albums and serving category scores, caching and serving album art, and storing changes to reviews and listening history. I also utilize the Spotify Developerâs &lt;span class="caps"&gt;API&lt;/span&gt; to fetch information about albums, including tracklists and album art. Upon adding a new album to my collection, the album art is cached locally on my computer to prevent me from making an unnecessarily large number of requests to Spotifyâs image &lt;span class="caps"&gt;CDN&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In summary, the application architecture is as&amp;nbsp;follows:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/architecture.png" alt="The application's architecture relies on Python, Flask, React/Redux, mongo DB, and the Spotify API"&gt;&lt;/p&gt;
&lt;p&gt;Some details are left out here, but the gist is conveyed. I toyed with the idea of using a relational database instead of NoSQL MongoDB, but figured the rest of my code would be most interpretable with a NoSQL framework. Since Iâm not pushing the edges of storage capabilities, I didnât anticipate this to create any noticeable performance issues&amp;nbsp;either.&lt;/p&gt;
&lt;p&gt;At the top of the application, there are methods for filtering and sorting. As my collection grows larger, itâs become increasingly important for me to be able to quickly find albums. Iâve noticed this as I talk about music and want to pull up an album quickly, and when Iâm rating albums, and need a refresher. I have five ways to sort: 1) by the date I added the record to my collection, 2) by the date I last listened to the record, 3) by the recordâs runtime, 4) by the recordâs rank, and 5) by the recordâs recommendation score. I can also search according to artist or album name, or by&amp;nbsp;genre:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/filter.png" alt="A screenshot of the application when filtering albums"&gt;&lt;/p&gt;
&lt;div class="caption"&gt;
    Example of the filtering methods: a search for &amp;#8220;bepop&amp;#8221; aptly returned John Coltrane&amp;#8217;s A Love Supreme
&lt;/div&gt;

&lt;p&gt;Below I discuss more about the album ranks and recommendation scores. The genres are a somewhat quasi-genres: theyâre actually genres of the artist. Spotify doesnât yet allow developers to access album genres, so I make due with broader genres. Usually, artists have genres; however, some music Iâve discovered is highly esoteric and therefore without classification. This was an edge case I didnât anticipate but luckily doesnât affect the &lt;span class="caps"&gt;UX&lt;/span&gt; too&amp;nbsp;much.&lt;/p&gt;
&lt;h2&gt;Design&lt;/h2&gt;
&lt;p&gt;At heart, Iâm a sucker for elegant and compelling design. This includes color schemes, negative space, typography, the whole thing. As a result, a constant voice inside me begged for better and better user interface design. I&amp;nbsp;acquiesced.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/album.png" alt="A screenshot of the album view, with Nevermind by Nirvana in focus"&gt;&lt;/p&gt;
&lt;div class="caption"&gt;
    Design was at the forefront of this project, going through many iterations. Clean, elegant, colorful, open was the goal.
&lt;/div&gt;

&lt;p&gt;The design stemmed from some ideas I had and styles I liked, as well as a combination of inspirations from Apple Music, Spotify, various projects I discovered on Behance, and input from friends. It amounted to a clean, straightforward design, with ample incorporation of the uniqueness and beauty inherent in many album artworks. I wanted plenty of negative space to let the application breathe, even if that meant having a long, long list of albums. Simplicity was in mind from the graphs to the&amp;nbsp;fonts.&lt;/p&gt;
&lt;p&gt;With some &lt;span class="caps"&gt;CSS&lt;/span&gt; blur, rotate, and saturation filters, I took the artwork for each record and placed it in the top right corner of each card, creating album themes unique to each record and gently bringing in a little more color and&amp;nbsp;pop.&lt;/p&gt;
&lt;p&gt;The icons come from &lt;a href="http://fontawesome.io/" target="_blank"&gt;FontAwesome.com&lt;/a&gt;, which conveniently can be tied together with specialized React components to ensure proper updates and loading. More can be &lt;a href="https://fontawesome.com/how-to-use/on-the-web/using-with/react" target="_blank"&gt;read&lt;/a&gt; here on&amp;nbsp;that.&lt;/p&gt;
&lt;h2&gt;Scoring and&amp;nbsp;Ranking&lt;/h2&gt;
&lt;p&gt;What really piqued my interest in the beginning was the idea of scoring or ranking albums, on some âbestâ to âworstâ subjective scale. I started out by researching what experts considered characteristics of the âbestâ music. I found quite a few academic papers. I read hours of album reviews from strangers on internet forums. I took notes while watching music reviewers on YouTube. And I considered my own personal taste. &lt;a href="https://pdfs.semanticscholar.org/d3dd/702011708b38c8bbb58a364a23d452753c52.pdf" target="_blank"&gt;Some researchers&lt;/a&gt; have attempted the idea of quantitatively judging aesthetics, but nothing&amp;#8217;s seemed to have gained recognition or suit my needs. All my research led to the conclusion that there probably wasnât a good way to rank music (or art in general), let alone quantify its relative superiority to other&amp;nbsp;works.&lt;/p&gt;
&lt;p&gt;But it wouldnât be fun if we just ended the project here. So, I abstracted out 12 categories which I think help explain what makes a good record. Each category became associated with a question, or prompt, for which someone could say one album or the other answered the question better. For example, I think the album art contributes a non-negligible amount to how good an album is. Some of the best albums ever have intriguing, provocative, relevant, and/or memorable artwork (see Dark Side of the Moon, Nevermind, etc.). Here is the list of questions which albums may be compared&amp;nbsp;under:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Which album art contributes to the album&amp;nbsp;more?&lt;/li&gt;
&lt;li&gt;Which album would you buy for a music&amp;nbsp;critic?&lt;/li&gt;
&lt;li&gt;Which has songs that build an album greater than its&amp;nbsp;parts?&lt;/li&gt;
&lt;li&gt;Which album makes you think&amp;nbsp;more?&lt;/li&gt;
&lt;li&gt;Which album better balances soft and&amp;nbsp;loud?&lt;/li&gt;
&lt;li&gt;More songs on which album didn&amp;#8217;t flow well with the&amp;nbsp;rest?&lt;/li&gt;
&lt;li&gt;Which album would you buy for your best&amp;nbsp;friend?&lt;/li&gt;
&lt;li&gt;Which are you more likely to get distracted during while&amp;nbsp;listening?&lt;/li&gt;
&lt;li&gt;Which would you choose if forced to listen to one once every&amp;nbsp;day?&lt;/li&gt;
&lt;li&gt;Taking only the lyrics from these albums, which would make a better story in book&amp;nbsp;form?&lt;/li&gt;
&lt;li&gt;Which album has more filler&amp;nbsp;tracks?&lt;/li&gt;
&lt;li&gt;Which album has better&amp;nbsp;vocals?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each of these questions results in a pairwise comparison, and luckily, there is a long history of ranking according to pairwise comparison. Much of the interest in this field relates to ranking sports teams, chess players, or other types of competitive activities. One popular algorithm the &lt;a href="https://en.wikipedia.org/wiki/Elo_rating_system" target="_blank"&gt;&lt;span class="caps"&gt;ELO&lt;/span&gt; rating method&lt;/a&gt;. However, we can also picture albums as competitors and apply the same&amp;nbsp;principles.&lt;/p&gt;
&lt;p&gt;Previous work by &lt;a href="https://arxiv.org/pdf/1801.01253.pdf" target="_blank"&gt;Heckel et al (2018)&lt;/a&gt; proposed a pairwise ranking algorithm with theoretically optimal guarantees for active learning. Their work was particularly relevant to my system because they designed their algorithm specifically for an active learning setting. This means that there are ways to figure out what the model doesnât know, or isnât confident in, and leverage this uncertainty to pick the next, most informative comparison. The algorithm by Heckel et al wasnât quite what I was looking for, but &lt;a href="https://dl.acm.org/doi/pdf/10.5555/3305890.3305923" target="_blank"&gt;Maystre &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; Grossglauserâs (2017)&lt;/a&gt; work proved valuable. They suggested optimal competitive uncertainty sampling with the popular Bradley-Terry ranking model simply by choosing to compare the pair whose relative ordering in most uncertain (has the smallest gap between&amp;nbsp;scores).&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model" target="_blank"&gt;Bradley-Terry model framework&lt;/a&gt; is by far one of the most common approaches to ranking entities given pairwise comparisons. Using the &lt;code&gt;choix&lt;/code&gt; Python package, I compute a Bradley-Terry model for each rating category over all albums. On output, each album is associated with a score from 0 to 1. I take all 12 scores, average them, and rank the albums using this average score. So far, this has worked quite&amp;nbsp;well.&lt;/p&gt;
&lt;p&gt;However, I have requirements for when an album can be ranked. Namely, each album is assigned default values for each category which fall between about 0.4 and 0.6, subject to some quirks in the Bradley-Terry models. Only after Iâve listened to the album three full times through may I begin to compare it to other albums. I do this a safeguard and baseline. I donât want to misjudge an album after having only listened to it once. It may take time to understand it, and often does. Once an album reaches three listens, it will be ready to be queued up in the comparison&amp;nbsp;framework.&lt;/p&gt;
&lt;h2&gt;Recommendation&amp;nbsp;Algorithm&lt;/h2&gt;
&lt;p&gt;With a desire to get the most out of the music I know, I sought out a recommendation engine to suggest which album I should listen to&amp;nbsp;next.&lt;/p&gt;
&lt;p&gt;Most engineers default to complex algorithms when they think of recommendation algorithms. However, my data are fairly slim, and I had some ideas in mind to build a heuristic algorithm instead. This just means that instead of a black-box recommendation algorithm, relying on the albums content or perhaps other data about my listening behavior, I have a deterministic set of rules to create a ârecommended&amp;nbsp;scoreâ.&lt;/p&gt;
&lt;p&gt;This recommended score is based on five things: when was the last time I listened to the album, when did I add the album to my collection, how many minutes long is the album, how many times have I listened to the album (and how close is it to having 3 listens), and how common are the genres the artist is listed under. Using these attributes, scores for each are given to each album, and then a weighted average is taken to create an overall recommended score. Sort the recommended score from highest (1) to lowest (0) and thereâs the recommendation. It appears on each album card to the right of a small compass&amp;nbsp;icon.&lt;/p&gt;
&lt;p&gt;Unfortunately, this doesnât discover new music Iâve never heard before; only records Iâve put in my collection already. This isnât the worst case scenario though. In fact, one of the things I enjoy most is crate-digging. During the &lt;span class="caps"&gt;COVID&lt;/span&gt;-19 lockdown, this now is usually digital crate digging, but I still love to put in the effort to find new artists myself. One great label I recommend for finding eclectic and esoteric music is the &lt;a href="http://www.numerogroup.com/" target="_blank"&gt;Numero Group&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Last&amp;nbsp;Words&lt;/h2&gt;
&lt;p&gt;Overall, I use my application almost every day now as both a repository and an information source. Itâs used to record my thoughts and feelings about albums and tracks, and itâs used to recommend what I should listen to next. I filter and sort and discover new patterns in what I like and donât like. I see gaps in genres of music which I should listen to, forcing me out of my comfort zone and into artists and albums I wouldnât normally think to&amp;nbsp;explore.&lt;/p&gt;
&lt;p&gt;I wonât be sharing my top albums or genres yet, I have lots of albums to add and listen to before that can&amp;nbsp;happen.&lt;/p&gt;
&lt;p&gt;I maintain &lt;a href="https://github.com/liebscher/MusicCollection" target="_blank"&gt;this project on GitHub&lt;/a&gt;, so feel free to fork it if you think youâd also benefit from it. If you have questions, you may email me or post an issue. Iâll respond either way. One potential use for this is to go beyond music â the framework would work just as great to determine your favorite books, beers, or birds (birds? why not, I needed the&amp;nbsp;alliteration).&lt;/p&gt;</content><category term="meta"></category><category term="music"></category><category term="programming"></category><category term="design"></category><category term="javascript"></category></entry><entry><title>Paper accepted at CogSciÂ 2020</title><link href="/posts/2020/Apr/cogsci-acceptance/" rel="alternate"></link><published>2020-04-30T00:00:00-07:00</published><updated>2020-04-30T00:00:00-07:00</updated><author><name>Alex Liebscher</name></author><id>tag:None,2020-04-30:/posts/2020/Apr/cogsci-acceptance/</id><summary type="html">&lt;p&gt;Our paper was accepted at this year&amp;#8217;s Cognitive Science&amp;nbsp;Conference.&lt;/p&gt;</summary><content type="html">&lt;p&gt;A first-half of my undergraduate thesis was accepted to &lt;a href="https://cognitivesciencesociety.org/cogsci-2020/"&gt;CogSci 2020&lt;/a&gt;. I am in the process of submitting this together with the second (confirmatory) half to&amp;nbsp;journals.&lt;/p&gt;
&lt;p&gt;If you wish, you may read &lt;a href="https://liebscher.github.io/assets/pdf/liebscher_cogsci20_proceedings.pdf"&gt;our CogSci paper here&lt;/a&gt;.&lt;/p&gt;</content><category term="meta"></category><category term="news"></category></entry></feed>