<!DOCTYPE html>
<html lang="en">

<head>
         <title>Alex Liebscher - Review: Evaluating Neural Toxic Degeneration in Language Models</title>
       <meta charset="utf-8" />
       <meta name="description" content="">
       <meta name="viewport" content="width=device-width, initial-scale=1" />
       <meta name="generator" content="Pelican" />

       <link href="/feeds/all.atom.xml"
              type="application/atom+xml" rel="alternate" title="Alex Liebscher Full Atom Feed" />
       <link href="/feeds/meta.atom.xml"
              type="application/atom+xml" rel="alternate" title="Alex Liebscher Categories Atom Feed" />




    <meta name="tags" content="machine learning" />
    <meta name="tags" content="nlp" />
    <meta name="tags" content="society" />


       <!-- UIkit CSS -->
       <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/uikit@3.10.0/dist/css/uikit.min.css" />

       <!-- UIkit JS -->
       <script src="https://cdn.jsdelivr.net/npm/uikit@3.10.0/dist/js/uikit.min.js"></script>
       <script src="https://cdn.jsdelivr.net/npm/uikit@3.10.0/dist/js/uikit-icons.min.js"></script>

       <link rel="preconnect" href="https://fonts.googleapis.com">
       <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
       <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Serif:ital,wght@0,200;0,400;0,600;1,400&family=Playfair+Display:wght@400;600&display=swap"
              rel="stylesheet">

       <link rel="stylesheet" type="text/css" href="/theme/css/default.css" />
       <link rel="stylesheet" type="text/css" href="/theme/css/style.css" />

       <!-- favicon licensing -->
       <!-- Copyright 2020 Twitter, Inc and other contributors (https://github.com/twitter/twemoji) -->
       <!-- License: CC-BY 4.0 (https://creativecommons.org/licenses/by/4.0/) -->
       <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
       <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
       <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
       <link rel="icon" type="image/png" href="/favicon.ico">

       <script data-goatcounter="https://liebscher.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>
</head>

<body>
       <header class="uk-section uk-section-small">
              <div class="uk-container uk-width-2-3@m uk-margin-auto">
                     <!-- show a bigger name on mobile -->
                     <h1 class="uk-heading uk-text-center uk-visible@s">
                            <a href="/">Alex Liebscher</a>
                     </h1>
                     <h1 class="uk-heading-medium uk-text-center uk-hidden@s">
                            <a href="/">Alex Liebscher</a>
                     </h1>
                     <!-- 
 -->
              </div>
       </header>
<section class="uk-section uk-section-xsmall uk-width-2-3@m uk-margin-auto">
  <div class="uk-container uk-text-justify">
    <ul class="uk-breadcrumb">
      <li><a href="/index.html">Home</a></li>
      <li><span>Review: <i>Evaluating Neural Toxic Degeneration in Language&nbsp;Models</i></span></li>
    </ul>
  </div>
</section>
<section class="uk-section uk-background-cover" style="background-image: url(/images/unsplash-header-bg.jpg)">
  <div class="uk-container uk-width-2-3@m">
    <header>
      <h1 class="uk-h1 uk-text-center">
        <a href="/posts/2020/Oct/toxic-degeneration/" rel="bookmark" title="Permalink to Review: Evaluating Neural Toxic Degeneration in Language Models">Review: <i>Evaluating Neural Toxic Degeneration in Language&nbsp;Models</i></a>
      </h1>
      
    </header>
    <footer class="uk-text-center uk-margin">
      <time datetime="2020-10-17T00:00:00-07:00">
        Sat 17 October 2020
      </time>
      <!--     <div>
        Category: <a href="/">meta</a>
    </div>
 -->
      <div>
        Tags:
        <a href="/tag/machine-learning.html">machine learning</a>, 
        <a href="/tag/nlp.html">nlp</a>, 
        <a href="/tag/society.html">society</a>
      </div>
    </footer>
  </div>
</section>
<section class="uk-section">
  <div class="uk-container uk-width-expand uk-width-2-3@m uk-text-justify">
    <p>This review is on &#8220;<a href="https://api.semanticscholar.org/CorpusID:221878771">RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models</a>&#8221; from a group of researchers at the University of Washington and the Allen Institute for <span class="caps">AI</span>, to be found in the <em>Findings of <span class="caps">EMNLP</span> 2020</em> <d-footnote><a href="https://2020.emnlp.org/blog/2020-04-19-findings-of-emnlp" target="_blank">A new (as of April 2020) &#8220;companion&#8221; to <span class="caps">EMNLP</span></a> which offers sort of a middle group between rejection and acceptance into the main conference. It seems like a happy middle ground for still notable research.</d-footnote>. The authors set the stage by&nbsp;saying,</p>
<blockquote>
<p>language models (LMs) pretrained on large web text corpora suffer from degenerate and biased&nbsp;behavior</p>
</blockquote>
<p>To no one&#8217;s surprise, LMs are being deployed at an increasing pace due to the popularity of tools such as <a href="https://huggingface.co/" target="_blank">huggingface</a>. The authors contribute to recent work on LMs by first offering an operationalization of toxic model generations, then introduce a novel dataset of labeled toxic and non-toxic phrases, and then demonstrate the danger of LMs (even when tuned to avoid toxic language). The authors evaluate a few different methods for preventing toxic language generation, although none is perfect. Lastly, they partially diagnose what&#8217;s causing bad generations (hint: garbage in, garbage out), and offer some recommendations to ameliorate the&nbsp;issue.</p>
<p>I was interested in this paper because it is one of those that integrates both <span class="caps">NLP</span> and sociolinguistics. Before even trying to prevent a <span class="caps">LM</span> from spitting forth profanity, one has to define and operationalize some very tricky subjects. What exactly is profanity? What is toxicity? At what point does a word move from being uncomfortable or taboo to profane or marginalizing? How does the definition of toxic change from person to person? Can something be profane when said by one person, but not another? Only one, maybe two, of these is discussed in this article, but I found them useful to consider while&nbsp;reading.</p>
<h2>Language Model&nbsp;Background</h2>
<p>Before beginning, I&#8217;ll give a bit of background on generative Language Models. In plain English, a Language Model scans huge collections of documents (millions of documents), word by word, learning statistical associations between words and their neighbors, and is then able to predict the next word in a phrase by just looking for the most probable in the English language. Your iPhone does this (if you have predictive typing turned on), as does Gmail when you&#8217;re drafting an email, and a suite of other&nbsp;tools.</p>
<p>You may have heard of <span class="caps">GPT</span>-3 (and <a href="https://openai.com/blog/gpt-2-1-5b-release/" target="_blank">it&#8217;s predecessors</a>)<d-footnote>For an excellent illustrated explanation, I recommend <a href="http://jalammar.github.io/illustrated-gpt2" target="_blank">the classic from Jay Alammar</a>.</d-footnote>, which has been <a href="https://www.theverge.com/21346343/gpt-3-explainer-openai-examples-errors-agi-potential" target="_blank">in</a> <a href="https://www.technologyreview.com/2020/07/20/1005454/openai-machine-learning-language-generator-gpt-3-nlp/" target="_blank">the</a> <a href="https://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3" target="_blank">news</a> <a href="https://www.vox.com/future-perfect/21355768/gpt-3-ai-openai-turing-test-language">a lot</a>, or of <a href="https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270" target="_blank"><span class="caps">BERT</span></a>, which might have received more attention in the academic press than the public press. The <span class="caps">GPT</span> family and <span class="caps">BERT</span> are both Language Models. <span class="caps">GPT</span> is distinct from <span class="caps">BERT</span> though in that it is <em>auto-regressive</em>, meaning it progressively sees and trains on more of the sentence as it reads, instead of seeing the whole sentence at once. This is what makes it particular useful for predicting what will comes next, making it <em>generative</em>.</p>
<h2>Operationalizing&nbsp;Toxicity</h2>
<p>The authors state the importance of having well-annotated data, and that the definition of &#8220;toxic&#8221; is crucial to their argument, although they pass off the responsibility to another tool. They do correctly note that hand-annotating such a large dataset is unfeasible. In the&nbsp;end,</p>
<blockquote>
<p>We rely on <a href="https://www.perspectiveapi.com/" target="_blank">Perspective <span class="caps">API</span></a>, an automated tool for toxic language and hate speech&nbsp;detection</p>
</blockquote>
<p>The <span class="caps">API</span>, when given a linguistic form (e.g., a sentence), calculates a score, or probability of toxicity. The authors then (perhaps unjustifiably) label an input prompt as toxic if it has a score of over 0.5, otherwise it&#8217;s labeled non-toxic. The authors confess that the <span class="caps">API</span> likely suffers from an over-reliance on lexical cues to detect toxicity, where lexical cues basically just means specific words, such as swear words or&nbsp;slurs.</p>
<h2><em>RealToxicityPrompts</em>&nbsp;Dataset</h2>
<p>One of the main contributions of the paper is the dataset the authors compiled. It consists of 100,000 sentences, spanning the whole range of toxicities, that were split in half into a <em>prompt</em> and a <em>continuation</em>. Both halves were scored for toxicity. Their hope is that by creating this big dataset of English web text scored for toxicity, others will be able to &#8220;systematically evaluate and compare the generations from language models&#8221; with this &#8220;testbed for&nbsp;toxicity.&#8221;</p>
<h2>Detoxifying Model&nbsp;Generations</h2>
<p><img class="uk-align-center" data-src="/images/toxicity-generations.png" height="" width="50%" alt="" uk-img></p>
<div class="caption">
    <strong>Figure 1.</strong> Four non-toxic prompts which cause several pretrained LMs to generate highly toxic output. Credit: Gehman et al. (2020)
</div>

<p>As illustrated in Figure 1, even non-toxic prompts (those with a toxicity score from Perspective less than 0.5) lead to toxic generations. The crux of the paper is in their assessment of five ways to detoxify <span class="caps">LM</span> generations. Below are quick summaries of these methods and how they reduce toxic&nbsp;generations.</p>
<h3>Data-Based&nbsp;Detoxification</h3>
<dl>
<dt>Domain-Adaptive Pretraining (<span class="caps">DAPT</span>)</dt>
<dd>Perform additional pretraining on a smaller <em>non-toxic</em>&nbsp;corpus.</dd>
<dt>Attribute&nbsp;Conditioning</dt>
<dd>Perform additional pretraining on a sample of the corpus which has been labeled with either a toxic attribute token, <code>&lt;|toxic|&gt;</code>, or a non-toxic attribute token, <code>&lt;|nontoxic|&gt;</code>. During generation, prepend the prompt with <code>&lt;|nontoxic|&gt;</code> specifically.</dd>
</dl>
<h3>Decoding-Based&nbsp;Detoxification</h3>
<dl>
<dt>Vocabulary&nbsp;Shifting</dt>
<dd>Modify the decoding algorithm to give higher probability to non-toxic tokens. Learns a weight for each vocabulary token which represents the association between each token and&nbsp;(non-)toxicity.</dd>
<dt>Word&nbsp;Filtering</dt>
<dd>Create a blacklist of profanity, slurs, and swearwords and set the probability of generating those words to zero to prevent the LMs from generating them&nbsp;altogether.</dd>
<dt><a href="https://openreview.net/pdf?id=H1edEyBKDS" target="_blank"><span class="caps">PPLM</span></a></dt>
<dd>Combine a pretrained, unconditional language model with one or more simple attribute models, which are small and cheap models that figure out latent gradients between attributes and the pretrained language models to steer&nbsp;outputs.</dd>
</dl>
<h3>Results</h3>
<p>As one might expect, none of these detoxification methods completely eradicated toxic generations; and toxic prompts yielded higher toxicity in generation than non-toxic prompts. What&#8217;s somewhat surprising is that non-toxic prompts occasionally also led to toxic generations. It should be clear then that even benign models can be harmful. The authors point out that it&#8217;s also surprising that the <span class="caps">CTRL</span>-<span class="caps">WIKI</span> model produced toxic content, despite being pretrained to generate Wikipedia-style output<d-footnote>The authors also state it&#8217;s interesting since &#8220;[the model] was trained on just Wikipedia.&#8221; I&#8217;m not sure this statement is true after skimming the original <span class="caps">CTRL</span> paper from Kaskar et al. (2019).</d-footnote>. Furthermore, <span class="caps">DAPT</span> is one of the most effective methods (which I found surprising, given its simplicity), along with vocabulary shifting, and <span class="caps">PPLM</span>. The authors lastly state that steering generations after pretraining must thus be crucial for preventing toxic&nbsp;output.</p>
<p>There were some phrases, which when used to prompt the models consistently returned toxic generations. These prompts were often toxic themselves or had the opening phrase of known toxic sentences (&#8220;his rant was full of&#8230;&#8221;). Lastly, about 10% of the prompts came from unreliable or toxic data sources, which we get into&nbsp;next.</p>
<h2>Analysis of Web Text&nbsp;Toxicity</h2>
<p><img class="uk-align-center" data-src="/images/toxicity-corpora.png" height="" width="40%" alt="" uk-img></p>
<div class="caption">
<strong>Figure 2.</strong> About 2.1% and 4.3% of <span class="caps">OWTC</span> and OpenAI-<span class="caps">WT</span> (the corpora which the x and x models were trained on) are toxic content (note: log-transformed y-axis). Credit: Gehman et al. (2020)
</div>

<p>Up next, the authors attempt to quantify the toxicity present in the training data for the LMs they tested. Specifically, the training data were OpenAI-<span class="caps">WT</span> (<span class="caps">GPT</span>-2&#8217;s training data) and its open-source replica <span class="caps">OWTC</span>. There is about 29% overlap between the two datasets, which is low enough to come to interesting conclusions about how the models may be affected by their training&nbsp;data.</p>
<p>In Figure 2 (note: it&#8217;s log-transformed), we can see that about 2.1% and 4.3% of <span class="caps">OWTC</span> and OpenAI-<span class="caps">WT</span> are toxic content. About 3% of <span class="caps">OWTC</span> in fact comes from links shared on banned or quarantined subreddits<d-footnote>Banned subreddits are inaccessible via the website and only via data dumps, whereas quarantined subreddits are special-access only but still online.</d-footnote>.</p>
<h2>Recommendations</h2>
<p>The authors conclude with a discussion and a variety of recommendations for <span class="caps">NLP</span> researchers. First, it should seem apparent that there&#8217;s an issue with LMs generating toxic content. From the analysis of the training corpora, we might guess that this degeneracy comes from poor training data. They concede that the steering methods did have some positive effect, but did not completely resolve the problem. There are three primary implications&nbsp;proposed:</p>
<ol>
<li>
<p><span class="dquo">&#8220;</span>Can language models ever fully &#8216;forget&#8217; toxic pretraining data through further&nbsp;adaptation?&#8221;</p>
<p>Essentially, you can start with a model trained on some bad data, but is there any amount of finite data you could then continue pretraining on to wash out, to &#8220;forget&#8221;, the bad stuff? It seems as though the LMs are &#8220;memorizing&#8221; the bad content, which might come from such content being more salient to the model. The authors recommend for future work to explore whether some types of toxicity are more difficult for models to&nbsp;forget.</p>
</li>
<li>
<p>Purposeful&nbsp;decoding</p>
<p>One of the most promising methods of eliminating toxic generations was <span class="caps">PPLM</span>. Perhaps there exist other methods to aid in the decoding phase and prevent toxic generation. For example, using handpicked toxic documents as &#8220;negative examples&#8221; which the model would learn not to produce. The authors also nebulously suggest &#8220;infusing models with more sophisticated or nuanced representations of social&nbsp;biases.&#8221;</p>
</li>
<li>
<p>Choice of Pretraining&nbsp;Data</p>
<p>The authors also call for a dramatic introspection of the training data with which LMs learn on. Some issues arise when relying on huge swathes of data with very little filtering e.g., Reddit is known to have a biased user-base. This should call to question: &#8220;who decides whose voices are going to be learned by the language model.&#8221; One appealing recommendation is to make the pretraining process more human-centered, including through participatory&nbsp;design.</p>
<p>The authors lastly caution that curating pretraining data without deep thought might have unintended side-effects, such as filtering out benign text from African American authors/users. They suggest engaging with the end-user during this&nbsp;phase.</p>
</li>
</ol>
<h2>Take-aways</h2>
<p>It&#8217;s a compelling and mostly uncontroversial article, which clearly highlights shortcomings currently in <span class="caps">NLP</span>. I think too little is said about the impact on industry (companies are currently trying to solve this problem), and society as a whole. In fact, regarding the latter, I think more could have been said about the practical implications of <span class="caps">LM</span> degeneration on the spreading of misinformation. In sum though, the authors created a valuable dataset for future research, and exposed a lower bound on toxicity degeneracy in&nbsp;LMs.</p>
<p>I&#8217;m also a little unsatisfied by the authors&#8217; operationalization of toxicity. While I understand their desire to create a large dataset for other researchers to train on, I&#8217;m not sure the Perspective <span class="caps">API</span> should be considered the end of the conversation. Undoubtedly, it will produce false positives and false negatives (which the authors briefly mentioned), which should lead us to question if their assessments are answering the right questions. If their ground truth is not what a human would believe, then this could easily disrupt their main&nbsp;claims.</p>
<p>Lastly, on the topic of what humans would believe: some people find some things offensive, others don&#8217;t. Are the authors (or by proxy, Perspective) justified in claiming what is toxic and what is not? More philosophically, can someone speak for someone else on what is right or&nbsp;wrong?</p>
  </div>
</section>
       <footer class="uk-section">
              <div class="uk-container uk-width-2-3@m uk-margin-auto">
                     <address>
                            <a href="/pages/copyright.html">© 2024 Alex Liebscher</a> | Powered by <a
                                   href="https://getpelican.com/">Pelican</a>.
                     </address>
              </div>
       </footer>
</body>

</html>